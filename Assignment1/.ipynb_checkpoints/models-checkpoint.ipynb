{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Training loss after  0  iterations is  1.5811388300841898\n",
      "Training loss after  500  iterations is  0.11515089871796896\n",
      "Training loss after  1000  iterations is  0.11515089872154656\n",
      "Training loss after  1500  iterations is  0.11515089872154524\n",
      "Training loss after  2000  iterations is  0.11515089872154423\n",
      "Training loss after  2500  iterations is  0.11515089872154423\n",
      "Training loss after  3000  iterations is  0.11515089872154423\n",
      "Training loss after  3500  iterations is  0.11515089872154423\n",
      "Training loss after  4000  iterations is  0.11515089872154423\n",
      "Training loss after  4500  iterations is  0.11515089872154423\n",
      "Predicted Values: [[2.76462962]\n",
      " [2.76462962]]\n",
      "True Values: [3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scratch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Running Linear Regression on Alabone dataset '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Running Linear Regression on Alabone dataset \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05352386  0.50788834  0.42431958 ... -0.02207576  0.24530597\n",
      "  -0.09951538]\n",
      " [ 1.26171644 -0.07501672 -0.18028967 ... -0.71816866 -0.61230092\n",
      "  -0.11029156]\n",
      " [ 1.26171644 -0.15828888 -0.18028967 ... -0.41630313 -0.35228181\n",
      "  -0.48027378]\n",
      " ...\n",
      " [-1.15466873  0.71606872  0.67624011 ...  0.43748071  0.43233726\n",
      "   0.19503356]\n",
      " [ 0.05352386 -0.36646926 -0.38182609 ... -0.50415951 -0.27473225\n",
      "  -0.57007528]\n",
      " [-1.15466873  0.84097695  0.67624011 ...  0.63572076  0.50076334\n",
      "   0.51472693]]\n",
      "[[ 6.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " ...\n",
      " [10.]\n",
      " [15.]\n",
      " [10.]]\n"
     ]
    }
   ],
   "source": [
    "pp = MyPreProcessor()\n",
    "X,Y = pp.pre_process(0)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k=5, loss = \"rmse\", epochs = 8000, learning_rate = 0.01):\n",
    "    \"\"\" Performs K fold cross validation\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : instance of the model to be used\n",
    "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as data.\n",
    "    y : 1-dimensional numpy array of shape (n_samples,) which acts as labels.\n",
    "    k : number of folds, default = 5\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : instance of model\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  #number of examples\n",
    "    fold_size = int(m/k)\n",
    "    start = 0\n",
    "    end = fold_size\n",
    "    models = {}\n",
    "    for i in range(k):\n",
    "        Xtrain_i = np.concatenate((X[0:start], X[end+1:]))\n",
    "        ytrain_i = np.concatenate((y[0:start],y[end+1:]))\n",
    "        X_test =  X[start:end]\n",
    "        y_test = y[start:end]\n",
    "        model = MyLinearRegression()\n",
    "        print(\"For fold : \", i , \"/\", k)\n",
    "        model.fit(Xtrain_i,ytrain_i,X_test,y_test,epochs,learning_rate, loss)\n",
    "        if(loss == \"rmse\"):\n",
    "            models[i] = (model.rmse_train_history[-1], model.rmse_val_history[-1], np.array(model.rmse_train_history), np.array(model.rmse_val_history))\n",
    "        if(loss == \"mae\"):\n",
    "            models[i] = (model.mae_train_history[-1], model.mae_val_history[-1], np.array(model.mae_train_history), np.array(model.mae_val_history))\n",
    "        print(model.W)\n",
    "        print(model.b)\n",
    "        start+=fold_size\n",
    "        end+=fold_size\n",
    "        \n",
    "    avg_train = 0\n",
    "    avg_val  = 0\n",
    "    for i in range(len(models)):\n",
    "        avg_train+=models[i][2]\n",
    "        avg_val+=models[i][3]\n",
    "            \n",
    "    \n",
    "    avg_train = avg_train/k\n",
    "    avg_val = avg_val/k\n",
    "    return models, avg_train, avg_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 2\n",
      "Training loss after  0  iterations is :  10.467738559547147  | validation loss is :  10.414953882174231\n",
      "Training loss after  500  iterations is :  5.8471513974625315  | validation loss is :  5.8247370226516075\n",
      "Training loss after  1000  iterations is :  2.897665502508849  | validation loss is :  2.8796515108795666\n",
      "Training loss after  1500  iterations is :  2.4114514013088986  | validation loss is :  2.3904294967106563\n",
      "Training loss after  2000  iterations is :  2.346891027831644  | validation loss is :  2.3204868846577713\n",
      "Training loss after  2500  iterations is :  2.3118577379959087  | validation loss is :  2.2813293528142213\n",
      "Training loss after  3000  iterations is :  2.28994097321903  | validation loss is :  2.2564675129353584\n",
      "Training loss after  3500  iterations is :  2.276076006545979  | validation loss is :  2.240437950682088\n",
      "Training loss after  4000  iterations is :  2.267176026148701  | validation loss is :  2.2299008487654928\n",
      "Training loss after  4500  iterations is :  2.261332035468685  | validation loss is :  2.2227964892289114\n",
      "[[-0.34740897]\n",
      " [ 0.27935559]\n",
      " [ 0.88691946]\n",
      " [ 0.382561  ]\n",
      " [ 0.47111369]\n",
      " [-2.33990744]\n",
      " [-0.28203819]\n",
      " [ 2.34414099]]\n",
      "9.943399447964415\n",
      "For fold :  1 / 2\n",
      "Training loss after  0  iterations is :  10.414953882174231  | validation loss is :  10.4697154941639\n",
      "Training loss after  500  iterations is :  5.827536161611287  | validation loss is :  5.859010083050921\n",
      "Training loss after  1000  iterations is :  2.87275308430038  | validation loss is :  2.901760626222573\n",
      "Training loss after  1500  iterations is :  2.366292253003246  | validation loss is :  2.40797279389013\n",
      "Training loss after  2000  iterations is :  2.2915879663788443  | validation loss is :  2.3525078034381015\n",
      "Training loss after  2500  iterations is :  2.2537352400401094  | validation loss is :  2.3291033789138145\n",
      "Training loss after  3000  iterations is :  2.2319767849223475  | validation loss is :  2.3162176624999575\n",
      "Training loss after  3500  iterations is :  2.219059659421722  | validation loss is :  2.3080396416432523\n",
      "Training loss after  4000  iterations is :  2.2110719350020003  | validation loss is :  2.3021280571607488\n",
      "Training loss after  4500  iterations is :  2.2058817953783745  | validation loss is :  2.297438813654871\n",
      "[[-0.32063312]\n",
      " [ 0.21021524]\n",
      " [ 0.58521389]\n",
      " [ 0.99935459]\n",
      " [ 0.48572546]\n",
      " [-2.47597553]\n",
      " [-0.36878853]\n",
      " [ 2.37533922]]\n",
      "9.923040219970256\n",
      "For fold :  0 / 3\n",
      "Training loss after  0  iterations is :  10.547089964505261  | validation loss is :  10.228566039205633\n",
      "Training loss after  500  iterations is :  5.914789373837279  | validation loss is :  5.68433286292897\n",
      "Training loss after  1000  iterations is :  2.949814651543037  | validation loss is :  2.7276572889546973\n",
      "Training loss after  1500  iterations is :  2.4381278451809267  | validation loss is :  2.2716511510189985\n",
      "Training loss after  2000  iterations is :  2.369184340398316  | validation loss is :  2.2180059750116734\n",
      "Training loss after  2500  iterations is :  2.3324290443951763  | validation loss is :  2.186313793378752\n",
      "Training loss after  3000  iterations is :  2.3096929098870778  | validation loss is :  2.1672182262279236\n",
      "Training loss after  3500  iterations is :  2.295450992857291  | validation loss is :  2.15604703047909\n",
      "Training loss after  4000  iterations is :  2.2863773545790083  | validation loss is :  2.149523431628352\n",
      "Training loss after  4500  iterations is :  2.280438168224606  | validation loss is :  2.1456651881919875\n",
      "[[-0.36690255]\n",
      " [ 0.28974383]\n",
      " [ 0.78920679]\n",
      " [ 0.43907255]\n",
      " [ 0.51095182]\n",
      " [-2.48498117]\n",
      " [-0.23446634]\n",
      " [ 2.4418374 ]]\n",
      "9.974102738029936\n",
      "For fold :  1 / 3\n",
      "Training loss after  0  iterations is :  10.404468483631323  | validation loss is :  10.519788414025353\n",
      "Training loss after  500  iterations is :  5.806356176103497  | validation loss is :  5.910905852588564\n",
      "Training loss after  1000  iterations is :  2.8745089607342154  | validation loss is :  2.9525444492767265\n",
      "Training loss after  1500  iterations is :  2.3930849180706426  | validation loss is :  2.429809740623213\n",
      "Training loss after  2000  iterations is :  2.3260668607459585  | validation loss is :  2.3504736982028636\n",
      "Training loss after  2500  iterations is :  2.289725211819558  | validation loss is :  2.3088614459383154\n",
      "Training loss after  3000  iterations is :  2.267084606997099  | validation loss is :  2.2829231107632877\n",
      "Training loss after  3500  iterations is :  2.2527829531813146  | validation loss is :  2.266304919408198\n",
      "Training loss after  4000  iterations is :  2.243622925943294  | validation loss is :  2.2554394543341396\n",
      "Training loss after  4500  iterations is :  2.2376374186423673  | validation loss is :  2.248157988898319\n",
      "[[-0.31944067]\n",
      " [ 0.24976723]\n",
      " [ 0.85239573]\n",
      " [ 0.48339178]\n",
      " [ 0.48407791]\n",
      " [-2.32076778]\n",
      " [-0.35572112]\n",
      " [ 2.37354491]]\n",
      "9.920725546033133\n",
      "For fold :  2 / 3\n",
      "Training loss after  0  iterations is :  10.375199070240843  | validation loss is :  10.57543374290505\n",
      "Training loss after  500  iterations is :  5.790981004121844  | validation loss is :  5.936398891210356\n",
      "Training loss after  1000  iterations is :  2.8327561872838616  | validation loss is :  2.9881321205878106\n",
      "Training loss after  1500  iterations is :  2.3406872494563205  | validation loss is :  2.491692844358679\n",
      "Training loss after  2000  iterations is :  2.270155906123404  | validation loss is :  2.434674202448976\n",
      "Training loss after  2500  iterations is :  2.2341756331444516  | validation loss is :  2.411409071811196\n",
      "Training loss after  3000  iterations is :  2.2133332099174994  | validation loss is :  2.3980889131870344\n",
      "Training loss after  3500  iterations is :  2.200850773822896  | validation loss is :  2.388951433555854\n",
      "Training loss after  4000  iterations is :  2.193062568661466  | validation loss is :  2.381840766411389\n",
      "Training loss after  4500  iterations is :  2.1879604065243106  | validation loss is :  2.3759124473184237\n",
      "[[-0.31487513]\n",
      " [ 0.22745647]\n",
      " [ 0.60493457]\n",
      " [ 0.96638279]\n",
      " [ 0.45845772]\n",
      " [-2.42351279]\n",
      " [-0.35368644]\n",
      " [ 2.31766044]]\n",
      "9.90334904731303\n",
      "For fold :  0 / 4\n",
      "Training loss after  0  iterations is :  10.562599512608134  | validation loss is :  10.074530687946485\n",
      "Training loss after  500  iterations is :  5.928059259562661  | validation loss is :  5.578550851571309\n",
      "Training loss after  1000  iterations is :  2.9641603833203742  | validation loss is :  2.609573908420789\n",
      "Training loss after  1500  iterations is :  2.4471348992415543  | validation loss is :  2.1757413847782883\n",
      "Training loss after  2000  iterations is :  2.377912049288239  | validation loss is :  2.1312165839986523\n",
      "Training loss after  2500  iterations is :  2.3416084234836347  | validation loss is :  2.102684221304495\n",
      "Training loss after  3000  iterations is :  2.3192894167106455  | validation loss is :  2.085539224295328\n",
      "Training loss after  3500  iterations is :  2.3053397355706737  | validation loss is :  2.0758206739300626\n",
      "Training loss after  4000  iterations is :  2.296459433171271  | validation loss is :  2.0704118107926566\n",
      "Training loss after  4500  iterations is :  2.290651354513141  | validation loss is :  2.0674149146139524\n",
      "[[-0.33930235]\n",
      " [ 0.24802496]\n",
      " [ 0.79543566]\n",
      " [ 0.48117275]\n",
      " [ 0.49535735]\n",
      " [-2.4671898 ]\n",
      " [-0.24032287]\n",
      " [ 2.46525121]]\n",
      "9.978952366366284\n",
      "For fold :  1 / 4\n",
      "Training loss after  0  iterations is :  10.338289508350968  | validation loss is :  10.74459678877047\n",
      "Training loss after  500  iterations is :  5.7514843443592865  | validation loss is :  6.085687224283684\n",
      "Training loss after  1000  iterations is :  2.8123785537457153  | validation loss is :  3.1404537323992083\n",
      "Training loss after  1500  iterations is :  2.3485835862588327  | validation loss is :  2.5935184146011308\n",
      "Training loss after  2000  iterations is :  2.2853153106638704  | validation loss is :  2.4988245512259217\n",
      "Training loss after  2500  iterations is :  2.2505568526830872  | validation loss is :  2.4491701933456596\n",
      "Training loss after  3000  iterations is :  2.228766041118222  | validation loss is :  2.4172515556087606\n",
      "Training loss after  3500  iterations is :  2.2149047291153967  | validation loss is :  2.3958284590626886\n",
      "Training loss after  4000  iterations is :  2.2059551277704337  | validation loss is :  2.3810908999989833\n",
      "Training loss after  4500  iterations is :  2.2000545568087952  | validation loss is :  2.3707046102829104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34142719]\n",
      " [ 0.30680965]\n",
      " [ 0.83243282]\n",
      " [ 0.48289568]\n",
      " [ 0.47401573]\n",
      " [-2.30146753]\n",
      " [-0.34918998]\n",
      " [ 2.28069597]]\n",
      "9.890414108466695\n",
      "For fold :  2 / 4\n",
      "Training loss after  0  iterations is :  10.487053051221666  | validation loss is :  10.304418202050934\n",
      "Training loss after  500  iterations is :  5.869543571900219  | validation loss is :  5.770643470673535\n",
      "Training loss after  1000  iterations is :  2.9308591157826585  | validation loss is :  2.7822085932926472\n",
      "Training loss after  1500  iterations is :  2.426081685009485  | validation loss is :  2.2808441418860106\n",
      "Training loss after  2000  iterations is :  2.354215144975679  | validation loss is :  2.2193023133365473\n",
      "Training loss after  2500  iterations is :  2.315909443074567  | validation loss is :  2.187947316268998\n",
      "Training loss after  3000  iterations is :  2.2924158953475016  | validation loss is :  2.1699129799656545\n",
      "Training loss after  3500  iterations is :  2.2777953994857385  | validation loss is :  2.159564825192064\n",
      "Training loss after  4000  iterations is :  2.268537571672566  | validation loss is :  2.1535735184987606\n",
      "Training loss after  4500  iterations is :  2.262518050692433  | validation loss is :  2.1500212770647162\n",
      "[[-0.34389091]\n",
      " [ 0.27199031]\n",
      " [ 0.80081518]\n",
      " [ 0.46504037]\n",
      " [ 0.5367251 ]\n",
      " [-2.45952084]\n",
      " [-0.31196359]\n",
      " [ 2.45792722]]\n",
      "9.947432215593963\n",
      "For fold :  3 / 4\n",
      "Training loss after  0  iterations is :  10.3782394656382  | validation loss is :  10.632443302210111\n",
      "Training loss after  500  iterations is :  5.801356766107412  | validation loss is :  5.949681472890555\n",
      "Training loss after  1000  iterations is :  2.831672065846625  | validation loss is :  3.0178481061026785\n",
      "Training loss after  1500  iterations is :  2.337799349312521  | validation loss is :  2.5452812575917245\n",
      "Training loss after  2000  iterations is :  2.268020672160123  | validation loss is :  2.50150027563146\n",
      "Training loss after  2500  iterations is :  2.2323243839880456  | validation loss is :  2.4864059475153435\n",
      "Training loss after  3000  iterations is :  2.211561689643025  | validation loss is :  2.4777597340335857\n",
      "Training loss after  3500  iterations is :  2.199081603139748  | validation loss is :  2.47088351751258\n",
      "Training loss after  4000  iterations is :  2.1912750460371373  | validation loss is :  2.464496461995495\n",
      "Training loss after  4500  iterations is :  2.1861577520226634  | validation loss is :  2.4583831703568566\n",
      "[[-0.31477928]\n",
      " [ 0.20755382]\n",
      " [ 0.59631774]\n",
      " [ 1.04252935]\n",
      " [ 0.43061194]\n",
      " [-2.43030819]\n",
      " [-0.33980597]\n",
      " [ 2.30026099]]\n",
      "9.909683258681014\n",
      "For fold :  0 / 5\n",
      "Training loss after  0  iterations is :  10.536193679195014  | validation loss is :  10.058868640326967\n",
      "Training loss after  500  iterations is :  5.914593648217831  | validation loss is :  5.542196305798125\n",
      "Training loss after  1000  iterations is :  2.953114080809788  | validation loss is :  2.572256141257078\n",
      "Training loss after  1500  iterations is :  2.4377255113176917  | validation loss is :  2.1499187492722\n",
      "Training loss after  2000  iterations is :  2.36747464149391  | validation loss is :  2.110838624244316\n",
      "Training loss after  2500  iterations is :  2.3304950157838986  | validation loss is :  2.0858244479060684\n",
      "Training loss after  3000  iterations is :  2.3077423128304755  | validation loss is :  2.071410548036739\n",
      "Training loss after  3500  iterations is :  2.2934826468440823  | validation loss is :  2.0638838014674508\n",
      "Training loss after  4000  iterations is :  2.2843689397753018  | validation loss is :  2.060254871866351\n",
      "Training loss after  4500  iterations is :  2.2783811928207127  | validation loss is :  2.058715279605369\n",
      "[[-0.34472909]\n",
      " [ 0.24963137]\n",
      " [ 0.77799273]\n",
      " [ 0.50452633]\n",
      " [ 0.5149468 ]\n",
      " [-2.48991206]\n",
      " [-0.24721022]\n",
      " [ 2.4608556 ]]\n",
      "9.972434549177533\n",
      "For fold :  1 / 5\n",
      "Training loss after  0  iterations is :  10.404110196553857  | validation loss is :  10.5797796067284\n",
      "Training loss after  500  iterations is :  5.804745344044398  | validation loss is :  5.950638261931168\n",
      "Training loss after  1000  iterations is :  2.856429407371497  | validation loss is :  3.006657037859221\n",
      "Training loss after  1500  iterations is :  2.377261697229892  | validation loss is :  2.48369043937933\n",
      "Training loss after  2000  iterations is :  2.3122858755516758  | validation loss is :  2.3968088040860787\n",
      "Training loss after  2500  iterations is :  2.2769893101552596  | validation loss is :  2.3499124881248754\n",
      "Training loss after  3000  iterations is :  2.2550364904582714  | validation loss is :  2.3203286531467064\n",
      "Training loss after  3500  iterations is :  2.241197120136107  | validation loss is :  2.3010581474004606\n",
      "Training loss after  4000  iterations is :  2.2323297019251935  | validation loss is :  2.2881817736398586\n",
      "Training loss after  4500  iterations is :  2.22650713324406  | validation loss is :  2.2793381467285334\n",
      "[[-0.3401189 ]\n",
      " [ 0.31550044]\n",
      " [ 0.80605696]\n",
      " [ 0.46624988]\n",
      " [ 0.49302332]\n",
      " [-2.36767254]\n",
      " [-0.28839584]\n",
      " [ 2.32307418]]\n",
      "9.918429383001076\n",
      "For fold :  2 / 5\n",
      "Training loss after  0  iterations is :  10.432144588653038  | validation loss is :  10.4873462390504\n",
      "Training loss after  500  iterations is :  5.826671617203592  | validation loss is :  5.890957761511285\n",
      "Training loss after  1000  iterations is :  2.8809969388505037  | validation loss is :  2.9298634927798495\n",
      "Training loss after  1500  iterations is :  2.3907227330999326  | validation loss is :  2.418141906585698\n",
      "Training loss after  2000  iterations is :  2.3223957498951524  | validation loss is :  2.3469750619606202\n",
      "Training loss after  2500  iterations is :  2.285621872480144  | validation loss is :  2.3107808985661196\n",
      "Training loss after  3000  iterations is :  2.2628485598684414  | validation loss is :  2.288886168939682\n",
      "Training loss after  3500  iterations is :  2.248536368285107  | validation loss is :  2.275328063368999\n",
      "Training loss after  4000  iterations is :  2.2394175147514805  | validation loss is :  2.2667684028510813\n",
      "Training loss after  4500  iterations is :  2.2334932252117885  | validation loss is :  2.2612085315311026\n",
      "[[-0.34930684]\n",
      " [ 0.26253972]\n",
      " [ 0.79675543]\n",
      " [ 0.54818011]\n",
      " [ 0.46309091]\n",
      " [-2.3777386 ]\n",
      " [-0.37763629]\n",
      " [ 2.42213481]]\n",
      "9.926915835957159\n",
      "For fold :  3 / 5\n",
      "Training loss after  0  iterations is :  10.463008692446145  | validation loss is :  10.361027854590143\n",
      "Training loss after  500  iterations is :  5.846607220313525  | validation loss is :  5.819849189764595\n",
      "Training loss after  1000  iterations is :  2.8979660536723673  | validation loss is :  2.857395528007349\n",
      "Training loss after  1500  iterations is :  2.3992917598160255  | validation loss is :  2.3649143695655948\n",
      "Training loss after  2000  iterations is :  2.3295394931706017  | validation loss is :  2.30020003684904\n",
      "Training loss after  2500  iterations is :  2.2925881453062327  | validation loss is :  2.266338311963366\n",
      "Training loss after  3000  iterations is :  2.270067979589418  | validation loss is :  2.24618160937801\n",
      "Training loss after  3500  iterations is :  2.256109517390696  | validation loss is :  2.2340298236964475\n",
      "Training loss after  4000  iterations is :  2.2472923713205457  | validation loss is :  2.2265502346769486\n",
      "Training loss after  4500  iterations is :  2.241568940349864  | validation loss is :  2.2217985909210407\n",
      "[[-0.31737482]\n",
      " [ 0.28945788]\n",
      " [ 0.80262874]\n",
      " [ 0.46621819]\n",
      " [ 0.49599472]\n",
      " [-2.40855034]\n",
      " [-0.29786341]\n",
      " [ 2.42345277]]\n",
      "9.944286468947052\n",
      "For fold :  4 / 5\n",
      "Training loss after  0  iterations is :  10.373619235729842  | validation loss is :  10.713470171413764\n",
      "Training loss after  500  iterations is :  5.7978224649037156  | validation loss is :  6.007146225093482\n",
      "Training loss after  1000  iterations is :  2.836536673930241  | validation loss is :  3.063301246006613\n",
      "Training loss after  1500  iterations is :  2.346775477186754  | validation loss is :  2.5691598685060817\n",
      "Training loss after  2000  iterations is :  2.2780655087755934  | validation loss is :  2.520870276020716\n",
      "Training loss after  2500  iterations is :  2.2427499838329767  | validation loss is :  2.5050546614266125\n",
      "Training loss after  3000  iterations is :  2.222082324604247  | validation loss is :  2.496062340216713\n",
      "Training loss after  3500  iterations is :  2.2096032874773073  | validation loss is :  2.488777708099523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  4000  iterations is :  2.201777236843951  | validation loss is :  2.48191123411859\n",
      "Training loss after  4500  iterations is :  2.1966422424920315  | validation loss is :  2.475291715851035\n",
      "[[-0.32031755]\n",
      " [ 0.18470025]\n",
      " [ 0.59650919]\n",
      " [ 1.00749594]\n",
      " [ 0.46529802]\n",
      " [-2.40835015]\n",
      " [-0.33707837]\n",
      " [ 2.30728225]]\n",
      "9.900688143127944\n",
      "For fold :  0 / 6\n",
      "Training loss after  0  iterations is :  10.50492978894445  | validation loss is :  10.124512192023712\n",
      "Training loss after  500  iterations is :  5.884543468934973  | validation loss is :  5.617557216715893\n",
      "Training loss after  1000  iterations is :  2.9268508450934028  | validation loss is :  2.657035418474632\n",
      "Training loss after  1500  iterations is :  2.4204037838017274  | validation loss is :  2.2159375797239003\n",
      "Training loss after  2000  iterations is :  2.3517249382154644  | validation loss is :  2.165877730377751\n",
      "Training loss after  2500  iterations is :  2.3155535285146804  | validation loss is :  2.1350940073347373\n",
      "Training loss after  3000  iterations is :  2.2933221951040568  | validation loss is :  2.1163039169580222\n",
      "Training loss after  3500  iterations is :  2.279386892860996  | validation loss is :  2.105228715143866\n",
      "Training loss after  4000  iterations is :  2.2704725247600535  | validation loss is :  2.0987423140271284\n",
      "Training loss after  4500  iterations is :  2.2646093048486935  | validation loss is :  2.094928436387326\n",
      "[[-0.33643666]\n",
      " [ 0.24013624]\n",
      " [ 0.79219525]\n",
      " [ 0.52283509]\n",
      " [ 0.49914607]\n",
      " [-2.44435499]\n",
      " [-0.26811151]\n",
      " [ 2.41925218]]\n",
      "9.95864943860239\n",
      "For fold :  1 / 6\n",
      "Training loss after  0  iterations is :  10.463915740501609  | validation loss is :  10.331571967050552\n",
      "Training loss after  500  iterations is :  5.855736379445467  | validation loss is :  5.742785507113453\n",
      "Training loss after  1000  iterations is :  2.8983145311191243  | validation loss is :  2.8013257990705585\n",
      "Training loss after  1500  iterations is :  2.4026853185593486  | validation loss is :  2.329747059936415\n",
      "Training loss after  2000  iterations is :  2.334848464519615  | validation loss is :  2.2667875335066214\n",
      "Training loss after  2500  iterations is :  2.2982521587211493  | validation loss is :  2.232709512422514\n",
      "Training loss after  3000  iterations is :  2.2755800898382117  | validation loss is :  2.2127633264284867\n",
      "Training loss after  3500  iterations is :  2.2613604403428664  | validation loss is :  2.201151808699942\n",
      "Training loss after  4000  iterations is :  2.2523046846357246  | validation loss is :  2.1943380513275588\n",
      "Training loss after  4500  iterations is :  2.246397687898924  | validation loss is :  2.1902560888027476\n",
      "[[-0.36059323]\n",
      " [ 0.31714096]\n",
      " [ 0.76245195]\n",
      " [ 0.48336974]\n",
      " [ 0.49889915]\n",
      " [-2.44124961]\n",
      " [-0.28041353]\n",
      " [ 2.41499007]]\n",
      "9.940330943772429\n",
      "For fold :  2 / 6\n",
      "Training loss after  0  iterations is :  10.372703914137855  | validation loss is :  10.778064142816495\n",
      "Training loss after  500  iterations is :  5.780111716979117  | validation loss is :  6.118453188151544\n",
      "Training loss after  1000  iterations is :  2.8398948052130346  | validation loss is :  3.158906287904618\n",
      "Training loss after  1500  iterations is :  2.3660910560476256  | validation loss is :  2.5954017815694947\n",
      "Training loss after  2000  iterations is :  2.3016649802402647  | validation loss is :  2.4972289948375592\n",
      "Training loss after  2500  iterations is :  2.266910663971152  | validation loss is :  2.445869394099987\n",
      "Training loss after  3000  iterations is :  2.2452963748637313  | validation loss is :  2.412812214080382\n",
      "Training loss after  3500  iterations is :  2.2316088216292678  | validation loss is :  2.390599014775668\n",
      "Training loss after  4000  iterations is :  2.22279814146689  | validation loss is :  2.3753013158433056\n",
      "Training loss after  4500  iterations is :  2.217003892197751  | validation loss is :  2.3645030799628928\n",
      "[[-0.31995356]\n",
      " [ 0.26476884]\n",
      " [ 0.83102193]\n",
      " [ 0.5170714 ]\n",
      " [ 0.463438  ]\n",
      " [-2.30568282]\n",
      " [-0.34411047]\n",
      " [ 2.32811394]]\n",
      "9.90325034396185\n",
      "For fold :  3 / 6\n",
      "Training loss after  0  iterations is :  10.480274796780812  | validation loss is :  10.255009989515246\n",
      "Training loss after  500  iterations is :  5.87060204690261  | validation loss is :  5.6977311071009655\n",
      "Training loss after  1000  iterations is :  2.9219379839900363  | validation loss is :  2.717235008040425\n",
      "Training loss after  1500  iterations is :  2.4185835104043294  | validation loss is :  2.2430066240049045\n",
      "Training loss after  2000  iterations is :  2.3482685611293097  | validation loss is :  2.189436123344218\n",
      "Training loss after  2500  iterations is :  2.310784155933811  | validation loss is :  2.1609933689854994\n",
      "Training loss after  3000  iterations is :  2.2877233299197424  | validation loss is :  2.1446870610496935\n",
      "Training loss after  3500  iterations is :  2.2733210720061474  | validation loss is :  2.135565057514258\n",
      "Training loss after  4000  iterations is :  2.264177066136494  | validation loss is :  2.1305023462751875\n",
      "Training loss after  4500  iterations is :  2.258228140464877  | validation loss is :  2.127675480970925\n",
      "[[-0.34193608]\n",
      " [ 0.25817286]\n",
      " [ 0.78000325]\n",
      " [ 0.52579476]\n",
      " [ 0.51115265]\n",
      " [-2.44276357]\n",
      " [-0.30459106]\n",
      " [ 2.44201838]]\n",
      "9.951032980158539\n",
      "For fold :  4 / 6\n",
      "Training loss after  0  iterations is :  10.456496335194371  | validation loss is :  10.365448795668819\n",
      "Training loss after  500  iterations is :  5.838202487911202  | validation loss is :  5.8375911425899725\n",
      "Training loss after  1000  iterations is :  2.8873857776661835  | validation loss is :  2.887724191864057\n",
      "Training loss after  1500  iterations is :  2.3925223040232155  | validation loss is :  2.3964221764500366\n",
      "Training loss after  2000  iterations is :  2.324184742831647  | validation loss is :  2.3282458082777917\n",
      "Training loss after  2500  iterations is :  2.287825717293873  | validation loss is :  2.2921995261742216\n",
      "Training loss after  3000  iterations is :  2.2655261631822983  | validation loss is :  2.2706333652479307\n",
      "Training loss after  3500  iterations is :  2.251614306097721  | validation loss is :  2.257548871651478\n",
      "Training loss after  4000  iterations is :  2.2427732757727084  | validation loss is :  2.249453258216604\n",
      "Training loss after  4500  iterations is :  2.2370046938231445  | validation loss is :  2.244299824264819\n",
      "[[-0.32697204]\n",
      " [ 0.30272391]\n",
      " [ 0.79370379]\n",
      " [ 0.4737551 ]\n",
      " [ 0.47423839]\n",
      " [-2.42133978]\n",
      " [-0.27750748]\n",
      " [ 2.39914251]]\n",
      "9.93625435529068\n",
      "For fold :  5 / 6\n",
      "Training loss after  0  iterations is :  10.373249748503483  | validation loss is :  10.78132964738196\n",
      "Training loss after  500  iterations is :  5.801364565525534  | validation loss is :  6.0384966099941275\n",
      "Training loss after  1000  iterations is :  2.8417560540816647  | validation loss is :  3.08959036241128\n",
      "Training loss after  1500  iterations is :  2.34834202387672  | validation loss is :  2.6007903952204092\n",
      "Training loss after  2000  iterations is :  2.278018159723339  | validation loss is :  2.5643512755675095\n",
      "Training loss after  2500  iterations is :  2.2420434209229434  | validation loss is :  2.5576283762918197\n",
      "Training loss after  3000  iterations is :  2.2211364742856405  | validation loss is :  2.554386663528544\n",
      "Training loss after  3500  iterations is :  2.208594461763842  | validation loss is :  2.550333448077814\n",
      "Training loss after  4000  iterations is :  2.200771632137431  | validation loss is :  2.5450172421981336\n",
      "Training loss after  4500  iterations is :  2.195659937390138  | validation loss is :  2.538901533979535\n",
      "[[-0.3270137 ]\n",
      " [ 0.18614111]\n",
      " [ 0.59437858]\n",
      " [ 1.03871682]\n",
      " [ 0.47344375]\n",
      " [-2.41191184]\n",
      " [-0.37461085]\n",
      " [ 2.31296493]]\n",
      "9.90466764218851\n",
      "For fold :  0 / 7\n",
      "Training loss after  0  iterations is :  10.500362558436718  | validation loss is :  10.079882283322439\n",
      "Training loss after  500  iterations is :  5.877352426207264  | validation loss is :  5.593848460930736\n",
      "Training loss after  1000  iterations is :  2.9186189682392785  | validation loss is :  2.6456426318697774\n",
      "Training loss after  1500  iterations is :  2.416103593825833  | validation loss is :  2.213481087120038\n",
      "Training loss after  2000  iterations is :  2.349163681617916  | validation loss is :  2.1582726638219234\n",
      "Training loss after  2500  iterations is :  2.314033708079615  | validation loss is :  2.1211899752711787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  3000  iterations is :  2.2924615036167504  | validation loss is :  2.096908736753503\n",
      "Training loss after  3500  iterations is :  2.2789454908797286  | validation loss is :  2.0813261065129467\n",
      "Training loss after  4000  iterations is :  2.2703056170136087  | validation loss is :  2.0712127141808576\n",
      "Training loss after  4500  iterations is :  2.264631999856739  | validation loss is :  2.0645019581690147\n",
      "[[-0.34095013]\n",
      " [ 0.24275765]\n",
      " [ 0.79002244]\n",
      " [ 0.52463159]\n",
      " [ 0.48449798]\n",
      " [-2.40922758]\n",
      " [-0.2716634 ]\n",
      " [ 2.38606445]]\n",
      "9.956669214578014\n",
      "For fold :  1 / 7\n",
      "Training loss after  0  iterations is :  10.489433739398555  | validation loss is :  10.15228013533456\n",
      "Training loss after  500  iterations is :  5.88564927794801  | validation loss is :  5.5697870950785004\n",
      "Training loss after  1000  iterations is :  2.927187243818813  | validation loss is :  2.600217549706887\n",
      "Training loss after  1500  iterations is :  2.4179265550542155  | validation loss is :  2.17294188158565\n",
      "Training loss after  2000  iterations is :  2.34618079735246  | validation loss is :  2.140245120353837\n",
      "Training loss after  2500  iterations is :  2.3078042211541447  | validation loss is :  2.124697298955154\n",
      "Training loss after  3000  iterations is :  2.2842016967989016  | validation loss is :  2.1186131120228415\n",
      "Training loss after  3500  iterations is :  2.2694800976504723  | validation loss is :  2.117706282810882\n",
      "Training loss after  4000  iterations is :  2.260140635656767  | validation loss is :  2.1191595451536425\n",
      "Training loss after  4500  iterations is :  2.254059333678761  | validation loss is :  2.1214600858047428\n",
      "[[-0.35009702]\n",
      " [ 0.29777883]\n",
      " [ 0.77813383]\n",
      " [ 0.49671957]\n",
      " [ 0.51233549]\n",
      " [-2.50086806]\n",
      " [-0.29285409]\n",
      " [ 2.49008483]]\n",
      "9.953476048258315\n",
      "For fold :  2 / 7\n",
      "Training loss after  0  iterations is :  10.361838662852893  | validation loss is :  10.914170949585612\n",
      "Training loss after  500  iterations is :  5.768040346748113  | validation loss is :  6.2348381905807795\n",
      "Training loss after  1000  iterations is :  2.82327961210823  | validation loss is :  3.281837717251466\n",
      "Training loss after  1500  iterations is :  2.3531563308386465  | validation loss is :  2.700168272013289\n",
      "Training loss after  2000  iterations is :  2.289439938930812  | validation loss is :  2.593749909001017\n",
      "Training loss after  2500  iterations is :  2.2549883181131665  | validation loss is :  2.539225039075414\n",
      "Training loss after  3000  iterations is :  2.233648253407587  | validation loss is :  2.5042630344673062\n",
      "Training loss after  3500  iterations is :  2.2201955715995267  | validation loss is :  2.4806163899948968\n",
      "Training loss after  4000  iterations is :  2.2115557444948295  | validation loss is :  2.4641638937248382\n",
      "Training loss after  4500  iterations is :  2.20586284730906  | validation loss is :  2.452429147520072\n",
      "[[-0.34885549]\n",
      " [ 0.31060299]\n",
      " [ 0.79970891]\n",
      " [ 0.49310753]\n",
      " [ 0.4911733 ]\n",
      " [-2.34135016]\n",
      " [-0.31626912]\n",
      " [ 2.29241781]]\n",
      "9.893099482860123\n",
      "For fold :  3 / 7\n",
      "Training loss after  0  iterations is :  10.451179762311087  | validation loss is :  10.395210564705447\n",
      "Training loss after  500  iterations is :  5.843602784587941  | validation loss is :  5.827701701091139\n",
      "Training loss after  1000  iterations is :  2.903108657450443  | validation loss is :  2.839756092178936\n",
      "Training loss after  1500  iterations is :  2.412041630144828  | validation loss is :  2.3157249745830577\n",
      "Training loss after  2000  iterations is :  2.3449581967646926  | validation loss is :  2.2371568175293635\n",
      "Training loss after  2500  iterations is :  2.308813561259769  | validation loss is :  2.1944827889427025\n",
      "Training loss after  3000  iterations is :  2.286216296130225  | validation loss is :  2.168030229674086\n",
      "Training loss after  3500  iterations is :  2.2718650304846295  | validation loss is :  2.1514334995244324\n",
      "Training loss after  4000  iterations is :  2.26261806850737  | validation loss is :  2.140870861934586\n",
      "Training loss after  4500  iterations is :  2.2565392014072576  | validation loss is :  2.133992184612102\n",
      "[[-0.32726319]\n",
      " [ 0.23975099]\n",
      " [ 0.78695142]\n",
      " [ 0.55160459]\n",
      " [ 0.48557468]\n",
      " [-2.37078404]\n",
      " [-0.32862153]\n",
      " [ 2.3956319 ]]\n",
      "9.936681161862243\n",
      "For fold :  4 / 7\n",
      "Training loss after  0  iterations is :  10.475186556296498  | validation loss is :  10.233597122572009\n",
      "Training loss after  500  iterations is :  5.861646220473779  | validation loss is :  5.716255489900897\n",
      "Training loss after  1000  iterations is :  2.9127477708969924  | validation loss is :  2.7363358907946456\n",
      "Training loss after  1500  iterations is :  2.4093071890400273  | validation loss is :  2.2648779907795418\n",
      "Training loss after  2000  iterations is :  2.3381715558517153  | validation loss is :  2.2154748557044663\n",
      "Training loss after  2500  iterations is :  2.3003809967579163  | validation loss is :  2.191064079107692\n",
      "Training loss after  3000  iterations is :  2.2772916765033817  | validation loss is :  2.177987323396616\n",
      "Training loss after  3500  iterations is :  2.2629623839675217  | validation loss is :  2.1713153024196945\n",
      "Training loss after  4000  iterations is :  2.253915623566094  | validation loss is :  2.1680719973648315\n",
      "Training loss after  4500  iterations is :  2.2480585623061624  | validation loss is :  2.166581438547938\n",
      "[[-0.32992771]\n",
      " [ 0.26711002]\n",
      " [ 0.76423621]\n",
      " [ 0.51207518]\n",
      " [ 0.50696207]\n",
      " [-2.4365364 ]\n",
      " [-0.30633922]\n",
      " [ 2.47190817]]\n",
      "9.947234876832484\n",
      "For fold :  5 / 7\n",
      "Training loss after  0  iterations is :  10.42449133332346  | validation loss is :  10.544329561714214\n",
      "Training loss after  500  iterations is :  5.821615230582936  | validation loss is :  5.945222535974019\n",
      "Training loss after  1000  iterations is :  2.871834918941597  | validation loss is :  2.999416241755766\n",
      "Training loss after  1500  iterations is :  2.381513983277551  | validation loss is :  2.4869720445008374\n",
      "Training loss after  2000  iterations is :  2.313248116616565  | validation loss is :  2.410555687647106\n",
      "Training loss after  2500  iterations is :  2.27654310878383  | validation loss is :  2.3717143851098488\n",
      "Training loss after  3000  iterations is :  2.253891074297  | validation loss is :  2.3488127249286155\n",
      "Training loss after  3500  iterations is :  2.2396878329641208  | validation loss is :  2.3350482824389003\n",
      "Training loss after  4000  iterations is :  2.23062866869138  | validation loss is :  2.3266349198804783\n",
      "Training loss after  4500  iterations is :  2.2247072060894877  | validation loss is :  2.3213645764410584\n",
      "[[-0.32704438]\n",
      " [ 0.29130216]\n",
      " [ 0.79013184]\n",
      " [ 0.48138029]\n",
      " [ 0.46948648]\n",
      " [-2.40758154]\n",
      " [-0.27555097]\n",
      " [ 2.40549662]]\n",
      "9.929777882349137\n",
      "For fold :  6 / 7\n",
      "Training loss after  0  iterations is :  10.391753667505473  | validation loss is :  10.744828629712025\n",
      "Training loss after  500  iterations is :  5.813906560526323  | validation loss is :  5.992698927899992\n",
      "Training loss after  1000  iterations is :  2.8462780857017225  | validation loss is :  3.0709665222100835\n",
      "Training loss after  1500  iterations is :  2.351051109419394  | validation loss is :  2.6133694340243268\n",
      "Training loss after  2000  iterations is :  2.282186642691783  | validation loss is :  2.589226666543871\n",
      "Training loss after  2500  iterations is :  2.2473823502032864  | validation loss is :  2.5882553864799713\n",
      "Training loss after  3000  iterations is :  2.2272846410033678  | validation loss is :  2.587392094093617\n",
      "Training loss after  3500  iterations is :  2.2152761925535636  | validation loss is :  2.5836810561686754\n",
      "Training loss after  4000  iterations is :  2.2077980278993588  | validation loss is :  2.5775600616881853\n",
      "Training loss after  4500  iterations is :  2.20290826402613  | validation loss is :  2.5700741041407182\n",
      "[[-0.32342598]\n",
      " [ 0.19094872]\n",
      " [ 0.60861289]\n",
      " [ 1.06495059]\n",
      " [ 0.45683506]\n",
      " [-2.41433401]\n",
      " [-0.36200429]\n",
      " [ 2.26870406]]\n",
      "9.910048382414283\n",
      "For fold :  0 / 8\n",
      "Training loss after  0  iterations is :  10.503867602064357  | validation loss is :  10.007181329471328\n",
      "Training loss after  500  iterations is :  5.880281165404852  | validation loss is :  5.553245434619833\n",
      "Training loss after  1000  iterations is :  2.9257882834131212  | validation loss is :  2.5792581316286523\n",
      "Training loss after  1500  iterations is :  2.422720665051332  | validation loss is :  2.1432453726130443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  2000  iterations is :  2.3555419666438544  | validation loss is :  2.090843587496163\n",
      "Training loss after  2500  iterations is :  2.3203114623284624  | validation loss is :  2.055189187485759\n",
      "Training loss after  3000  iterations is :  2.298657519581208  | validation loss is :  2.0317740945534473\n",
      "Training loss after  3500  iterations is :  2.2850681881399986  | validation loss is :  2.0167870248408635\n",
      "Training loss after  4000  iterations is :  2.2763617019416027  | validation loss is :  2.007120027569671\n",
      "Training loss after  4500  iterations is :  2.270627572719627  | validation loss is :  2.000769204885973\n",
      "[[-0.34025774]\n",
      " [ 0.22955324]\n",
      " [ 0.79304418]\n",
      " [ 0.54413177]\n",
      " [ 0.49314426]\n",
      " [-2.40690669]\n",
      " [-0.29089712]\n",
      " [ 2.38902663]]\n",
      "9.95564143547721\n",
      "For fold :  1 / 8\n",
      "Training loss after  0  iterations is :  10.485034242218678  | validation loss is :  10.14143278850831\n",
      "Training loss after  500  iterations is :  5.875860929145306  | validation loss is :  5.586763011896462\n",
      "Training loss after  1000  iterations is :  2.915399799925749  | validation loss is :  2.63902654068125\n",
      "Training loss after  1500  iterations is :  2.410582262981943  | validation loss is :  2.212756113927932\n",
      "Training loss after  2000  iterations is :  2.3408652365391394  | validation loss is :  2.170756129164005\n",
      "Training loss after  2500  iterations is :  2.3036825451458562  | validation loss is :  2.147977510159812\n",
      "Training loss after  3000  iterations is :  2.2807988998388797  | validation loss is :  2.1365274413508155\n",
      "Training loss after  3500  iterations is :  2.266503815419608  | validation loss is :  2.131638157961878\n",
      "Training loss after  4000  iterations is :  2.257423775496291  | validation loss is :  2.1301227408848136\n",
      "Training loss after  4500  iterations is :  2.2515118064513033  | validation loss is :  2.130192027857693\n",
      "[[-0.33455695]\n",
      " [ 0.29050245]\n",
      " [ 0.76565644]\n",
      " [ 0.49743689]\n",
      " [ 0.49308961]\n",
      " [-2.46593651]\n",
      " [-0.26066859]\n",
      " [ 2.46787223]]\n",
      "9.949944943535185\n",
      "For fold :  2 / 8\n",
      "Training loss after  0  iterations is :  10.405177805179731  | validation loss is :  10.703062905334882\n",
      "Training loss after  500  iterations is :  5.813864126073402  | validation loss is :  6.022547050826049\n",
      "Training loss after  1000  iterations is :  2.868372098980188  | validation loss is :  3.0663800874265164\n",
      "Training loss after  1500  iterations is :  2.3839588802339815  | validation loss is :  2.5234681321635986\n",
      "Training loss after  2000  iterations is :  2.3169626771097995  | validation loss is :  2.4327731250390308\n",
      "Training loss after  2500  iterations is :  2.2805646819808127  | validation loss is :  2.3854749277858884\n",
      "Training loss after  3000  iterations is :  2.2578641823771237  | validation loss is :  2.3563368429823255\n",
      "Training loss after  3500  iterations is :  2.24348196497723  | validation loss is :  2.337843762963827\n",
      "Training loss after  4000  iterations is :  2.234216099209098  | validation loss is :  2.3258596166978474\n",
      "Training loss after  4500  iterations is :  2.2281031274864254  | validation loss is :  2.317906783787517\n",
      "[[-0.33780887]\n",
      " [ 0.31606834]\n",
      " [ 0.77312498]\n",
      " [ 0.50745598]\n",
      " [ 0.51369556]\n",
      " [-2.40060565]\n",
      " [-0.30521835]\n",
      " [ 2.34411691]]\n",
      "9.926740261978036\n",
      "For fold :  3 / 8\n",
      "Training loss after  0  iterations is :  10.391198448745193  | validation loss is :  10.78597073753484\n",
      "Training loss after  500  iterations is :  5.791462609225373  | validation loss is :  6.138013454058757\n",
      "Training loss after  1000  iterations is :  2.8431112874451645  | validation loss is :  3.1874236282821973\n",
      "Training loss after  1500  iterations is :  2.3651304553122663  | validation loss is :  2.6260190850332137\n",
      "Training loss after  2000  iterations is :  2.3002550406780333  | validation loss is :  2.530815335498158\n",
      "Training loss after  2500  iterations is :  2.26546810457173  | validation loss is :  2.4823178477491004\n",
      "Training loss after  3000  iterations is :  2.24400714837106  | validation loss is :  2.4513728002320323\n",
      "Training loss after  3500  iterations is :  2.2305359512389096  | validation loss is :  2.430593256618559\n",
      "Training loss after  4000  iterations is :  2.221938286826739  | validation loss is :  2.416226169348088\n",
      "Training loss after  4500  iterations is :  2.2163258311748595  | validation loss is :  2.406004767572512\n",
      "[[-0.33877274]\n",
      " [ 0.25518738]\n",
      " [ 0.8163593 ]\n",
      " [ 0.53297175]\n",
      " [ 0.45484324]\n",
      " [-2.33049028]\n",
      " [-0.34366663]\n",
      " [ 2.36092457]]\n",
      "9.903377074712164\n",
      "For fold :  4 / 8\n",
      "Training loss after  0  iterations is :  10.472102603194683  | validation loss is :  10.226271478524831\n",
      "Training loss after  500  iterations is :  5.868020459856023  | validation loss is :  5.643338899414698\n",
      "Training loss after  1000  iterations is :  2.9129864536948338  | validation loss is :  2.6856016687853765\n",
      "Training loss after  1500  iterations is :  2.411712072821227  | validation loss is :  2.2377838691633207\n",
      "Training loss after  2000  iterations is :  2.342758926155494  | validation loss is :  2.187072527054047\n",
      "Training loss after  2500  iterations is :  2.3059505549165884  | validation loss is :  2.157730010024736\n",
      "Training loss after  3000  iterations is :  2.283211821271828  | validation loss is :  2.1402429978116935\n",
      "Training loss after  3500  iterations is :  2.268939973150189  | validation loss is :  2.1300895301732097\n",
      "Training loss after  4000  iterations is :  2.259834387927558  | validation loss is :  2.124188422241123\n",
      "Training loss after  4500  iterations is :  2.253886657437859  | validation loss is :  2.1206955198698\n",
      "[[-0.3464233 ]\n",
      " [ 0.27323363]\n",
      " [ 0.77104408]\n",
      " [ 0.54961304]\n",
      " [ 0.50529121]\n",
      " [-2.4356205 ]\n",
      " [-0.32148275]\n",
      " [ 2.41549157]]\n",
      "9.950652093968076\n",
      "For fold :  5 / 8\n",
      "Training loss after  0  iterations is :  10.450186873637723  | validation loss is :  10.381976719923177\n",
      "Training loss after  500  iterations is :  5.836502801617262  | validation loss is :  5.889363742821493\n",
      "Training loss after  1000  iterations is :  2.8985174919342285  | validation loss is :  2.8704621567443787\n",
      "Training loss after  1500  iterations is :  2.4028861448882597  | validation loss is :  2.3277073272359936\n",
      "Training loss after  2000  iterations is :  2.332843191766565  | validation loss is :  2.25560094137331\n",
      "Training loss after  2500  iterations is :  2.2956270061719164  | validation loss is :  2.221828515039188\n",
      "Training loss after  3000  iterations is :  2.272874151985953  | validation loss is :  2.202707394042457\n",
      "Training loss after  3500  iterations is :  2.258710887820777  | validation loss is :  2.1915949302339683\n",
      "Training loss after  4000  iterations is :  2.249717602015618  | validation loss is :  2.185004121342217\n",
      "Training loss after  4500  iterations is :  2.2438442165471764  | validation loss is :  2.1809803051522785\n",
      "[[-0.33248361]\n",
      " [ 0.27003884]\n",
      " [ 0.79343861]\n",
      " [ 0.47485503]\n",
      " [ 0.51596895]\n",
      " [-2.42832516]\n",
      " [-0.29417788]\n",
      " [ 2.43357678]]\n",
      "9.926700716538257\n",
      "For fold :  6 / 8\n",
      "Training loss after  0  iterations is :  10.445313378975216  | validation loss is :  10.420654409113482\n",
      "Training loss after  500  iterations is :  5.840380471357305  | validation loss is :  5.8236135652764505\n",
      "Training loss after  1000  iterations is :  2.883325306014414  | validation loss is :  2.8997864803929283\n",
      "Training loss after  1500  iterations is :  2.387914626288941  | validation loss is :  2.429457127768647\n",
      "Training loss after  2000  iterations is :  2.3189474659383245  | validation loss is :  2.367009707316366\n",
      "Training loss after  2500  iterations is :  2.2817695247299854  | validation loss is :  2.3348551211943964\n",
      "Training loss after  3000  iterations is :  2.258770110673573  | validation loss is :  2.316835061305046\n",
      "Training loss after  3500  iterations is :  2.2443268668220098  | validation loss is :  2.3069356316430545\n",
      "Training loss after  4000  iterations is :  2.235106174040843  | validation loss is :  2.301596073008489\n",
      "Training loss after  4500  iterations is :  2.2290767983577213  | validation loss is :  2.2987613941575815\n",
      "[[-0.32755773]\n",
      " [ 0.27704603]\n",
      " [ 0.7833754 ]\n",
      " [ 0.50362043]\n",
      " [ 0.4673141 ]\n",
      " [-2.42693041]\n",
      " [-0.28370065]\n",
      " [ 2.43688786]]\n",
      "9.940459178286005\n",
      "For fold :  7 / 8\n",
      "Training loss after  0  iterations is :  10.38430935010645  | validation loss is :  10.840095148809766\n",
      "Training loss after  500  iterations is :  5.805933685326715  | validation loss is :  6.0701909892920085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  1000  iterations is :  2.8430305234782693  | validation loss is :  3.1392929048819425\n",
      "Training loss after  1500  iterations is :  2.34971768609708  | validation loss is :  2.66762840226546\n",
      "Training loss after  2000  iterations is :  2.2807366705979417  | validation loss is :  2.6452881628107203\n",
      "Training loss after  2500  iterations is :  2.2458349747697555  | validation loss is :  2.6473221932269047\n",
      "Training loss after  3000  iterations is :  2.225667763935977  | validation loss is :  2.6479560781707523\n",
      "Training loss after  3500  iterations is :  2.2136059831727093  | validation loss is :  2.644539703738982\n",
      "Training loss after  4000  iterations is :  2.2060882789090344  | validation loss is :  2.6380025221348666\n",
      "Training loss after  4500  iterations is :  2.201171480891758  | validation loss is :  2.629740127935213\n",
      "[[-0.32498795]\n",
      " [ 0.20181965]\n",
      " [ 0.59240095]\n",
      " [ 1.06648229]\n",
      " [ 0.45709722]\n",
      " [-2.41136275]\n",
      " [-0.3581011 ]\n",
      " [ 2.26477793]]\n",
      "9.905768806086199\n",
      "For fold :  0 / 9\n",
      "Training loss after  0  iterations is :  10.495690817392795  | validation loss is :  10.006678804161508\n",
      "Training loss after  500  iterations is :  5.877590595807602  | validation loss is :  5.524025229216823\n",
      "Training loss after  1000  iterations is :  2.9192547725082822  | validation loss is :  2.5697030022344234\n",
      "Training loss after  1500  iterations is :  2.4166372062285855  | validation loss is :  2.1511306039272147\n",
      "Training loss after  2000  iterations is :  2.349400565488533  | validation loss is :  2.1025595224009845\n",
      "Training loss after  2500  iterations is :  2.3140395215132212  | validation loss is :  2.0691333051605296\n",
      "Training loss after  3000  iterations is :  2.2922855777848365  | validation loss is :  2.0475831834937694\n",
      "Training loss after  3500  iterations is :  2.2786246054747243  | validation loss is :  2.0341643861629723\n",
      "Training loss after  4000  iterations is :  2.2698698424905897  | validation loss is :  2.025790048040321\n",
      "Training loss after  4500  iterations is :  2.2641059088853623  | validation loss is :  2.020489048717524\n",
      "[[-0.3436252 ]\n",
      " [ 0.23119114]\n",
      " [ 0.80068068]\n",
      " [ 0.54046389]\n",
      " [ 0.48424218]\n",
      " [-2.41859611]\n",
      " [-0.2871358 ]\n",
      " [ 2.39615866]]\n",
      "9.95287216630232\n",
      "For fold :  1 / 9\n",
      "Training loss after  0  iterations is :  10.482421671861411  | validation loss is :  10.12114123999858\n",
      "Training loss after  500  iterations is :  5.872624762142161  | validation loss is :  5.586302384822332\n",
      "Training loss after  1000  iterations is :  2.91535386801541  | validation loss is :  2.624067477267852\n",
      "Training loss after  1500  iterations is :  2.4098262198305207  | validation loss is :  2.1984478097937012\n",
      "Training loss after  2000  iterations is :  2.339003946237325  | validation loss is :  2.1633221307169155\n",
      "Training loss after  2500  iterations is :  2.301096037281252  | validation loss is :  2.1461101853354325\n",
      "Training loss after  3000  iterations is :  2.2777204580790786  | validation loss is :  2.1389826101549803\n",
      "Training loss after  3500  iterations is :  2.2630817594144226  | validation loss is :  2.137488644069836\n",
      "Training loss after  4000  iterations is :  2.253755404165603  | validation loss is :  2.1386736950990803\n",
      "Training loss after  4500  iterations is :  2.2476616448001008  | validation loss is :  2.1409158778824233\n",
      "[[-0.33338634]\n",
      " [ 0.27578789]\n",
      " [ 0.75442194]\n",
      " [ 0.52017855]\n",
      " [ 0.51513973]\n",
      " [-2.48900702]\n",
      " [-0.25500367]\n",
      " [ 2.4664526 ]]\n",
      "9.950740945162226\n",
      "For fold :  2 / 9\n",
      "Training loss after  0  iterations is :  10.428401097368102  | validation loss is :  10.549861087498604\n",
      "Training loss after  500  iterations is :  5.8258599428620474  | validation loss is :  5.923045781270919\n",
      "Training loss after  1000  iterations is :  2.8737540065159206  | validation loss is :  2.984558240317908\n",
      "Training loss after  1500  iterations is :  2.386948258955606  | validation loss is :  2.4669533548630946\n",
      "Training loss after  2000  iterations is :  2.320932237860682  | validation loss is :  2.3824916219087613\n",
      "Training loss after  2500  iterations is :  2.2854998677773937  | validation loss is :  2.336875271924228\n",
      "Training loss after  3000  iterations is :  2.263623134271767  | validation loss is :  2.3082508672926583\n",
      "Training loss after  3500  iterations is :  2.2499071692082824  | validation loss is :  2.289685391224567\n",
      "Training loss after  4000  iterations is :  2.2411583811441314  | validation loss is :  2.2772938022626454\n",
      "Training loss after  4500  iterations is :  2.2354361757967074  | validation loss is :  2.268761126019071\n",
      "[[-0.35526656]\n",
      " [ 0.3181456 ]\n",
      " [ 0.76571997]\n",
      " [ 0.50335844]\n",
      " [ 0.48876227]\n",
      " [-2.38394148]\n",
      " [-0.31365277]\n",
      " [ 2.36537325]]\n",
      "9.9254533042135\n",
      "For fold :  3 / 9\n",
      "Training loss after  0  iterations is :  10.37448884826815  | validation loss is :  10.971063035910925\n",
      "Training loss after  500  iterations is :  5.791917024623119  | validation loss is :  6.221690153834014\n",
      "Training loss after  1000  iterations is :  2.852829764979613  | validation loss is :  3.2234394145104526\n",
      "Training loss after  1500  iterations is :  2.3738777322003664  | validation loss is :  2.6204666837014705\n",
      "Training loss after  2000  iterations is :  2.3077024709121563  | validation loss is :  2.516333477233946\n",
      "Training loss after  2500  iterations is :  2.2720426280843746  | validation loss is :  2.465678134625343\n",
      "Training loss after  3000  iterations is :  2.2499395947325525  | validation loss is :  2.4343257874333517\n",
      "Training loss after  3500  iterations is :  2.235987557393358  | validation loss is :  2.413835345818684\n",
      "Training loss after  4000  iterations is :  2.2270231707154156  | validation loss is :  2.400059353093167\n",
      "Training loss after  4500  iterations is :  2.221124550710081  | validation loss is :  2.3905428887866105\n",
      "[[-0.3316213 ]\n",
      " [ 0.28941007]\n",
      " [ 0.80647586]\n",
      " [ 0.51245502]\n",
      " [ 0.4893337 ]\n",
      " [-2.36858815]\n",
      " [-0.32412245]\n",
      " [ 2.3477114 ]]\n",
      "9.903876701246311\n",
      "For fold :  4 / 9\n",
      "Training loss after  0  iterations is :  10.477832009612738  | validation loss is :  10.159079513013182\n",
      "Training loss after  500  iterations is :  5.854072892548274  | validation loss is :  5.712443204045881\n",
      "Training loss after  1000  iterations is :  2.901846447463039  | validation loss is :  2.7548980191076393\n",
      "Training loss after  1500  iterations is :  2.4061127930912236  | validation loss is :  2.2818462594285847\n",
      "Training loss after  2000  iterations is :  2.3391795304461622  | validation loss is :  2.2167505370625595\n",
      "Training loss after  2500  iterations is :  2.3037361523807185  | validation loss is :  2.177975921008709\n",
      "Training loss after  3000  iterations is :  2.281942241932135  | validation loss is :  2.152926579829676\n",
      "Training loss after  3500  iterations is :  2.268306315420976  | validation loss is :  2.1366063563479045\n",
      "Training loss after  4000  iterations is :  2.259626506085013  | validation loss is :  2.1257234424339813\n",
      "Training loss after  4500  iterations is :  2.2539684847558332  | validation loss is :  2.118229826931076\n",
      "[[-0.33563864]\n",
      " [ 0.23633273]\n",
      " [ 0.78663086]\n",
      " [ 0.54472451]\n",
      " [ 0.48121432]\n",
      " [-2.37123771]\n",
      " [-0.32337725]\n",
      " [ 2.40369965]]\n",
      "9.94187640366625\n",
      "For fold :  5 / 9\n",
      "Training loss after  0  iterations is :  10.446834515190048  | validation loss is :  10.41281537999839\n",
      "Training loss after  500  iterations is :  5.847355663492927  | validation loss is :  5.792388736904047\n",
      "Training loss after  1000  iterations is :  2.8968769530704552  | validation loss is :  2.8296646838296504\n",
      "Training loss after  1500  iterations is :  2.3992061208764075  | validation loss is :  2.348309531174412\n",
      "Training loss after  2000  iterations is :  2.3295964974343177  | validation loss is :  2.289136796085092\n",
      "Training loss after  2500  iterations is :  2.292318159011335  | validation loss is :  2.258658647790964\n",
      "Training loss after  3000  iterations is :  2.2693244478977865  | validation loss is :  2.2411837429228045\n",
      "Training loss after  3500  iterations is :  2.2549101889143994  | validation loss is :  2.2312812472318826\n",
      "Training loss after  4000  iterations is :  2.2457231933841513  | validation loss is :  2.2256883322796153\n",
      "Training loss after  4500  iterations is :  2.2397281801442612  | validation loss is :  2.2225003985592613\n",
      "[[-0.32964643]\n",
      " [ 0.27038718]\n",
      " [ 0.77505858]\n",
      " [ 0.54083692]\n",
      " [ 0.49615498]\n",
      " [-2.42635607]\n",
      " [-0.30219173]\n",
      " [ 2.42421852]]\n",
      "9.942880498118534\n",
      "For fold :  6 / 9\n",
      "Training loss after  0  iterations is :  10.446937692019244  | validation loss is :  10.413332799829265\n",
      "Training loss after  500  iterations is :  5.835725568976923  | validation loss is :  5.900714054815291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  1000  iterations is :  2.8967617337578457  | validation loss is :  2.882456836008062\n",
      "Training loss after  1500  iterations is :  2.401063833105391  | validation loss is :  2.3422807356546294\n",
      "Training loss after  2000  iterations is :  2.3313450150085733  | validation loss is :  2.2708787291334245\n",
      "Training loss after  2500  iterations is :  2.2943905224830172  | validation loss is :  2.2372124488624134\n",
      "Training loss after  3000  iterations is :  2.271771808670392  | validation loss is :  2.218096320598236\n",
      "Training loss after  3500  iterations is :  2.2576483427596075  | validation loss is :  2.207020294405029\n",
      "Training loss after  4000  iterations is :  2.2486431080016684  | validation loss is :  2.200527155669309\n",
      "Training loss after  4500  iterations is :  2.2427348638550058  | validation loss is :  2.1966590721330093\n",
      "[[-0.33224262]\n",
      " [ 0.28281444]\n",
      " [ 0.79341512]\n",
      " [ 0.49940115]\n",
      " [ 0.52377293]\n",
      " [-2.42236664]\n",
      " [-0.32952947]\n",
      " [ 2.40571387]]\n",
      "9.929844828361745\n",
      "For fold :  7 / 9\n",
      "Training loss after  0  iterations is :  10.430080557528788  | validation loss is :  10.538926203235384\n",
      "Training loss after  500  iterations is :  5.826736183315014  | validation loss is :  5.917094823425072\n",
      "Training loss after  1000  iterations is :  2.8673343539144  | validation loss is :  3.009305833616822\n",
      "Training loss after  1500  iterations is :  2.3773760574383602  | validation loss is :  2.5208835072339233\n",
      "Training loss after  2000  iterations is :  2.3094790304416692  | validation loss is :  2.4502912594693527\n",
      "Training loss after  2500  iterations is :  2.2726157584068534  | validation loss is :  2.4156378755866426\n",
      "Training loss after  3000  iterations is :  2.249745684001898  | validation loss is :  2.396428051164499\n",
      "Training loss after  3500  iterations is :  2.235371032582175  | validation loss is :  2.385808118621039\n",
      "Training loss after  4000  iterations is :  2.2262031078066182  | validation loss is :  2.379986052091576\n",
      "Training loss after  4500  iterations is :  2.2202267726708667  | validation loss is :  2.3768013808623367\n",
      "[[-0.33611075]\n",
      " [ 0.28207237]\n",
      " [ 0.76963113]\n",
      " [ 0.50449796]\n",
      " [ 0.44909066]\n",
      " [-2.42737664]\n",
      " [-0.25552726]\n",
      " [ 2.43275711]]\n",
      "9.929074241261954\n",
      "For fold :  8 / 9\n",
      "Training loss after  0  iterations is :  10.400570897792102  | validation loss is :  10.7709299057455\n",
      "Training loss after  500  iterations is :  5.820593353303008  | validation loss is :  5.992324407117232\n",
      "Training loss after  1000  iterations is :  2.855245451288227  | validation loss is :  3.0786656185980217\n",
      "Training loss after  1500  iterations is :  2.3580181784693357  | validation loss is :  2.6384887482145287\n",
      "Training loss after  2000  iterations is :  2.2890916539078967  | validation loss is :  2.6268769189646877\n",
      "Training loss after  2500  iterations is :  2.2544913049671576  | validation loss is :  2.6326195605715514\n",
      "Training loss after  3000  iterations is :  2.2345844563471338  | validation loss is :  2.6344037448163493\n",
      "Training loss after  3500  iterations is :  2.222720662144743  | validation loss is :  2.6308041266119466\n",
      "Training loss after  4000  iterations is :  2.215345611523234  | validation loss is :  2.6233798787405376\n",
      "Training loss after  4500  iterations is :  2.2105285449989713  | validation loss is :  2.613911817735407\n",
      "[[-0.32246046]\n",
      " [ 0.19331041]\n",
      " [ 0.59559005]\n",
      " [ 1.06123881]\n",
      " [ 0.463884  ]\n",
      " [-2.39023383]\n",
      " [-0.37236657]\n",
      " [ 2.26904505]]\n",
      "9.917682565434875\n",
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  10.486870576920118  | validation loss is :  10.035787282465531\n",
      "Training loss after  500  iterations is :  5.86907393260698  | validation loss is :  5.558509690298923\n",
      "Training loss after  1000  iterations is :  2.9117640760436343  | validation loss is :  2.6091901742753114\n",
      "Training loss after  1500  iterations is :  2.411476545074841  | validation loss is :  2.1811086835804305\n",
      "Training loss after  2000  iterations is :  2.3443196764251732  | validation loss is :  2.129056984798654\n",
      "Training loss after  2500  iterations is :  2.308898726845898  | validation loss is :  2.094707801195554\n",
      "Training loss after  3000  iterations is :  2.287083200981841  | validation loss is :  2.072691222910684\n",
      "Training loss after  3500  iterations is :  2.273365979989512  | validation loss is :  2.0589457299117515\n",
      "Training loss after  4000  iterations is :  2.264562722960866  | validation loss is :  2.0503270070950363\n",
      "Training loss after  4500  iterations is :  2.2587581739034395  | validation loss is :  2.0448400602357637\n",
      "[[-0.34481579]\n",
      " [ 0.23900213]\n",
      " [ 0.78961662]\n",
      " [ 0.54593682]\n",
      " [ 0.49972155]\n",
      " [-2.40964471]\n",
      " [-0.31118653]\n",
      " [ 2.39908766]]\n",
      "9.947522735683288\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  10.481705596353182  | validation loss is :  10.082037592391158\n",
      "Training loss after  500  iterations is :  5.877165936797628  | validation loss is :  5.512810730706732\n",
      "Training loss after  1000  iterations is :  2.921388110629869  | validation loss is :  2.535791392206774\n",
      "Training loss after  1500  iterations is :  2.4146311193094947  | validation loss is :  2.1250177168401776\n",
      "Training loss after  2000  iterations is :  2.3439598561541075  | validation loss is :  2.095266267980043\n",
      "Training loss after  2500  iterations is :  2.3063979874725353  | validation loss is :  2.0788592848378404\n",
      "Training loss after  3000  iterations is :  2.2833213090920643  | validation loss is :  2.0717132329328565\n",
      "Training loss after  3500  iterations is :  2.268903540043426  | validation loss is :  2.070028211659999\n",
      "Training loss after  4000  iterations is :  2.2597311306917707  | validation loss is :  2.0709923452179964\n",
      "Training loss after  4500  iterations is :  2.2537427972063586  | validation loss is :  2.073029976012584\n",
      "[[-0.33476601]\n",
      " [ 0.28247899]\n",
      " [ 0.75434582]\n",
      " [ 0.51652948]\n",
      " [ 0.50362021]\n",
      " [-2.48427063]\n",
      " [-0.24577794]\n",
      " [ 2.45418907]]\n",
      "9.953308969540608\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  10.443555680042602  | validation loss is :  10.40268366038925\n",
      "Training loss after  500  iterations is :  5.836585881826906  | validation loss is :  5.828692467839856\n",
      "Training loss after  1000  iterations is :  2.884572407692349  | validation loss is :  2.876299654117402\n",
      "Training loss after  1500  iterations is :  2.3920547778323225  | validation loss is :  2.3822262285466285\n",
      "Training loss after  2000  iterations is :  2.324143477780035  | validation loss is :  2.315840002528866\n",
      "Training loss after  2500  iterations is :  2.287865058116803  | validation loss is :  2.281789887044549\n",
      "Training loss after  3000  iterations is :  2.265591157090005  | validation loss is :  2.2615490859823693\n",
      "Training loss after  3500  iterations is :  2.2516990646677155  | validation loss is :  2.2492335021184835\n",
      "Training loss after  4000  iterations is :  2.2428822459045525  | validation loss is :  2.241530318027634\n",
      "Training loss after  4500  iterations is :  2.2371431723338686  | validation loss is :  2.2365254026871244\n",
      "[[-0.34345608]\n",
      " [ 0.28943598]\n",
      " [ 0.79188393]\n",
      " [ 0.51160972]\n",
      " [ 0.47949738]\n",
      " [-2.40664801]\n",
      " [-0.31375175]\n",
      " [ 2.4002976 ]]\n",
      "9.931454985338592\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  10.409599696092537  | validation loss is :  10.739894028388138\n",
      "Training loss after  500  iterations is :  5.815671884994044  | validation loss is :  6.053870419235388\n",
      "Training loss after  1000  iterations is :  2.8681193102214917  | validation loss is :  3.100695939849918\n",
      "Training loss after  1500  iterations is :  2.3855105599144406  | validation loss is :  2.540500800277251\n",
      "Training loss after  2000  iterations is :  2.320520078632222  | validation loss is :  2.4346755429217275\n",
      "Training loss after  2500  iterations is :  2.2854339515971103  | validation loss is :  2.376696717519253\n",
      "Training loss after  3000  iterations is :  2.2635481550143672  | validation loss is :  2.3396893243555277\n",
      "Training loss after  3500  iterations is :  2.2496605782248182  | validation loss is :  2.3152829097571024\n",
      "Training loss after  4000  iterations is :  2.240691208653811  | validation loss is :  2.2988183047238633\n",
      "Training loss after  4500  iterations is :  2.2347547922201425  | validation loss is :  2.2874567182687766\n",
      "[[-0.33351234]\n",
      " [ 0.29933752]\n",
      " [ 0.78018692]\n",
      " [ 0.50894946]\n",
      " [ 0.50560326]\n",
      " [-2.377008  ]\n",
      " [-0.28178364]\n",
      " [ 2.32167975]]\n",
      "9.923377043198116\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  10.40397435011291  | validation loss is :  10.78534844162182\n",
      "Training loss after  500  iterations is :  5.800506930427566  | validation loss is :  6.158170612279852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  1000  iterations is :  2.8515719087823155  | validation loss is :  3.2091569325949196\n",
      "Training loss after  1500  iterations is :  2.3701310109371594  | validation loss is :  2.6464799237408103\n",
      "Training loss after  2000  iterations is :  2.304359133987599  | validation loss is :  2.5517450369230823\n",
      "Training loss after  2500  iterations is :  2.2690655589454707  | validation loss is :  2.5036145808261354\n",
      "Training loss after  3000  iterations is :  2.2472508184767275  | validation loss is :  2.4730144623862755\n",
      "Training loss after  3500  iterations is :  2.233528047529832  | validation loss is :  2.4526088545168934\n",
      "Training loss after  4000  iterations is :  2.224754774942791  | validation loss is :  2.4386228596088895\n",
      "Training loss after  4500  iterations is :  2.2190235872771855  | validation loss is :  2.4287563807959978\n",
      "[[-0.33303679]\n",
      " [ 0.25957287]\n",
      " [ 0.79031697]\n",
      " [ 0.54938413]\n",
      " [ 0.46499602]\n",
      " [-2.33673194]\n",
      " [-0.35705867]\n",
      " [ 2.38249216]]\n",
      "9.911920135911746\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  10.468597711831103  | validation loss is :  10.20403362196898\n",
      "Training loss after  500  iterations is :  5.86291946925238  | validation loss is :  5.639164704286458\n",
      "Training loss after  1000  iterations is :  2.9122099825317007  | validation loss is :  2.652673971760259\n",
      "Training loss after  1500  iterations is :  2.4101286693135417  | validation loss is :  2.2007857418544488\n",
      "Training loss after  2000  iterations is :  2.340102676602759  | validation loss is :  2.158526220086554\n",
      "Training loss after  2500  iterations is :  2.302808731410538  | validation loss is :  2.1363747493633145\n",
      "Training loss after  3000  iterations is :  2.279859639479235  | validation loss is :  2.1243265965470455\n",
      "Training loss after  3500  iterations is :  2.265501798744617  | validation loss is :  2.1182187573641356\n",
      "Training loss after  4000  iterations is :  2.256365859290145  | validation loss is :  2.1153204702117296\n",
      "Training loss after  4500  iterations is :  2.2504114497012306  | validation loss is :  2.114062146035216\n",
      "[[-0.35368064]\n",
      " [ 0.27013653]\n",
      " [ 0.77382669]\n",
      " [ 0.55523497]\n",
      " [ 0.49176766]\n",
      " [-2.45041335]\n",
      " [-0.31927166]\n",
      " [ 2.42804329]]\n",
      "9.945917257710356\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  10.461757825345039  | validation loss is :  10.269276195192214\n",
      "Training loss after  500  iterations is :  5.848077926156333  | validation loss is :  5.771310371639907\n",
      "Training loss after  1000  iterations is :  2.8981212786853834  | validation loss is :  2.797691448643031\n",
      "Training loss after  1500  iterations is :  2.401117909658005  | validation loss is :  2.307886355544695\n",
      "Training loss after  2000  iterations is :  2.3321081768254466  | validation loss is :  2.246815288186379\n",
      "Training loss after  2500  iterations is :  2.2953576937573246  | validation loss is :  2.2160833407272538\n",
      "Training loss after  3000  iterations is :  2.272812069110694  | validation loss is :  2.198813389921191\n",
      "Training loss after  3500  iterations is :  2.2587499962589983  | validation loss is :  2.1891888883209685\n",
      "Training loss after  4000  iterations is :  2.2498211271808195  | validation loss is :  2.183831982212573\n",
      "Training loss after  4500  iterations is :  2.2440052292705945  | validation loss is :  2.180815161942584\n",
      "[[-0.32548126]\n",
      " [ 0.26361875]\n",
      " [ 0.76149524]\n",
      " [ 0.50898293]\n",
      " [ 0.51154557]\n",
      " [-2.41122095]\n",
      " [-0.29531126]\n",
      " [ 2.44128554]]\n",
      "9.938019341330797\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  10.440459433754144  | validation loss is :  10.464740386980772\n",
      "Training loss after  500  iterations is :  5.836422722271372  | validation loss is :  5.870505995342452\n",
      "Training loss after  1000  iterations is :  2.885682973875023  | validation loss is :  2.9181313138323453\n",
      "Training loss after  1500  iterations is :  2.3908045500076995  | validation loss is :  2.4259728392004236\n",
      "Training loss after  2000  iterations is :  2.322423511422818  | validation loss is :  2.3584812531411323\n",
      "Training loss after  2500  iterations is :  2.286224962603316  | validation loss is :  2.321762499887433\n",
      "Training loss after  3000  iterations is :  2.2640787637883126  | validation loss is :  2.2986993471139443\n",
      "Training loss after  3500  iterations is :  2.2502582450893733  | validation loss is :  2.283915196902531\n",
      "Training loss after  4000  iterations is :  2.2414578149113136  | validation loss is :  2.2742195410168025\n",
      "Training loss after  4500  iterations is :  2.2357001012869278  | validation loss is :  2.2676798450582463\n",
      "[[-0.33052936]\n",
      " [ 0.29386201]\n",
      " [ 0.80694339]\n",
      " [ 0.5180274 ]\n",
      " [ 0.47432041]\n",
      " [-2.41307998]\n",
      " [-0.30320538]\n",
      " [ 2.37387173]]\n",
      "9.93703097680214\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  10.42861386893045  | validation loss is :  10.570648471721583\n",
      "Training loss after  500  iterations is :  5.829597321164224  | validation loss is :  5.93632721852108\n",
      "Training loss after  1000  iterations is :  2.8823528699789063  | validation loss is :  2.960440904080159\n",
      "Training loss after  1500  iterations is :  2.3930933668213905  | validation loss is :  2.4185713343115056\n",
      "Training loss after  2000  iterations is :  2.3249802974943803  | validation loss is :  2.3372327019306214\n",
      "Training loss after  2500  iterations is :  2.2878036584583983  | validation loss is :  2.300772594676139\n",
      "Training loss after  3000  iterations is :  2.2645269677240307  | validation loss is :  2.2823952214529917\n",
      "Training loss after  3500  iterations is :  2.249745261255558  | validation loss is :  2.2735814051137835\n",
      "Training loss after  4000  iterations is :  2.2402105635225786  | validation loss is :  2.269867943628408\n",
      "Training loss after  4500  iterations is :  2.233918139509878  | validation loss is :  2.268788929534471\n",
      "[[-0.33445247]\n",
      " [ 0.26079245]\n",
      " [ 0.77722383]\n",
      " [ 0.50830628]\n",
      " [ 0.48955368]\n",
      " [-2.44087248]\n",
      " [-0.26618869]\n",
      " [ 2.43226447]]\n",
      "9.922443647729951\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  10.398691427070192  | validation loss is :  10.831165619685407\n",
      "Training loss after  500  iterations is :  5.813772054240874  | validation loss is :  6.062394608595482\n",
      "Training loss after  1000  iterations is :  2.847179656852017  | validation loss is :  3.161628901612178\n",
      "Training loss after  1500  iterations is :  2.3513847086054755  | validation loss is :  2.721627077759142\n",
      "Training loss after  2000  iterations is :  2.2828839422967038  | validation loss is :  2.7123508764370876\n",
      "Training loss after  2500  iterations is :  2.2487639840970326  | validation loss is :  2.718925557223615\n",
      "Training loss after  3000  iterations is :  2.229289298903231  | validation loss is :  2.7196278968660095\n",
      "Training loss after  3500  iterations is :  2.217757680047901  | validation loss is :  2.713975405992899\n",
      "Training loss after  4000  iterations is :  2.2106232569275504  | validation loss is :  2.7041949450855953\n",
      "Training loss after  4500  iterations is :  2.2059777824915074  | validation loss is :  2.692421742377851\n",
      "[[-0.32299233]\n",
      " [ 0.19375098]\n",
      " [ 0.5909261 ]\n",
      " [ 1.0567095 ]\n",
      " [ 0.46303537]\n",
      " [-2.37136893]\n",
      " [-0.37794965]\n",
      " [ 2.26406495]]\n",
      "9.914418861848038\n",
      "{2: (2.229849698620617, 2.2556926183537986), 3: (2.2314885848865598, 2.2524277996500284), 4: (2.2309664698155514, 2.257347751227494), 5: (2.2314455100122847, 2.255293611505139), 6: (2.232600791143001, 2.256092005783986), 7: (2.2327921822046193, 2.2575755910303568), 8: (2.232910815564624, 2.2566897090788074), 9: (2.2333792985994214, 2.2570576995427825), 10: (2.2334369389349833, 2.255565748030461)}\n"
     ]
    }
   ],
   "source": [
    "k_models = {}\n",
    "for i in range(2,11):\n",
    "    models_rmse,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = i, epochs = 5000, learning_rate = 0.01, loss = \"rmse\")\n",
    "#     print(\"The average loss with k = \", i, \" is \", \" train loss = \", avg_train_loss[-1], \"\")\n",
    "    k_models[i] = (avg_train_loss[-1], avg_val_loss[-1])\n",
    "print(k_models)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average loss with k =  2  is   train loss =  2.229849698620617  validation loss =  2.2556926183537986\n",
      "The average loss with k =  3  is   train loss =  2.2314885848865598  validation loss =  2.2524277996500284\n",
      "The average loss with k =  4  is   train loss =  2.2309664698155514  validation loss =  2.257347751227494\n",
      "The average loss with k =  5  is   train loss =  2.2314455100122847  validation loss =  2.255293611505139\n",
      "The average loss with k =  6  is   train loss =  2.232600791143001  validation loss =  2.256092005783986\n",
      "The average loss with k =  7  is   train loss =  2.2327921822046193  validation loss =  2.2575755910303568\n",
      "The average loss with k =  8  is   train loss =  2.232910815564624  validation loss =  2.2566897090788074\n",
      "The average loss with k =  9  is   train loss =  2.2333792985994214  validation loss =  2.2570576995427825\n",
      "The average loss with k =  10  is   train loss =  2.2334369389349833  validation loss =  2.255565748030461\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,11):\n",
    "    print(\"The average loss with k = \", i, \" is \", \" train loss = \", k_models[i][0], \" validation loss = \", k_models[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  9.973922299095264  | validation loss is :  9.56115107913669\n",
      "Training loss after  500  iterations is :  4.992397534114976  | validation loss is :  4.628441281937107\n",
      "Training loss after  1000  iterations is :  1.8479146662305792  | validation loss is :  1.7009201472033393\n",
      "Training loss after  1500  iterations is :  1.6970923622974468  | validation loss is :  1.5888658381947858\n",
      "Training loss after  2000  iterations is :  1.649486910808258  | validation loss is :  1.5493801299052479\n",
      "Training loss after  2500  iterations is :  1.6239091596853774  | validation loss is :  1.5278217559700904\n",
      "Training loss after  3000  iterations is :  1.607215313767489  | validation loss is :  1.5152855268097898\n",
      "Training loss after  3500  iterations is :  1.5956880927951018  | validation loss is :  1.505956604476291\n",
      "Training loss after  4000  iterations is :  1.588098165855292  | validation loss is :  1.500195137805582\n",
      "Training loss after  4500  iterations is :  1.5828384691861033  | validation loss is :  1.4968401008788519\n",
      "[[-0.32421138]\n",
      " [ 0.24925473]\n",
      " [ 0.60628164]\n",
      " [ 0.82328699]\n",
      " [ 0.21051633]\n",
      " [-1.76089092]\n",
      " [-0.18016992]\n",
      " [ 1.77100926]]\n",
      "9.44606705694524\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  9.960883448642894  | validation loss is :  9.676258992805755\n",
      "Training loss after  500  iterations is :  4.998690824608461  | validation loss is :  4.727409383752707\n",
      "Training loss after  1000  iterations is :  1.8624935521814567  | validation loss is :  1.628984548567385\n",
      "Training loss after  1500  iterations is :  1.707236772817976  | validation loss is :  1.492962840919338\n",
      "Training loss after  2000  iterations is :  1.6576353306331295  | validation loss is :  1.4654947408083225\n",
      "Training loss after  2500  iterations is :  1.6311088551892576  | validation loss is :  1.4507318644288043\n",
      "Training loss after  3000  iterations is :  1.614396972400704  | validation loss is :  1.4403388985431091\n",
      "Training loss after  3500  iterations is :  1.603074870154494  | validation loss is :  1.4326899409138971\n",
      "Training loss after  4000  iterations is :  1.5952162361957951  | validation loss is :  1.4291944076107446\n",
      "Training loss after  4500  iterations is :  1.5899684572690325  | validation loss is :  1.4266664403558267\n",
      "[[-0.32259108]\n",
      " [ 0.28514143]\n",
      " [ 0.5926179 ]\n",
      " [ 0.79253135]\n",
      " [ 0.22045466]\n",
      " [-1.78637502]\n",
      " [-0.1500573 ]\n",
      " [ 1.78042046]]\n",
      "9.450510910058322\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  9.935870143693453  | validation loss is :  9.880095923261392\n",
      "Training loss after  500  iterations is :  4.971698158137106  | validation loss is :  4.926728805452566\n",
      "Training loss after  1000  iterations is :  1.83826514370204  | validation loss is :  1.8256536492348823\n",
      "Training loss after  1500  iterations is :  1.684249881798103  | validation loss is :  1.697416878780035\n",
      "Training loss after  2000  iterations is :  1.6372695053934663  | validation loss is :  1.6572241211560739\n",
      "Training loss after  2500  iterations is :  1.6114549682745754  | validation loss is :  1.6372715275488834\n",
      "Training loss after  3000  iterations is :  1.595117798337889  | validation loss is :  1.6223808740503685\n",
      "Training loss after  3500  iterations is :  1.583680266890891  | validation loss is :  1.6109657471186307\n",
      "Training loss after  4000  iterations is :  1.576374716028785  | validation loss is :  1.6027822503249418\n",
      "Training loss after  4500  iterations is :  1.5713118146179366  | validation loss is :  1.59681814231793\n",
      "[[-0.33611869]\n",
      " [ 0.29509989]\n",
      " [ 0.59397288]\n",
      " [ 0.78575119]\n",
      " [ 0.19735246]\n",
      " [-1.72541221]\n",
      " [-0.19984799]\n",
      " [ 1.76900163]]\n",
      "9.438637573177271\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  9.905268759978712  | validation loss is :  10.184652278177458\n",
      "Training loss after  500  iterations is :  4.963945400316948  | validation loss is :  5.234697072992123\n",
      "Training loss after  1000  iterations is :  1.834419674623325  | validation loss is :  1.9430129616216858\n",
      "Training loss after  1500  iterations is :  1.6839076283856451  | validation loss is :  1.7639242642304074\n",
      "Training loss after  2000  iterations is :  1.6385953791215002  | validation loss is :  1.7009592312057422\n",
      "Training loss after  2500  iterations is :  1.614133131396665  | validation loss is :  1.6645959072740053\n",
      "Training loss after  3000  iterations is :  1.5982463350284362  | validation loss is :  1.6396583407278924\n",
      "Training loss after  3500  iterations is :  1.5866093328096866  | validation loss is :  1.6216137593345414\n",
      "Training loss after  4000  iterations is :  1.5786231718601478  | validation loss is :  1.610738071461687\n",
      "Training loss after  4500  iterations is :  1.5727366304478412  | validation loss is :  1.6029097003279897\n",
      "[[-0.32350367]\n",
      " [ 0.30242507]\n",
      " [ 0.62529257]\n",
      " [ 0.75880654]\n",
      " [ 0.19938754]\n",
      " [-1.75388973]\n",
      " [-0.1405929 ]\n",
      " [ 1.70828325]]\n",
      "9.432139435869926\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  9.90367216604577  | validation loss is :  10.194244604316546\n",
      "Training loss after  500  iterations is :  4.950653652863134  | validation loss is :  5.232407201365766\n",
      "Training loss after  1000  iterations is :  1.817943952877791  | validation loss is :  2.0605045197398737\n",
      "Training loss after  1500  iterations is :  1.671339187382314  | validation loss is :  1.875390440443844\n",
      "Training loss after  2000  iterations is :  1.6264455634426263  | validation loss is :  1.8145462541601778\n",
      "Training loss after  2500  iterations is :  1.6027461263554448  | validation loss is :  1.777482471206337\n",
      "Training loss after  3000  iterations is :  1.587262201564244  | validation loss is :  1.7512403942179477\n",
      "Training loss after  3500  iterations is :  1.5756725312379392  | validation loss is :  1.7327668562714713\n",
      "Training loss after  4000  iterations is :  1.5679177499505796  | validation loss is :  1.7208126896387277\n",
      "Training loss after  4500  iterations is :  1.5625575817346442  | validation loss is :  1.7117716356233805\n",
      "[[-0.31480985]\n",
      " [ 0.25649094]\n",
      " [ 0.61761626]\n",
      " [ 0.82638517]\n",
      " [ 0.19545862]\n",
      " [-1.66641565]\n",
      " [-0.23522292]\n",
      " [ 1.71737308]]\n",
      "9.420255455029016\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  9.952368281000533  | validation loss is :  9.752997601918466\n",
      "Training loss after  500  iterations is :  4.9859044789423805  | validation loss is :  4.79229501641971\n",
      "Training loss after  1000  iterations is :  1.8514547813870519  | validation loss is :  1.728493411358325\n",
      "Training loss after  1500  iterations is :  1.6983609721025452  | validation loss is :  1.5795443399607285\n",
      "Training loss after  2000  iterations is :  1.6492121851412276  | validation loss is :  1.5492573553769629\n",
      "Training loss after  2500  iterations is :  1.622978045628332  | validation loss is :  1.5330975537667895\n",
      "Training loss after  3000  iterations is :  1.6060697491575195  | validation loss is :  1.5233878301862012\n",
      "Training loss after  3500  iterations is :  1.5941842348476658  | validation loss is :  1.5158888448284054\n",
      "Training loss after  4000  iterations is :  1.586568435753774  | validation loss is :  1.511127174419457\n",
      "Training loss after  4500  iterations is :  1.5812691842848425  | validation loss is :  1.5084797932584786\n",
      "[[-0.3376122 ]\n",
      " [ 0.28276729]\n",
      " [ 0.56898115]\n",
      " [ 0.82871733]\n",
      " [ 0.21443447]\n",
      " [-1.76619063]\n",
      " [-0.18876077]\n",
      " [ 1.77188446]]\n",
      "9.439686003193149\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  9.948642895156999  | validation loss is :  9.788968824940047\n",
      "Training loss after  500  iterations is :  4.974155977269197  | validation loss is :  4.848585136974253\n",
      "Training loss after  1000  iterations is :  1.8468838527368283  | validation loss is :  1.782635790723942\n",
      "Training loss after  1500  iterations is :  1.694194713439581  | validation loss is :  1.6423562252251884\n",
      "Training loss after  2000  iterations is :  1.6463561800558202  | validation loss is :  1.5958453498906913\n",
      "Training loss after  2500  iterations is :  1.6195605442110939  | validation loss is :  1.5744629914556525\n",
      "Training loss after  3000  iterations is :  1.6019335753848496  | validation loss is :  1.5645983968378474\n",
      "Training loss after  3500  iterations is :  1.5900427198666016  | validation loss is :  1.5572588480341394\n",
      "Training loss after  4000  iterations is :  1.5818297665831018  | validation loss is :  1.5538945351061046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  4500  iterations is :  1.5762217968392496  | validation loss is :  1.5520675298683624\n",
      "[[-0.33017278]\n",
      " [ 0.27297914]\n",
      " [ 0.58418844]\n",
      " [ 0.76848373]\n",
      " [ 0.23094129]\n",
      " [-1.76571289]\n",
      " [-0.17223156]\n",
      " [ 1.80345188]]\n",
      "9.454821713677445\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  9.934007450771688  | validation loss is :  9.92326139088729\n",
      "Training loss after  500  iterations is :  4.970610767614121  | validation loss is :  4.96476254128554\n",
      "Training loss after  1000  iterations is :  1.8461177953403802  | validation loss is :  1.8330276127018232\n",
      "Training loss after  1500  iterations is :  1.6904545107997704  | validation loss is :  1.6846444104538407\n",
      "Training loss after  2000  iterations is :  1.6427148233726274  | validation loss is :  1.639891495937965\n",
      "Training loss after  2500  iterations is :  1.617768420371997  | validation loss is :  1.6127265241093658\n",
      "Training loss after  3000  iterations is :  1.6017876431013593  | validation loss is :  1.5926915558669883\n",
      "Training loss after  3500  iterations is :  1.590474006509995  | validation loss is :  1.5769785493721193\n",
      "Training loss after  4000  iterations is :  1.5834892274940797  | validation loss is :  1.5649574217033138\n",
      "Training loss after  4500  iterations is :  1.578525715802294  | validation loss is :  1.5554694704615486\n",
      "[[-0.31949624]\n",
      " [ 0.30962446]\n",
      " [ 0.63876187]\n",
      " [ 0.77521276]\n",
      " [ 0.1968717 ]\n",
      " [-1.73390986]\n",
      " [-0.1804209 ]\n",
      " [ 1.70516632]]\n",
      "9.44129856306533\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  9.918839808408729  | validation loss is :  10.059952038369305\n",
      "Training loss after  500  iterations is :  4.969475538347089  | validation loss is :  5.094655869632586\n",
      "Training loss after  1000  iterations is :  1.8329119934277749  | validation loss is :  1.914434263966283\n",
      "Training loss after  1500  iterations is :  1.6864617215465096  | validation loss is :  1.74530612379885\n",
      "Training loss after  2000  iterations is :  1.6422658107921848  | validation loss is :  1.6756939258885593\n",
      "Training loss after  2500  iterations is :  1.6171481724625976  | validation loss is :  1.6388860555967577\n",
      "Training loss after  3000  iterations is :  1.6009640357389376  | validation loss is :  1.6154032172107609\n",
      "Training loss after  3500  iterations is :  1.5888186848767025  | validation loss is :  1.6007130604888602\n",
      "Training loss after  4000  iterations is :  1.5807537453146234  | validation loss is :  1.5916352025898495\n",
      "Training loss after  4500  iterations is :  1.5746541090401227  | validation loss is :  1.5855985982842686\n",
      "[[-0.31430645]\n",
      " [ 0.27893338]\n",
      " [ 0.61357699]\n",
      " [ 0.77100655]\n",
      " [ 0.18955165]\n",
      " [-1.7584044 ]\n",
      " [-0.12072763]\n",
      " [ 1.72982968]]\n",
      "9.422692921767085\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  9.892229909526344  | validation loss is :  10.29736211031175\n",
      "Training loss after  500  iterations is :  4.970236205661091  | validation loss is :  5.331864338576637\n",
      "Training loss after  1000  iterations is :  1.8221746709486244  | validation loss is :  2.000212043190252\n",
      "Training loss after  1500  iterations is :  1.670635560079633  | validation loss is :  1.8356311591223238\n",
      "Training loss after  2000  iterations is :  1.6246299734378755  | validation loss is :  1.7908696362495988\n",
      "Training loss after  2500  iterations is :  1.598508444750589  | validation loss is :  1.7714049767633506\n",
      "Training loss after  3000  iterations is :  1.5815041030544772  | validation loss is :  1.7612571170892217\n",
      "Training loss after  3500  iterations is :  1.5694198341882981  | validation loss is :  1.7540205457922786\n",
      "Training loss after  4000  iterations is :  1.5619575326067485  | validation loss is :  1.748720364970306\n",
      "Training loss after  4500  iterations is :  1.5569626451601715  | validation loss is :  1.7444247546012237\n",
      "[[-0.30531297]\n",
      " [ 0.29034785]\n",
      " [ 0.55781546]\n",
      " [ 0.8846345 ]\n",
      " [ 0.20562381]\n",
      " [-1.74577713]\n",
      " [-0.20112433]\n",
      " [ 1.73945082]]\n",
      "9.430463012240425\n"
     ]
    }
   ],
   "source": [
    "models_mae,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = 10, epochs = 5000, learning_rate = 0.01, loss = \"mae\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x for x in range(len(models_mae[1][2]))],models_mae[1][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(models_mae[1][3]))], models_mae[1][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  10.486870576920118  | validation loss is :  10.035787282465531\n",
      "Training loss after  500  iterations is :  5.86907393260698  | validation loss is :  5.558509690298923\n",
      "Training loss after  1000  iterations is :  2.9117640760436343  | validation loss is :  2.6091901742753114\n",
      "Training loss after  1500  iterations is :  2.411476545074841  | validation loss is :  2.1811086835804305\n",
      "Training loss after  2000  iterations is :  2.3443196764251732  | validation loss is :  2.129056984798654\n",
      "Training loss after  2500  iterations is :  2.308898726845898  | validation loss is :  2.094707801195554\n",
      "Training loss after  3000  iterations is :  2.287083200981841  | validation loss is :  2.072691222910684\n",
      "Training loss after  3500  iterations is :  2.273365979989512  | validation loss is :  2.0589457299117515\n",
      "Training loss after  4000  iterations is :  2.264562722960866  | validation loss is :  2.0503270070950363\n",
      "Training loss after  4500  iterations is :  2.2587581739034395  | validation loss is :  2.0448400602357637\n",
      "[[-0.34481579]\n",
      " [ 0.23900213]\n",
      " [ 0.78961662]\n",
      " [ 0.54593682]\n",
      " [ 0.49972155]\n",
      " [-2.40964471]\n",
      " [-0.31118653]\n",
      " [ 2.39908766]]\n",
      "9.947522735683288\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  10.481705596353182  | validation loss is :  10.082037592391158\n",
      "Training loss after  500  iterations is :  5.877165936797628  | validation loss is :  5.512810730706732\n",
      "Training loss after  1000  iterations is :  2.921388110629869  | validation loss is :  2.535791392206774\n",
      "Training loss after  1500  iterations is :  2.4146311193094947  | validation loss is :  2.1250177168401776\n",
      "Training loss after  2000  iterations is :  2.3439598561541075  | validation loss is :  2.095266267980043\n",
      "Training loss after  2500  iterations is :  2.3063979874725353  | validation loss is :  2.0788592848378404\n",
      "Training loss after  3000  iterations is :  2.2833213090920643  | validation loss is :  2.0717132329328565\n",
      "Training loss after  3500  iterations is :  2.268903540043426  | validation loss is :  2.070028211659999\n",
      "Training loss after  4000  iterations is :  2.2597311306917707  | validation loss is :  2.0709923452179964\n",
      "Training loss after  4500  iterations is :  2.2537427972063586  | validation loss is :  2.073029976012584\n",
      "[[-0.33476601]\n",
      " [ 0.28247899]\n",
      " [ 0.75434582]\n",
      " [ 0.51652948]\n",
      " [ 0.50362021]\n",
      " [-2.48427063]\n",
      " [-0.24577794]\n",
      " [ 2.45418907]]\n",
      "9.953308969540608\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  10.443555680042602  | validation loss is :  10.40268366038925\n",
      "Training loss after  500  iterations is :  5.836585881826906  | validation loss is :  5.828692467839856\n",
      "Training loss after  1000  iterations is :  2.884572407692349  | validation loss is :  2.876299654117402\n",
      "Training loss after  1500  iterations is :  2.3920547778323225  | validation loss is :  2.3822262285466285\n",
      "Training loss after  2000  iterations is :  2.324143477780035  | validation loss is :  2.315840002528866\n",
      "Training loss after  2500  iterations is :  2.287865058116803  | validation loss is :  2.281789887044549\n",
      "Training loss after  3000  iterations is :  2.265591157090005  | validation loss is :  2.2615490859823693\n",
      "Training loss after  3500  iterations is :  2.2516990646677155  | validation loss is :  2.2492335021184835\n",
      "Training loss after  4000  iterations is :  2.2428822459045525  | validation loss is :  2.241530318027634\n",
      "Training loss after  4500  iterations is :  2.2371431723338686  | validation loss is :  2.2365254026871244\n",
      "[[-0.34345608]\n",
      " [ 0.28943598]\n",
      " [ 0.79188393]\n",
      " [ 0.51160972]\n",
      " [ 0.47949738]\n",
      " [-2.40664801]\n",
      " [-0.31375175]\n",
      " [ 2.4002976 ]]\n",
      "9.931454985338592\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  10.409599696092537  | validation loss is :  10.739894028388138\n",
      "Training loss after  500  iterations is :  5.815671884994044  | validation loss is :  6.053870419235388\n",
      "Training loss after  1000  iterations is :  2.8681193102214917  | validation loss is :  3.100695939849918\n",
      "Training loss after  1500  iterations is :  2.3855105599144406  | validation loss is :  2.540500800277251\n",
      "Training loss after  2000  iterations is :  2.320520078632222  | validation loss is :  2.4346755429217275\n",
      "Training loss after  2500  iterations is :  2.2854339515971103  | validation loss is :  2.376696717519253\n",
      "Training loss after  3000  iterations is :  2.2635481550143672  | validation loss is :  2.3396893243555277\n",
      "Training loss after  3500  iterations is :  2.2496605782248182  | validation loss is :  2.3152829097571024\n",
      "Training loss after  4000  iterations is :  2.240691208653811  | validation loss is :  2.2988183047238633\n",
      "Training loss after  4500  iterations is :  2.2347547922201425  | validation loss is :  2.2874567182687766\n",
      "[[-0.33351234]\n",
      " [ 0.29933752]\n",
      " [ 0.78018692]\n",
      " [ 0.50894946]\n",
      " [ 0.50560326]\n",
      " [-2.377008  ]\n",
      " [-0.28178364]\n",
      " [ 2.32167975]]\n",
      "9.923377043198116\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  10.40397435011291  | validation loss is :  10.78534844162182\n",
      "Training loss after  500  iterations is :  5.800506930427566  | validation loss is :  6.158170612279852\n",
      "Training loss after  1000  iterations is :  2.8515719087823155  | validation loss is :  3.2091569325949196\n",
      "Training loss after  1500  iterations is :  2.3701310109371594  | validation loss is :  2.6464799237408103\n",
      "Training loss after  2000  iterations is :  2.304359133987599  | validation loss is :  2.5517450369230823\n",
      "Training loss after  2500  iterations is :  2.2690655589454707  | validation loss is :  2.5036145808261354\n",
      "Training loss after  3000  iterations is :  2.2472508184767275  | validation loss is :  2.4730144623862755\n",
      "Training loss after  3500  iterations is :  2.233528047529832  | validation loss is :  2.4526088545168934\n",
      "Training loss after  4000  iterations is :  2.224754774942791  | validation loss is :  2.4386228596088895\n",
      "Training loss after  4500  iterations is :  2.2190235872771855  | validation loss is :  2.4287563807959978\n",
      "[[-0.33303679]\n",
      " [ 0.25957287]\n",
      " [ 0.79031697]\n",
      " [ 0.54938413]\n",
      " [ 0.46499602]\n",
      " [-2.33673194]\n",
      " [-0.35705867]\n",
      " [ 2.38249216]]\n",
      "9.911920135911746\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  10.468597711831103  | validation loss is :  10.20403362196898\n",
      "Training loss after  500  iterations is :  5.86291946925238  | validation loss is :  5.639164704286458\n",
      "Training loss after  1000  iterations is :  2.9122099825317007  | validation loss is :  2.652673971760259\n",
      "Training loss after  1500  iterations is :  2.4101286693135417  | validation loss is :  2.2007857418544488\n",
      "Training loss after  2000  iterations is :  2.340102676602759  | validation loss is :  2.158526220086554\n",
      "Training loss after  2500  iterations is :  2.302808731410538  | validation loss is :  2.1363747493633145\n",
      "Training loss after  3000  iterations is :  2.279859639479235  | validation loss is :  2.1243265965470455\n",
      "Training loss after  3500  iterations is :  2.265501798744617  | validation loss is :  2.1182187573641356\n",
      "Training loss after  4000  iterations is :  2.256365859290145  | validation loss is :  2.1153204702117296\n",
      "Training loss after  4500  iterations is :  2.2504114497012306  | validation loss is :  2.114062146035216\n",
      "[[-0.35368064]\n",
      " [ 0.27013653]\n",
      " [ 0.77382669]\n",
      " [ 0.55523497]\n",
      " [ 0.49176766]\n",
      " [-2.45041335]\n",
      " [-0.31927166]\n",
      " [ 2.42804329]]\n",
      "9.945917257710356\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  10.461757825345039  | validation loss is :  10.269276195192214\n",
      "Training loss after  500  iterations is :  5.848077926156333  | validation loss is :  5.771310371639907\n",
      "Training loss after  1000  iterations is :  2.8981212786853834  | validation loss is :  2.797691448643031\n",
      "Training loss after  1500  iterations is :  2.401117909658005  | validation loss is :  2.307886355544695\n",
      "Training loss after  2000  iterations is :  2.3321081768254466  | validation loss is :  2.246815288186379\n",
      "Training loss after  2500  iterations is :  2.2953576937573246  | validation loss is :  2.2160833407272538\n",
      "Training loss after  3000  iterations is :  2.272812069110694  | validation loss is :  2.198813389921191\n",
      "Training loss after  3500  iterations is :  2.2587499962589983  | validation loss is :  2.1891888883209685\n",
      "Training loss after  4000  iterations is :  2.2498211271808195  | validation loss is :  2.183831982212573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  4500  iterations is :  2.2440052292705945  | validation loss is :  2.180815161942584\n",
      "[[-0.32548126]\n",
      " [ 0.26361875]\n",
      " [ 0.76149524]\n",
      " [ 0.50898293]\n",
      " [ 0.51154557]\n",
      " [-2.41122095]\n",
      " [-0.29531126]\n",
      " [ 2.44128554]]\n",
      "9.938019341330797\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  10.440459433754144  | validation loss is :  10.464740386980772\n",
      "Training loss after  500  iterations is :  5.836422722271372  | validation loss is :  5.870505995342452\n",
      "Training loss after  1000  iterations is :  2.885682973875023  | validation loss is :  2.9181313138323453\n",
      "Training loss after  1500  iterations is :  2.3908045500076995  | validation loss is :  2.4259728392004236\n",
      "Training loss after  2000  iterations is :  2.322423511422818  | validation loss is :  2.3584812531411323\n",
      "Training loss after  2500  iterations is :  2.286224962603316  | validation loss is :  2.321762499887433\n",
      "Training loss after  3000  iterations is :  2.2640787637883126  | validation loss is :  2.2986993471139443\n",
      "Training loss after  3500  iterations is :  2.2502582450893733  | validation loss is :  2.283915196902531\n",
      "Training loss after  4000  iterations is :  2.2414578149113136  | validation loss is :  2.2742195410168025\n",
      "Training loss after  4500  iterations is :  2.2357001012869278  | validation loss is :  2.2676798450582463\n",
      "[[-0.33052936]\n",
      " [ 0.29386201]\n",
      " [ 0.80694339]\n",
      " [ 0.5180274 ]\n",
      " [ 0.47432041]\n",
      " [-2.41307998]\n",
      " [-0.30320538]\n",
      " [ 2.37387173]]\n",
      "9.93703097680214\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  10.42861386893045  | validation loss is :  10.570648471721583\n",
      "Training loss after  500  iterations is :  5.829597321164224  | validation loss is :  5.93632721852108\n",
      "Training loss after  1000  iterations is :  2.8823528699789063  | validation loss is :  2.960440904080159\n",
      "Training loss after  1500  iterations is :  2.3930933668213905  | validation loss is :  2.4185713343115056\n",
      "Training loss after  2000  iterations is :  2.3249802974943803  | validation loss is :  2.3372327019306214\n",
      "Training loss after  2500  iterations is :  2.2878036584583983  | validation loss is :  2.300772594676139\n",
      "Training loss after  3000  iterations is :  2.2645269677240307  | validation loss is :  2.2823952214529917\n",
      "Training loss after  3500  iterations is :  2.249745261255558  | validation loss is :  2.2735814051137835\n",
      "Training loss after  4000  iterations is :  2.2402105635225786  | validation loss is :  2.269867943628408\n",
      "Training loss after  4500  iterations is :  2.233918139509878  | validation loss is :  2.268788929534471\n",
      "[[-0.33445247]\n",
      " [ 0.26079245]\n",
      " [ 0.77722383]\n",
      " [ 0.50830628]\n",
      " [ 0.48955368]\n",
      " [-2.44087248]\n",
      " [-0.26618869]\n",
      " [ 2.43226447]]\n",
      "9.922443647729951\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  10.398691427070192  | validation loss is :  10.831165619685407\n",
      "Training loss after  500  iterations is :  5.813772054240874  | validation loss is :  6.062394608595482\n",
      "Training loss after  1000  iterations is :  2.847179656852017  | validation loss is :  3.161628901612178\n",
      "Training loss after  1500  iterations is :  2.3513847086054755  | validation loss is :  2.721627077759142\n",
      "Training loss after  2000  iterations is :  2.2828839422967038  | validation loss is :  2.7123508764370876\n",
      "Training loss after  2500  iterations is :  2.2487639840970326  | validation loss is :  2.718925557223615\n",
      "Training loss after  3000  iterations is :  2.229289298903231  | validation loss is :  2.7196278968660095\n",
      "Training loss after  3500  iterations is :  2.217757680047901  | validation loss is :  2.713975405992899\n",
      "Training loss after  4000  iterations is :  2.2106232569275504  | validation loss is :  2.7041949450855953\n",
      "Training loss after  4500  iterations is :  2.2059777824915074  | validation loss is :  2.692421742377851\n",
      "[[-0.32299233]\n",
      " [ 0.19375098]\n",
      " [ 0.5909261 ]\n",
      " [ 1.0567095 ]\n",
      " [ 0.46303537]\n",
      " [-2.37136893]\n",
      " [-0.37794965]\n",
      " [ 2.26406495]]\n",
      "9.914418861848038\n"
     ]
    }
   ],
   "source": [
    "models,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = 10, epochs = 5000, learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x for x in range(len(models[0][2]))],models[0][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(models[0][3]))], models[0][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for LR MAE Loss\n",
      "For CV number  0 the train loss =  1.5790079453446764  and the val loss =  1.4946051438626606\n",
      "For CV number  1 the train loss =  1.5862642965124074  and the val loss =  1.4240977434729953\n",
      "For CV number  2 the train loss =  1.5677558915056704  and the val loss =  1.5922469082822315\n",
      "For CV number  3 the train loss =  1.5686591348646493  and the val loss =  1.5981470753429103\n",
      "For CV number  4 the train loss =  1.5591642739969063  and the val loss =  1.705209587161556\n",
      "For CV number  5 the train loss =  1.5774588854337188  and the val loss =  1.5066208910780743\n",
      "For CV number  6 the train loss =  1.5724052579682588  and the val loss =  1.5507165483914473\n",
      "For CV number  7 the train loss =  1.5750455710534508  and the val loss =  1.5486485293038714\n",
      "For CV number  8 the train loss =  1.5704778436554456  and the val loss =  1.5821056761087104\n",
      "For CV number  9 the train loss =  1.5533079332565543  and the val loss =  1.7405564842853942\n",
      "Stats for LR RMSE Loss\n",
      "For CV number  0 the train loss =  2.2547985464586877  and the val loss =  2.0412690011059484\n",
      "For CV number  1 the train loss =  2.249698767346536  and the val loss =  2.07532426614657\n",
      "For CV number  2 the train loss =  2.233280663288711  and the val loss =  2.233114347387998\n",
      "For CV number  3 the train loss =  2.2307004213868926  and the val loss =  2.2794417841527426\n",
      "For CV number  4 the train loss =  2.2151739359965603  and the val loss =  2.421574336456404\n",
      "For CV number  5 the train loss =  2.246405823462628  and the val loss =  2.1135795977029903\n",
      "For CV number  6 the train loss =  2.2400883442676127  and the val loss =  2.1790555885215483\n",
      "For CV number  7 the train loss =  2.2318012813871224  and the val loss =  2.2631278876872827\n",
      "For CV number  8 the train loss =  2.22963868553345  and the val loss =  2.2689822586442823\n",
      "For CV number  9 the train loss =  2.2027829202216322  and the val loss =  2.6801884124988433\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats for LR MAE Loss\")\n",
    "for i in range(len(models_mae)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", models_mae[i][0], \" and the val loss = \", models_mae[i][1] )\n",
    "print(\"Stats for LR RMSE Loss\")\n",
    "for i in range(len(models)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", models[i][0], \" and the val loss = \", models[i][1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LR on Video Game dataset '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"LR on Video Game dataset \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.08066146e-01  1.75631681e+00]\n",
      " [ 0.00000000e+00  1.33360330e-05]\n",
      " [ 0.00000000e+00  1.33360330e-05]\n",
      " ...\n",
      " [ 0.00000000e+00  1.33360330e-05]\n",
      " [ 0.00000000e+00  1.33360330e-05]\n",
      " [-9.95233958e-02  1.36051350e+00]]\n",
      "[[0.07]\n",
      " [0.01]\n",
      " [0.02]\n",
      " ...\n",
      " [0.08]\n",
      " [1.19]\n",
      " [0.29]]\n"
     ]
    }
   ],
   "source": [
    "pp = MyPreProcessor()\n",
    "X,Y = pp.pre_process(1)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 2\n",
      "Training loss after  0  iterations is :  1.5499161784774869  | validation loss is :  1.7202719733113556\n",
      "Training loss after  500  iterations is :  1.4242307554767732  | validation loss is :  1.6031046456102707\n",
      "Training loss after  1000  iterations is :  1.423667133087237  | validation loss is :  1.6022164803660952\n",
      "Training loss after  1500  iterations is :  1.4236501996666484  | validation loss is :  1.6021438449675633\n",
      "Training loss after  2000  iterations is :  1.423649598611362  | validation loss is :  1.6021328581611702\n",
      "Training loss after  2500  iterations is :  1.4236495771826103  | validation loss is :  1.6021308789164812\n",
      "Training loss after  3000  iterations is :  1.423649576418553  | validation loss is :  1.6021305085528823\n",
      "Training loss after  3500  iterations is :  1.42364957639131  | validation loss is :  1.6021304387375777\n",
      "Training loss after  4000  iterations is :  1.4236495763903385  | validation loss is :  1.6021304255587725\n",
      "Training loss after  4500  iterations is :  1.423649576390304  | validation loss is :  1.6021304230704032\n",
      "[[ 0.33082385]\n",
      " [-0.05304041]]\n",
      "0.5318715468541051\n",
      "For fold :  1 / 2\n",
      "Training loss after  0  iterations is :  1.7202719733113556  | validation loss is :  1.5499129675444863\n",
      "Training loss after  500  iterations is :  1.6027423192680306  | validation loss is :  1.424005548821384\n",
      "Training loss after  1000  iterations is :  1.6020627665994762  | validation loss is :  1.4236938327893751\n",
      "Training loss after  1500  iterations is :  1.6020442403629187  | validation loss is :  1.423737959695957\n",
      "Training loss after  2000  iterations is :  1.6020435608929415  | validation loss is :  1.4237490234843815\n",
      "Training loss after  2500  iterations is :  1.6020435355188014  | validation loss is :  1.4237512535514922\n",
      "Training loss after  3000  iterations is :  1.6020435345703468  | validation loss is :  1.4237516880176357\n",
      "Training loss after  3500  iterations is :  1.6020435345348931  | validation loss is :  1.4237517721366197\n",
      "Training loss after  4000  iterations is :  1.6020435345335675  | validation loss is :  1.4237517884044968\n",
      "Training loss after  4500  iterations is :  1.6020435345335182  | validation loss is :  1.423751791549891\n",
      "[[ 0.34940032]\n",
      " [-0.06339115]]\n",
      "0.5350663100240225\n",
      "For fold :  0 / 3\n",
      "Training loss after  0  iterations is :  1.4881044455063484  | validation loss is :  1.9008772187098473\n",
      "Training loss after  500  iterations is :  1.3616511155416262  | validation loss is :  1.786062835789678\n",
      "Training loss after  1000  iterations is :  1.3611991835185915  | validation loss is :  1.784899850990897\n",
      "Training loss after  1500  iterations is :  1.3611882097124768  | validation loss is :  1.784802173788976\n",
      "Training loss after  2000  iterations is :  1.3611878991065547  | validation loss is :  1.7847880437351458\n",
      "Training loss after  2500  iterations is :  1.3611878902828594  | validation loss is :  1.7847857244151737\n",
      "Training loss after  3000  iterations is :  1.3611878900321752  | validation loss is :  1.7847853351691847\n",
      "Training loss after  3500  iterations is :  1.3611878900250531  | validation loss is :  1.784785269606011\n",
      "Training loss after  4000  iterations is :  1.3611878900248509  | validation loss is :  1.784785258556338\n",
      "Training loss after  4500  iterations is :  1.361187890024845  | validation loss is :  1.7847852566939082\n",
      "[[ 0.32591216]\n",
      " [-0.05272749]]\n",
      "0.5204048901936789\n",
      "For fold :  1 / 3\n",
      "Training loss after  0  iterations is :  1.7797438128785805  | validation loss is :  1.3066934819356062\n",
      "Training loss after  500  iterations is :  1.661161432708095  | validation loss is :  1.1734550711879492\n",
      "Training loss after  1000  iterations is :  1.6603572789635914  | validation loss is :  1.1739061554037025\n",
      "Training loss after  1500  iterations is :  1.6603307984374052  | validation loss is :  1.1739743568422203\n",
      "Training loss after  2000  iterations is :  1.6603296070422484  | validation loss is :  1.173984393243346\n",
      "Training loss after  2500  iterations is :  1.6603295523868866  | validation loss is :  1.1739863014661245\n",
      "Training loss after  3000  iterations is :  1.6603295498770099  | validation loss is :  1.173986697836173\n",
      "Training loss after  3500  iterations is :  1.6603295497617454  | validation loss is :  1.1739867821309145\n",
      "Training loss after  4000  iterations is :  1.660329549756452  | validation loss is :  1.1739868001620883\n",
      "Training loss after  4500  iterations is :  1.6603295497562092  | validation loss is :  1.1739868040244743\n",
      "[[ 0.34787286]\n",
      " [-0.05534746]]\n",
      "0.552257232877972\n",
      "For fold :  2 / 3\n",
      "Training loss after  0  iterations is :  1.6310705160021566  | validation loss is :  1.6495759477709904\n",
      "Training loss after  500  iterations is :  1.5105492281096695  | validation loss is :  1.5268240485923288\n",
      "Training loss after  1000  iterations is :  1.5099282084104968  | validation loss is :  1.526492656732901\n",
      "Training loss after  1500  iterations is :  1.5099111638069433  | validation loss is :  1.5265569653581494\n",
      "Training loss after  2000  iterations is :  1.5099105793159897  | validation loss is :  1.5265725382735205\n",
      "Training loss after  2500  iterations is :  1.509910559082448  | validation loss is :  1.5265755618133336\n",
      "Training loss after  3000  iterations is :  1.5099105583817662  | validation loss is :  1.5265761288758077\n",
      "Training loss after  3500  iterations is :  1.5099105583575014  | validation loss is :  1.5265762345562799\n",
      "Training loss after  4000  iterations is :  1.509910558356661  | validation loss is :  1.526576254227998\n",
      "Training loss after  4500  iterations is :  1.509910558356632  | validation loss is :  1.526576257888937\n",
      "[[ 0.34680327]\n",
      " [-0.06727778]]\n",
      "0.5279528578445503\n",
      "For fold :  0 / 4\n",
      "Training loss after  0  iterations is :  1.5376436992311158  | validation loss is :  1.9053237652417057\n",
      "Training loss after  500  iterations is :  1.4116736700946064  | validation loss is :  1.7935583491147518\n",
      "Training loss after  1000  iterations is :  1.4111709155118195  | validation loss is :  1.7925900528580507\n",
      "Training loss after  1500  iterations is :  1.4111584447449392  | validation loss is :  1.79251078646575\n",
      "Training loss after  2000  iterations is :  1.4111580743222227  | validation loss is :  1.7924992452435633\n",
      "Training loss after  2500  iterations is :  1.4111580632598402  | validation loss is :  1.7924973134298237\n",
      "Training loss after  3000  iterations is :  1.411158062929422  | validation loss is :  1.792496981410972\n",
      "Training loss after  3500  iterations is :  1.4111580629195528  | validation loss is :  1.7924969240840387\n",
      "Training loss after  4000  iterations is :  1.4111580629192582  | validation loss is :  1.792496914178073\n",
      "Training loss after  4500  iterations is :  1.4111580629192493  | validation loss is :  1.7924969124661143\n",
      "[[ 0.33178555]\n",
      " [-0.05589219]]\n",
      "0.5287500666982907\n",
      "For fold :  1 / 4\n",
      "Training loss after  0  iterations is :  1.6766999900659438  | validation loss is :  1.512920021714094\n",
      "Training loss after  500  iterations is :  1.556952376894001  | validation loss is :  1.3867229917694373\n",
      "Training loss after  1000  iterations is :  1.5562473540189123  | validation loss is :  1.3861862780564531\n",
      "Training loss after  1500  iterations is :  1.5562236313906017  | validation loss is :  1.3861821316673253\n",
      "Training loss after  2000  iterations is :  1.5562226223563012  | validation loss is :  1.3861845257837864\n",
      "Training loss after  2500  iterations is :  1.5562225790140853  | validation loss is :  1.3861851644722276\n",
      "Training loss after  3000  iterations is :  1.5562225771516796  | validation loss is :  1.3861853031779794\n",
      "Training loss after  3500  iterations is :  1.5562225770716513  | validation loss is :  1.3861853322095994\n",
      "Training loss after  4000  iterations is :  1.5562225770682125  | validation loss is :  1.3861853382399434\n",
      "Training loss after  4500  iterations is :  1.5562225770680649  | validation loss is :  1.386185339490532\n",
      "[[ 0.34277968]\n",
      " [-0.05777749]]\n",
      "0.5373329322963952\n",
      "For fold :  2 / 4\n",
      "Training loss after  0  iterations is :  1.6746489205495743  | validation loss is :  1.5197847088120833\n",
      "Training loss after  500  iterations is :  1.5551055493054156  | validation loss is :  1.3925017914737625\n",
      "Training loss after  1000  iterations is :  1.554473329067104  | validation loss is :  1.392061838759172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  1500  iterations is :  1.5544560775633491  | validation loss is :  1.3920620894665998\n",
      "Training loss after  2000  iterations is :  1.554455463314047  | validation loss is :  1.3920640196076217\n",
      "Training loss after  2500  iterations is :  1.5544554411458462  | validation loss is :  1.392064450772795\n",
      "Training loss after  3000  iterations is :  1.5544554403453228  | validation loss is :  1.3920645349328866\n",
      "Training loss after  3500  iterations is :  1.5544554403164141  | validation loss is :  1.392064551002305\n",
      "Training loss after  4000  iterations is :  1.55445544031537  | validation loss is :  1.3920645540586072\n",
      "Training loss after  4500  iterations is :  1.5544554403153326  | validation loss is :  1.3920645546394907\n",
      "[[ 0.34154077]\n",
      " [-0.05883244]]\n",
      "0.5355486129105738\n",
      "For fold :  3 / 4\n",
      "Training loss after  0  iterations is :  1.6560975814541212  | validation loss is :  1.5796022032029848\n",
      "Training loss after  500  iterations is :  1.5358718048207078  | validation loss is :  1.4550760521516874\n",
      "Training loss after  1000  iterations is :  1.5352233483527322  | validation loss is :  1.4547297763101052\n",
      "Training loss after  1500  iterations is :  1.535204331155074  | validation loss is :  1.454777748631064\n",
      "Training loss after  2000  iterations is :  1.535203624012141  | validation loss is :  1.4547904006216958\n",
      "Training loss after  2500  iterations is :  1.535203597441488  | validation loss is :  1.4547929790140288\n",
      "Training loss after  3000  iterations is :  1.5352035964426998  | validation loss is :  1.454793483642882\n",
      "Training loss after  3500  iterations is :  1.5352035964051547  | validation loss is :  1.4547935816594946\n",
      "Training loss after  4000  iterations is :  1.5352035964037434  | validation loss is :  1.4547936006699131\n",
      "Training loss after  4500  iterations is :  1.5352035964036903  | validation loss is :  1.4547936043559564\n",
      "[[ 0.34532619]\n",
      " [-0.0616964 ]]\n",
      "0.5326740774351051\n",
      "For fold :  0 / 5\n",
      "Training loss after  0  iterations is :  1.6843023836669495  | validation loss is :  1.4339734545596348\n",
      "Training loss after  500  iterations is :  1.5669301677634662  | validation loss is :  1.2943030044094126\n",
      "Training loss after  1000  iterations is :  1.5662364549135182  | validation loss is :  1.2930300908297874\n",
      "Training loss after  1500  iterations is :  1.566213504707815  | validation loss is :  1.2929046673633842\n",
      "Training loss after  2000  iterations is :  1.5662125373302016  | validation loss is :  1.2928830415854582\n",
      "Training loss after  2500  iterations is :  1.5662124961152832  | validation loss is :  1.292878757456231\n",
      "Training loss after  3000  iterations is :  1.5662124943585973  | validation loss is :  1.2928778807250574\n",
      "Training loss after  3500  iterations is :  1.566212494283722  | validation loss is :  1.2928777000524823\n",
      "Training loss after  4000  iterations is :  1.5662124942805302  | validation loss is :  1.2928776627661323\n",
      "Training loss after  4500  iterations is :  1.5662124942803943  | validation loss is :  1.2928776550688235\n",
      "[[ 0.3385016]\n",
      " [-0.0530051]]\n",
      "0.5347015732851208\n",
      "For fold :  1 / 5\n",
      "Training loss after  0  iterations is :  1.4726801912820016  | validation loss is :  2.1746726711032434\n",
      "Training loss after  500  iterations is :  1.3428290730809134  | validation loss is :  2.069043619034563\n",
      "Training loss after  1000  iterations is :  1.3424039891056647  | validation loss is :  2.0681966760787667\n",
      "Training loss after  1500  iterations is :  1.3423951780227033  | validation loss is :  2.068147883960404\n",
      "Training loss after  2000  iterations is :  1.342394965236067  | validation loss is :  2.0681420340956995\n",
      "Training loss after  2500  iterations is :  1.3423949600774416  | validation loss is :  2.0681411646664487\n",
      "Training loss after  3000  iterations is :  1.342394959952369  | validation loss is :  2.06814103028115\n",
      "Training loss after  3500  iterations is :  1.3423949599493366  | validation loss is :  2.068141009379919\n",
      "Training loss after  4000  iterations is :  1.3423949599492628  | validation loss is :  2.068141006125972\n",
      "Training loss after  4500  iterations is :  1.342394959949261  | validation loss is :  2.0681410056193155\n",
      "[[ 0.33064234]\n",
      " [-0.0611939 ]]\n",
      "0.5233890913185905\n",
      "For fold :  2 / 5\n",
      "Training loss after  0  iterations is :  1.7025149621256437  | validation loss is :  1.3451820943225608\n",
      "Training loss after  500  iterations is :  1.5846777969226482  | validation loss is :  1.204068302574595\n",
      "Training loss after  1000  iterations is :  1.5839863565588923  | validation loss is :  1.2033208440272918\n",
      "Training loss after  1500  iterations is :  1.583964374778193  | validation loss is :  1.2032344823508618\n",
      "Training loss after  2000  iterations is :  1.5839634568870367  | validation loss is :  1.2032177991366226\n",
      "Training loss after  2500  iterations is :  1.5839634180430444  | validation loss is :  1.203214411064817\n",
      "Training loss after  3000  iterations is :  1.5839634163982863  | validation loss is :  1.2032137158199083\n",
      "Training loss after  3500  iterations is :  1.5839634163286413  | validation loss is :  1.2032135728386655\n",
      "Training loss after  4000  iterations is :  1.5839634163256922  | validation loss is :  1.2032135434201583\n",
      "Training loss after  4500  iterations is :  1.5839634163255674  | validation loss is :  1.2032135373666932\n",
      "[[ 0.33896858]\n",
      " [-0.05434415]]\n",
      "0.539121256056578\n",
      "For fold :  3 / 5\n",
      "Training loss after  0  iterations is :  1.639642532274904  | validation loss is :  1.6279317739402497\n",
      "Training loss after  500  iterations is :  1.5159772800892122  | validation loss is :  1.5161977732176928\n",
      "Training loss after  1000  iterations is :  1.5153626814836865  | validation loss is :  1.516121365553656\n",
      "Training loss after  1500  iterations is :  1.5153457777292485  | validation loss is :  1.516170945190315\n",
      "Training loss after  2000  iterations is :  1.515345188869098  | validation loss is :  1.5161817785584428\n",
      "Training loss after  2500  iterations is :  1.5153451681453443  | validation loss is :  1.5161838641495629\n",
      "Training loss after  3000  iterations is :  1.51534516741573  | validation loss is :  1.5161842573082251\n",
      "Training loss after  3500  iterations is :  1.5153451673900424  | validation loss is :  1.5161843311417535\n",
      "Training loss after  4000  iterations is :  1.5153451673891383  | validation loss is :  1.5161843449977264\n",
      "Training loss after  4500  iterations is :  1.5153451673891063  | validation loss is :  1.5161843475976766\n",
      "[[ 0.34613173]\n",
      " [-0.05902591]]\n",
      "0.5359059444417589\n",
      "For fold :  4 / 5\n",
      "Training loss after  0  iterations is :  1.6765355734287186  | validation loss is :  1.4699445373433957\n",
      "Training loss after  500  iterations is :  1.5571528247976048  | validation loss is :  1.3389126044852406\n",
      "Training loss after  1000  iterations is :  1.5564656496192781  | validation loss is :  1.3391011958312007\n",
      "Training loss after  1500  iterations is :  1.5564449417153785  | validation loss is :  1.3392498820577015\n",
      "Training loss after  2000  iterations is :  1.5564441462984082  | validation loss is :  1.3392822461663108\n",
      "Training loss after  2500  iterations is :  1.5564441153973334  | validation loss is :  1.3392887433995624\n",
      "Training loss after  3000  iterations is :  1.5564441141963017  | validation loss is :  1.339290028838481\n",
      "Training loss after  3500  iterations is :  1.5564441141496201  | validation loss is :  1.3392902824362165\n",
      "Training loss after  4000  iterations is :  1.5564441141478058  | validation loss is :  1.339290332439528\n",
      "Training loss after  4500  iterations is :  1.5564441141477354  | validation loss is :  1.339290342297896\n",
      "[[ 0.3474302 ]\n",
      " [-0.06535612]]\n",
      "0.5347154360161646\n",
      "For fold :  0 / 6\n",
      "Training loss after  0  iterations is :  1.682198400460771  | validation loss is :  1.3912806234370805\n",
      "Training loss after  500  iterations is :  1.5638302016597232  | validation loss is :  1.2513594098186078\n",
      "Training loss after  1000  iterations is :  1.5631420423621536  | validation loss is :  1.250533650283775\n",
      "Training loss after  1500  iterations is :  1.5631202076897404  | validation loss is :  1.2504681735265055\n",
      "Training loss after  2000  iterations is :  1.5631193232749967  | validation loss is :  1.2504574553764893\n",
      "Training loss after  2500  iterations is :  1.5631192870513646  | validation loss is :  1.2504553876568736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  3000  iterations is :  1.563119285567066  | validation loss is :  1.2504549733091388\n",
      "Training loss after  3500  iterations is :  1.5631192855062441  | validation loss is :  1.2504548896084922\n",
      "Training loss after  4000  iterations is :  1.563119285503752  | validation loss is :  1.2504548726724416\n",
      "Training loss after  4500  iterations is :  1.5631192855036495  | validation loss is :  1.250454869244425\n",
      "[[ 0.33957056]\n",
      " [-0.05626023]]\n",
      "0.5365457233477875\n",
      "For fold :  1 / 6\n",
      "Training loss after  0  iterations is :  1.4692038044878073  | validation loss is :  2.3004788118604997\n",
      "Training loss after  500  iterations is :  1.340147496906679  | validation loss is :  2.194127704203531\n",
      "Training loss after  1000  iterations is :  1.3397241468122942  | validation loss is :  2.1928907821303114\n",
      "Training loss after  1500  iterations is :  1.3397148014284812  | validation loss is :  2.1927984283275395\n",
      "Training loss after  2000  iterations is :  1.3397145615482706  | validation loss is :  2.192785978921319\n",
      "Training loss after  2500  iterations is :  1.3397145553692174  | validation loss is :  2.192784038348566\n",
      "Training loss after  3000  iterations is :  1.3397145552100393  | validation loss is :  2.192783728289558\n",
      "Training loss after  3500  iterations is :  1.3397145552059389  | validation loss is :  2.192783678558901\n",
      "Training loss after  4000  iterations is :  1.3397145552058332  | validation loss is :  2.192783670577872\n",
      "Training loss after  4500  iterations is :  1.3397145552058305  | validation loss is :  2.192783669296921\n",
      "[[ 0.32974577]\n",
      " [-0.0564685 ]]\n",
      "0.5200756238121382\n",
      "For fold :  2 / 6\n",
      "Training loss after  0  iterations is :  1.6990217963874046  | validation loss is :  1.2848789541341008\n",
      "Training loss after  500  iterations is :  1.5785863949807841  | validation loss is :  1.15374232950915\n",
      "Training loss after  1000  iterations is :  1.577854916541473  | validation loss is :  1.154501017557592\n",
      "Training loss after  1500  iterations is :  1.5778311111080543  | validation loss is :  1.1546442542852844\n",
      "Training loss after  2000  iterations is :  1.5778301160214259  | validation loss is :  1.154671505437201\n",
      "Training loss after  2500  iterations is :  1.5778300739333808  | validation loss is :  1.1546770209764072\n",
      "Training loss after  3000  iterations is :  1.577830072152367  | validation loss is :  1.1546781519903648\n",
      "Training loss after  3500  iterations is :  1.5778300720769995  | validation loss is :  1.1546783845101156\n",
      "Training loss after  4000  iterations is :  1.57783007207381  | validation loss is :  1.1546784323364578\n",
      "Training loss after  4500  iterations is :  1.577830072073675  | validation loss is :  1.154678442174667\n",
      "[[ 0.34611869]\n",
      " [-0.05990437]]\n",
      "0.5430965801513535\n",
      "For fold :  3 / 6\n",
      "Training loss after  0  iterations is :  1.6923315860799133  | validation loss is :  1.3283972012136998\n",
      "Training loss after  500  iterations is :  1.5729052817597173  | validation loss is :  1.1925376766681586\n",
      "Training loss after  1000  iterations is :  1.5722498677765506  | validation loss is :  1.192141581902942\n",
      "Training loss after  1500  iterations is :  1.5722309452894905  | validation loss is :  1.1920918694655807\n",
      "Training loss after  2000  iterations is :  1.5722302256367882  | validation loss is :  1.192081630434507\n",
      "Training loss after  2500  iterations is :  1.5722301978770181  | validation loss is :  1.192079590715944\n",
      "Training loss after  3000  iterations is :  1.5722301968055468  | validation loss is :  1.1920791885052962\n",
      "Training loss after  3500  iterations is :  1.572230196764189  | validation loss is :  1.1920791094108487\n",
      "Training loss after  4000  iterations is :  1.5722301967625925  | validation loss is :  1.192079093867908\n",
      "Training loss after  4500  iterations is :  1.572230196762531  | validation loss is :  1.1920790908140753\n",
      "[[ 0.34070485]\n",
      " [-0.0548149 ]]\n",
      "0.5389294563268211\n",
      "For fold :  4 / 6\n",
      "Training loss after  0  iterations is :  1.5964746570958632  | validation loss is :  1.8267502891222782\n",
      "Training loss after  500  iterations is :  1.4750545340658312  | validation loss is :  1.7063647307409102\n",
      "Training loss after  1000  iterations is :  1.4744803609929327  | validation loss is :  1.7055876095055886\n",
      "Training loss after  1500  iterations is :  1.4744649098359748  | validation loss is :  1.7055986006766444\n",
      "Training loss after  2000  iterations is :  1.474464398992286  | validation loss is :  1.7056050509769816\n",
      "Training loss after  2500  iterations is :  1.4744643819732008  | validation loss is :  1.7056063754253563\n",
      "Training loss after  3000  iterations is :  1.474464381406054  | validation loss is :  1.7056066220938506\n",
      "Training loss after  3500  iterations is :  1.4744643813871543  | validation loss is :  1.705606667286002\n",
      "Training loss after  4000  iterations is :  1.4744643813865246  | validation loss is :  1.7056066755412451\n",
      "Training loss after  4500  iterations is :  1.4744643813865037  | validation loss is :  1.705606677048419\n",
      "[[ 0.33998995]\n",
      " [-0.06244791]]\n",
      "0.525548874757649\n",
      "For fold :  5 / 6\n",
      "Training loss after  0  iterations is :  1.672062537981645  | validation loss is :  1.4510951999277684\n",
      "Training loss after  500  iterations is :  1.551674278858219  | validation loss is :  1.3235742229124596\n",
      "Training loss after  1000  iterations is :  1.5510066418275459  | validation loss is :  1.3237670687994552\n",
      "Training loss after  1500  iterations is :  1.55098695644842  | validation loss is :  1.3238712249119653\n",
      "Training loss after  2000  iterations is :  1.5509862126974083  | validation loss is :  1.323892918309369\n",
      "Training loss after  2500  iterations is :  1.5509861842708332  | validation loss is :  1.3238972081366627\n",
      "Training loss after  3000  iterations is :  1.5509861831838416  | validation loss is :  1.3238980487462075\n",
      "Training loss after  3500  iterations is :  1.550986183142276  | validation loss is :  1.323898213189294\n",
      "Training loss after  4000  iterations is :  1.5509861831406864  | validation loss is :  1.323898245348204\n",
      "Training loss after  4500  iterations is :  1.5509861831406258  | validation loss is :  1.3238982516369215\n",
      "[[ 0.34545954]\n",
      " [-0.06136641]]\n",
      "0.5370087036887746\n",
      "For fold :  0 / 7\n",
      "Training loss after  0  iterations is :  1.6694605707677617  | validation loss is :  1.4292777046882554\n",
      "Training loss after  500  iterations is :  1.5508532497427097  | validation loss is :  1.289495341866512\n",
      "Training loss after  1000  iterations is :  1.550182028350598  | validation loss is :  1.2882895130940197\n",
      "Training loss after  1500  iterations is :  1.5501608531868218  | validation loss is :  1.2881529572769923\n",
      "Training loss after  2000  iterations is :  1.5501600053476552  | validation loss is :  1.2881282171728416\n",
      "Training loss after  2500  iterations is :  1.55015997104711  | validation loss is :  1.2881233487816655\n",
      "Training loss after  3000  iterations is :  1.5501599696588801  | validation loss is :  1.2881223738256526\n",
      "Training loss after  3500  iterations is :  1.5501599696026942  | validation loss is :  1.2881221778680974\n",
      "Training loss after  4000  iterations is :  1.5501599696004198  | validation loss is :  1.2881221384530095\n",
      "Training loss after  4500  iterations is :  1.5501599696003279  | validation loss is :  1.2881221305238137\n",
      "[[ 0.33741902]\n",
      " [-0.05480679]]\n",
      "0.5358229387502856\n",
      "For fold :  1 / 7\n",
      "Training loss after  0  iterations is :  1.5162356723465853  | validation loss is :  2.229746616742443\n",
      "Training loss after  500  iterations is :  1.3892469975836659  | validation loss is :  2.123860854583763\n",
      "Training loss after  1000  iterations is :  1.3887849021326115  | validation loss is :  2.1228548450161964\n",
      "Training loss after  1500  iterations is :  1.3887745360457768  | validation loss is :  2.122783354438644\n",
      "Training loss after  2000  iterations is :  1.3887742575766444  | validation loss is :  2.122773790810952\n",
      "Training loss after  2500  iterations is :  1.3887742500559839  | validation loss is :  2.1227722773090942\n",
      "Training loss after  3000  iterations is :  1.3887742498528428  | validation loss is :  2.1227720301342483\n",
      "Training loss after  3500  iterations is :  1.3887742498473559  | validation loss is :  2.122771989553246\n",
      "Training loss after  4000  iterations is :  1.3887742498472075  | validation loss is :  2.122771982884873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  4500  iterations is :  1.3887742498472035  | validation loss is :  2.1227719817889517\n",
      "[[ 0.33124856]\n",
      " [-0.06056381]]\n",
      "0.5265516669936208\n",
      "For fold :  2 / 7\n",
      "Training loss after  0  iterations is :  1.6386197321701461  | validation loss is :  1.6294182815803955\n",
      "Training loss after  500  iterations is :  1.517089898520794  | validation loss is :  1.5105333199286222\n",
      "Training loss after  1000  iterations is :  1.5164482403462922  | validation loss is :  1.5099105151367969\n",
      "Training loss after  1500  iterations is :  1.5164289515773386  | validation loss is :  1.50990557655867\n",
      "Training loss after  2000  iterations is :  1.5164282311257826  | validation loss is :  1.5099080367495767\n",
      "Training loss after  2500  iterations is :  1.5164282039824182  | validation loss is :  1.5099086414996532\n",
      "Training loss after  3000  iterations is :  1.516428202959463  | validation loss is :  1.5099087636820772\n",
      "Training loss after  3500  iterations is :  1.5164282029209102  | validation loss is :  1.5099087875815669\n",
      "Training loss after  4000  iterations is :  1.5164282029194573  | validation loss is :  1.5099087922280108\n",
      "Training loss after  4500  iterations is :  1.5164282029194025  | validation loss is :  1.5099087931302926\n",
      "[[ 0.34241622]\n",
      " [-0.05796542]]\n",
      "0.5331883782398498\n",
      "For fold :  3 / 7\n",
      "Training loss after  0  iterations is :  1.6874454114868667  | validation loss is :  1.2963579313203406\n",
      "Training loss after  500  iterations is :  1.5685370960913092  | validation loss is :  1.152834355511156\n",
      "Training loss after  1000  iterations is :  1.5678623164508732  | validation loss is :  1.1525377491557784\n",
      "Training loss after  1500  iterations is :  1.5678415085462403  | validation loss is :  1.1525410355583823\n",
      "Training loss after  2000  iterations is :  1.567840674376796  | validation loss is :  1.1525427128489596\n",
      "Training loss after  2500  iterations is :  1.5678406405165564  | validation loss is :  1.1525430908978984\n",
      "Training loss after  3000  iterations is :  1.5678406391414101  | validation loss is :  1.1525431687009582\n",
      "Training loss after  3500  iterations is :  1.5678406390855608  | validation loss is :  1.152543184445185\n",
      "Training loss after  4000  iterations is :  1.5678406390832926  | validation loss is :  1.1525431876206698\n",
      "Training loss after  4500  iterations is :  1.5678406390832005  | validation loss is :  1.1525431882607209\n",
      "[[ 0.34126725]\n",
      " [-0.05823264]]\n",
      "0.5374622333168532\n",
      "For fold :  4 / 7\n",
      "Training loss after  0  iterations is :  1.6893335501565632  | validation loss is :  1.2811773532127626\n",
      "Training loss after  500  iterations is :  1.568495001322652  | validation loss is :  1.1527886177379347\n",
      "Training loss after  1000  iterations is :  1.567814556028578  | validation loss is :  1.1531231242705304\n",
      "Training loss after  1500  iterations is :  1.5677940136628774  | validation loss is :  1.1531859852564539\n",
      "Training loss after  2000  iterations is :  1.5677932087395579  | validation loss is :  1.1531971221648019\n",
      "Training loss after  2500  iterations is :  1.5677931767980828  | validation loss is :  1.1531992824993962\n",
      "Training loss after  3000  iterations is :  1.567793175529884  | validation loss is :  1.1531997104279987\n",
      "Training loss after  3500  iterations is :  1.5677931754795302  | validation loss is :  1.1531997955868956\n",
      "Training loss after  4000  iterations is :  1.5677931754775312  | validation loss is :  1.1531998125509224\n",
      "Training loss after  4500  iterations is :  1.5677931754774517  | validation loss is :  1.153199815930977\n",
      "[[ 0.34610797]\n",
      " [-0.05554606]]\n",
      "0.5397147657617786\n",
      "For fold :  5 / 7\n",
      "Training loss after  0  iterations is :  1.5718127000350983  | validation loss is :  1.9854756949767438\n",
      "Training loss after  500  iterations is :  1.450476611515901  | validation loss is :  1.862702281907167\n",
      "Training loss after  1000  iterations is :  1.4499441816443845  | validation loss is :  1.861376023969973\n",
      "Training loss after  1500  iterations is :  1.449930536997187  | validation loss is :  1.8613104014166095\n",
      "Training loss after  2000  iterations is :  1.4499301101158801  | validation loss is :  1.861303535079976\n",
      "Training loss after  2500  iterations is :  1.4499300966674307  | validation loss is :  1.86130246550796\n",
      "Training loss after  3000  iterations is :  1.4499300962436588  | validation loss is :  1.861302280335315\n",
      "Training loss after  3500  iterations is :  1.4499300962303052  | validation loss is :  1.8613022476121568\n",
      "Training loss after  4000  iterations is :  1.4499300962298844  | validation loss is :  1.8613022418079908\n",
      "Training loss after  4500  iterations is :  1.4499300962298711  | validation loss is :  1.8613022407778184\n",
      "[[ 0.33504607]\n",
      " [-0.06067336]]\n",
      "0.5225189508566298\n",
      "For fold :  6 / 7\n",
      "Training loss after  0  iterations is :  1.6798782916236898  | validation loss is :  1.354048879041732\n",
      "Training loss after  500  iterations is :  1.558468982751007  | validation loss is :  1.230703303106068\n",
      "Training loss after  1000  iterations is :  1.5577747028830324  | validation loss is :  1.231814878814149\n",
      "Training loss after  1500  iterations is :  1.5577535719039615  | validation loss is :  1.2320500794909188\n",
      "Training loss after  2000  iterations is :  1.5577527500277402  | validation loss is :  1.232096079261901\n",
      "Training loss after  2500  iterations is :  1.5577527176953745  | validation loss is :  1.2321051769023046\n",
      "Training loss after  3000  iterations is :  1.5577527164228377  | validation loss is :  1.2321069805837275\n",
      "Training loss after  3500  iterations is :  1.5577527163727525  | validation loss is :  1.2321073383652323\n",
      "Training loss after  4000  iterations is :  1.5577527163707812  | validation loss is :  1.2321074093434452\n",
      "Training loss after  4500  iterations is :  1.5577527163707034  | validation loss is :  1.2321074234247478\n",
      "[[ 0.34896251]\n",
      " [-0.06245236]]\n",
      "0.5395451099028363\n",
      "For fold :  0 / 8\n",
      "Training loss after  0  iterations is :  1.6628917242136976  | validation loss is :  1.44554439979406\n",
      "Training loss after  500  iterations is :  1.543898864757826  | validation loss is :  1.305848976594769\n",
      "Training loss after  1000  iterations is :  1.5432397416253556  | validation loss is :  1.3050197069155087\n",
      "Training loss after  1500  iterations is :  1.5432199224681469  | validation loss is :  1.304970624645954\n",
      "Training loss after  2000  iterations is :  1.5432191670715416  | validation loss is :  1.3049638629075866\n",
      "Training loss after  2500  iterations is :  1.5432191379753435  | validation loss is :  1.3049626424742757\n",
      "Training loss after  3000  iterations is :  1.5432191368541601  | validation loss is :  1.3049624069979533\n",
      "Training loss after  3500  iterations is :  1.5432191368109562  | validation loss is :  1.304962360931067\n",
      "Training loss after  4000  iterations is :  1.5432191368092913  | validation loss is :  1.3049623518941218\n",
      "Training loss after  4500  iterations is :  1.5432191368092274  | validation loss is :  1.304962350120388\n",
      "[[ 0.33901107]\n",
      " [-0.0583295 ]]\n",
      "0.5346389401561877\n",
      "For fold :  1 / 8\n",
      "Training loss after  0  iterations is :  1.5247795746875141  | validation loss is :  2.274304379734654\n",
      "Training loss after  500  iterations is :  1.396967464155129  | validation loss is :  2.174738537470497\n",
      "Training loss after  1000  iterations is :  1.3964846035289542  | validation loss is :  2.173678579640101\n",
      "Training loss after  1500  iterations is :  1.3964730141443251  | validation loss is :  2.173590406317288\n",
      "Training loss after  2000  iterations is :  1.396472681774254  | validation loss is :  2.1735777059011143\n",
      "Training loss after  2500  iterations is :  1.3964726721932865  | validation loss is :  2.1735756129313644\n",
      "Training loss after  3000  iterations is :  1.3964726719170664  | validation loss is :  2.17357525934528\n",
      "Training loss after  3500  iterations is :  1.3964726719091027  | validation loss is :  2.173575199358748\n",
      "Training loss after  4000  iterations is :  1.3964726719088731  | validation loss is :  2.1735751891747856\n",
      "Training loss after  4500  iterations is :  1.3964726719088665  | validation loss is :  2.1735751874456435\n",
      "[[ 0.3342456 ]\n",
      " [-0.05660363]]\n",
      "0.5283164446799776\n",
      "For fold :  2 / 8\n",
      "Training loss after  0  iterations is :  1.601332268832439  | validation loss is :  1.869967476415844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  500  iterations is :  1.4818234647996384  | validation loss is :  1.738817207151164\n",
      "Training loss after  1000  iterations is :  1.4812483243366321  | validation loss is :  1.737055572002543\n",
      "Training loss after  1500  iterations is :  1.4812320421663747  | validation loss is :  1.7369231731017016\n",
      "Training loss after  2000  iterations is :  1.481231474478431  | validation loss is :  1.7369042391613936\n",
      "Training loss after  2500  iterations is :  1.481231454535298  | validation loss is :  1.7369008892466036\n",
      "Training loss after  3000  iterations is :  1.4812314538345137  | validation loss is :  1.7369002681209966\n",
      "Training loss after  3500  iterations is :  1.4812314538098885  | validation loss is :  1.7369001519225131\n",
      "Training loss after  4000  iterations is :  1.4812314538090234  | validation loss is :  1.7369001301485785\n",
      "Training loss after  4500  iterations is :  1.481231453808993  | validation loss is :  1.73690012606722\n",
      "[[ 0.33301292]\n",
      " [-0.0588503 ]]\n",
      "0.5252837628661281\n",
      "For fold :  3 / 8\n",
      "Training loss after  0  iterations is :  1.7056142111595616  | validation loss is :  1.040100291588724\n",
      "Training loss after  500  iterations is :  1.5838506701932624  | validation loss is :  0.9093841286027994\n",
      "Training loss after  1000  iterations is :  1.5831096706117742  | validation loss is :  0.9117444015803392\n",
      "Training loss after  1500  iterations is :  1.5830851296589399  | validation loss is :  0.9120532504643719\n",
      "Training loss after  2000  iterations is :  1.5830840809283881  | validation loss is :  0.912108547289106\n",
      "Training loss after  2500  iterations is :  1.5830840355704736  | validation loss is :  0.9121196868163057\n",
      "Training loss after  3000  iterations is :  1.5830840336077598  | validation loss is :  0.9121219892303553\n",
      "Training loss after  3500  iterations is :  1.583084033522828  | validation loss is :  0.9121224675730868\n",
      "Training loss after  4000  iterations is :  1.5830840335191527  | validation loss is :  0.9121225670534587\n",
      "Training loss after  4500  iterations is :  1.5830840335189937  | validation loss is :  0.9121225877464216\n",
      "[[ 0.34963041]\n",
      " [-0.05764917]]\n",
      "0.5450922806162829\n",
      "For fold :  4 / 8\n",
      "Training loss after  0  iterations is :  1.6608173632382845  | validation loss is :  1.4621470373502032\n",
      "Training loss after  500  iterations is :  1.542517233186439  | validation loss is :  1.3174437846650626\n",
      "Training loss after  1000  iterations is :  1.5418948089352893  | validation loss is :  1.316094642550761\n",
      "Training loss after  1500  iterations is :  1.5418772300197443  | validation loss is :  1.3159550705091225\n",
      "Training loss after  2000  iterations is :  1.5418765899667384  | validation loss is :  1.3159313691752568\n",
      "Training loss after  2500  iterations is :  1.5418765663838734  | validation loss is :  1.3159269306872785\n",
      "Training loss after  3000  iterations is :  1.5418765655145377  | validation loss is :  1.3159260826441366\n",
      "Training loss after  3500  iterations is :  1.5418765654824904  | validation loss is :  1.3159259199737048\n",
      "Training loss after  4000  iterations is :  1.541876565481309  | validation loss is :  1.3159258887467533\n",
      "Training loss after  4500  iterations is :  1.5418765654812656  | validation loss is :  1.3159258827514067\n",
      "[[ 0.3357651 ]\n",
      " [-0.05676741]]\n",
      "0.5333228794140656\n",
      "For fold :  5 / 8\n",
      "Training loss after  0  iterations is :  1.6459015544662219  | validation loss is :  1.5758408668167987\n",
      "Training loss after  500  iterations is :  1.5232700418979266  | validation loss is :  1.4640744908212948\n",
      "Training loss after  1000  iterations is :  1.522640116855039  | validation loss is :  1.4643865412241468\n",
      "Training loss after  1500  iterations is :  1.5226224347232697  | validation loss is :  1.4645050401251214\n",
      "Training loss after  2000  iterations is :  1.5226218066844144  | validation loss is :  1.4645288675313937\n",
      "Training loss after  2500  iterations is :  1.5226217841474592  | validation loss is :  1.464533430550883\n",
      "Training loss after  3000  iterations is :  1.5226217833384088  | validation loss is :  1.4645342968094222\n",
      "Training loss after  3500  iterations is :  1.5226217833093643  | validation loss is :  1.4645344609997255\n",
      "Training loss after  4000  iterations is :  1.5226217833083215  | validation loss is :  1.4645344921111405\n",
      "Training loss after  4500  iterations is :  1.5226217833082842  | validation loss is :  1.464534498005934\n",
      "[[ 0.34611582]\n",
      " [-0.06069265]]\n",
      "0.535474611992658\n",
      "For fold :  6 / 8\n",
      "Training loss after  0  iterations is :  1.6189392695422706  | validation loss is :  1.7606024987057751\n",
      "Training loss after  500  iterations is :  1.4989916149299534  | validation loss is :  1.6318092764329344\n",
      "Training loss after  1000  iterations is :  1.4983953517915192  | validation loss is :  1.6305619910559428\n",
      "Training loss after  1500  iterations is :  1.4983782720192735  | validation loss is :  1.6305017024979769\n",
      "Training loss after  2000  iterations is :  1.4983776626212648  | validation loss is :  1.6304956527856058\n",
      "Training loss after  2500  iterations is :  1.4983776406929363  | validation loss is :  1.6304946947845553\n",
      "Training loss after  3000  iterations is :  1.4983776399036455  | validation loss is :  1.6304945197704757\n",
      "Training loss after  3500  iterations is :  1.4983776398752353  | validation loss is :  1.6304944868060525\n",
      "Training loss after  4000  iterations is :  1.4983776398742128  | validation loss is :  1.6304944805604928\n",
      "Training loss after  4500  iterations is :  1.498377639874176  | validation loss is :  1.6304944793758749\n",
      "[[ 0.33724211]\n",
      " [-0.05991933]]\n",
      "0.5279137598544273\n",
      "For fold :  7 / 8\n",
      "Training loss after  0  iterations is :  1.6713660954619765  | validation loss is :  1.3753995198332343\n",
      "Training loss after  500  iterations is :  1.549741264104383  | validation loss is :  1.2547941602441726\n",
      "Training loss after  1000  iterations is :  1.5490724621044218  | validation loss is :  1.2556487629679904\n",
      "Training loss after  1500  iterations is :  1.5490528909247703  | validation loss is :  1.2558215935172234\n",
      "Training loss after  2000  iterations is :  1.5490521580913628  | validation loss is :  1.2558544140815342\n",
      "Training loss after  2500  iterations is :  1.5490521303339309  | validation loss is :  1.2558607678539444\n",
      "Training loss after  3000  iterations is :  1.549052129282073  | validation loss is :  1.2558620031936114\n",
      "Training loss after  3500  iterations is :  1.5490521292422124  | validation loss is :  1.2558622436075133\n",
      "Training loss after  4000  iterations is :  1.5490521292407018  | validation loss is :  1.2558622904054086\n",
      "Training loss after  4500  iterations is :  1.5490521292406445  | validation loss is :  1.255862299515312\n",
      "[[ 0.34762293]\n",
      " [-0.05968139]]\n",
      "0.5385716073835441\n",
      "For fold :  0 / 9\n",
      "Training loss after  0  iterations is :  1.6550120161401851  | validation loss is :  1.4880894025579865\n",
      "Training loss after  500  iterations is :  1.5359897014981998  | validation loss is :  1.347667773103486\n",
      "Training loss after  1000  iterations is :  1.5353477567950655  | validation loss is :  1.3464831764496346\n",
      "Training loss after  1500  iterations is :  1.535328801147367  | validation loss is :  1.346370092836454\n",
      "Training loss after  2000  iterations is :  1.5353280922138195  | validation loss is :  1.346351076116327\n",
      "Training loss after  2500  iterations is :  1.5353280654246486  | validation loss is :  1.346347486937471\n",
      "Training loss after  3000  iterations is :  1.535328064411936  | validation loss is :  1.3463467931569237\n",
      "Training loss after  3500  iterations is :  1.5353280643736518  | validation loss is :  1.346346658417554\n",
      "Training loss after  4000  iterations is :  1.5353280643722045  | validation loss is :  1.3463466322257513\n",
      "Training loss after  4500  iterations is :  1.5353280643721496  | validation loss is :  1.3463466271334545\n",
      "[[ 0.33708041]\n",
      " [-0.0573285 ]]\n",
      "0.5340626747869409\n",
      "For fold :  1 / 9\n",
      "Training loss after  0  iterations is :  1.5306567281772112  | validation loss is :  2.320496031220495\n",
      "Training loss after  500  iterations is :  1.401570166585931  | validation loss is :  2.2295107895344857\n",
      "Training loss after  1000  iterations is :  1.4010667610656418  | validation loss is :  2.228938148864075\n",
      "Training loss after  1500  iterations is :  1.4010544500887223  | validation loss is :  2.2289268442664403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  2000  iterations is :  1.4010540913019498  | validation loss is :  2.228927144922556\n",
      "Training loss after  2500  iterations is :  1.401054080792337  | validation loss is :  2.2289272602491845\n",
      "Training loss after  3000  iterations is :  1.4010540804844474  | validation loss is :  2.2289272818189265\n",
      "Training loss after  3500  iterations is :  1.4010540804754272  | validation loss is :  2.2289272855633158\n",
      "Training loss after  4000  iterations is :  1.401054080475163  | validation loss is :  2.2289272862057152\n",
      "Training loss after  4500  iterations is :  1.4010540804751555  | validation loss is :  2.228927286315713\n",
      "[[ 0.34056015]\n",
      " [-0.05879015]]\n",
      "0.5294047341055725\n",
      "For fold :  2 / 9\n",
      "Training loss after  0  iterations is :  1.6156519726359981  | validation loss is :  1.8011538514390104\n",
      "Training loss after  500  iterations is :  1.4962502610061506  | validation loss is :  1.6684877584051718\n",
      "Training loss after  1000  iterations is :  1.4956646264489861  | validation loss is :  1.666358573587389\n",
      "Training loss after  1500  iterations is :  1.4956474917809317  | validation loss is :  1.6661490088629674\n",
      "Training loss after  2000  iterations is :  1.495646867869225  | validation loss is :  1.6661149783490994\n",
      "Training loss after  2500  iterations is :  1.4956468449647873  | validation loss is :  1.6661086735157309\n",
      "Training loss after  3000  iterations is :  1.4956468441237125  | validation loss is :  1.6661074730541556\n",
      "Training loss after  3500  iterations is :  1.4956468440928272  | validation loss is :  1.6661072432879762\n",
      "Training loss after  4000  iterations is :  1.495646844091693  | validation loss is :  1.6661071992681717\n",
      "Training loss after  4500  iterations is :  1.4956468440916515  | validation loss is :  1.6661071908330882\n",
      "[[ 0.3327892 ]\n",
      " [-0.05550387]]\n",
      "0.5273256498641823\n",
      "For fold :  3 / 9\n",
      "Training loss after  0  iterations is :  1.669772648334709  | validation loss is :  1.3495642807886605\n",
      "Training loss after  500  iterations is :  1.5487333138212152  | validation loss is :  1.2233719262577964\n",
      "Training loss after  1000  iterations is :  1.5480482403127747  | validation loss is :  1.2243997213602364\n",
      "Training loss after  1500  iterations is :  1.5480277984229398  | validation loss is :  1.2245951341950299\n",
      "Training loss after  2000  iterations is :  1.5480270261970688  | validation loss is :  1.2246314242228884\n",
      "Training loss after  2500  iterations is :  1.548026996709003  | validation loss is :  1.2246384357778837\n",
      "Training loss after  3000  iterations is :  1.548026995582488  | validation loss is :  1.22463980288932\n",
      "Training loss after  3500  iterations is :  1.5480269955394519  | validation loss is :  1.2246400699636788\n",
      "Training loss after  4000  iterations is :  1.5480269955378076  | validation loss is :  1.2246401221594807\n",
      "Training loss after  4500  iterations is :  1.5480269955377448  | validation loss is :  1.2246401323612468\n",
      "[[ 0.34441078]\n",
      " [-0.06211598]]\n",
      "0.5395940255401899\n",
      "For fold :  4 / 9\n",
      "Training loss after  0  iterations is :  1.6730302729055064  | validation loss is :  1.3170158497600235\n",
      "Training loss after  500  iterations is :  1.5542116483415622  | validation loss is :  1.1683432523752342\n",
      "Training loss after  1000  iterations is :  1.5535722798639597  | validation loss is :  1.1672605687840918\n",
      "Training loss after  1500  iterations is :  1.5535533069257093  | validation loss is :  1.167129706087623\n",
      "Training loss after  2000  iterations is :  1.5535525754180424  | validation loss is :  1.1671056537678155\n",
      "Training loss after  2500  iterations is :  1.5535525468690137  | validation loss is :  1.167100969349135\n",
      "Training loss after  3000  iterations is :  1.5535525457542634  | validation loss is :  1.1671000463214292\n",
      "Training loss after  3500  iterations is :  1.5535525457107349  | validation loss is :  1.1670998640279582\n",
      "Training loss after  4000  iterations is :  1.5535525457090353  | validation loss is :  1.16709982800977\n",
      "Training loss after  4500  iterations is :  1.5535525457089692  | validation loss is :  1.1670998208925498\n",
      "[[ 0.33758361]\n",
      " [-0.056456  ]]\n",
      "0.535529428874369\n",
      "For fold :  5 / 9\n",
      "Training loss after  0  iterations is :  1.6792055592275283  | validation loss is :  1.2525437122957552\n",
      "Training loss after  500  iterations is :  1.5579763228699561  | validation loss is :  1.1265437799688474\n",
      "Training loss after  1000  iterations is :  1.5573044370849705  | validation loss is :  1.1268746224958304\n",
      "Training loss after  1500  iterations is :  1.5572840007204252  | validation loss is :  1.1269082059286346\n",
      "Training loss after  2000  iterations is :  1.5572831997945096  | validation loss is :  1.1269126136718943\n",
      "Training loss after  2500  iterations is :  1.5572831680351251  | validation loss is :  1.1269134025759917\n",
      "Training loss after  3000  iterations is :  1.5572831667751625  | validation loss is :  1.126913556154829\n",
      "Training loss after  3500  iterations is :  1.5572831667251765  | validation loss is :  1.1269135866015247\n",
      "Training loss after  4000  iterations is :  1.5572831667231932  | validation loss is :  1.1269135926601366\n",
      "Training loss after  4500  iterations is :  1.5572831667231146  | validation loss is :  1.1269135938666615\n",
      "[[ 0.34475715]\n",
      " [-0.05489144]]\n",
      "0.5395011485148844\n",
      "For fold :  6 / 9\n",
      "Training loss after  0  iterations is :  1.6257409836891914  | validation loss is :  1.7271141586989103\n",
      "Training loss after  500  iterations is :  1.5046415842798306  | validation loss is :  1.6053651808862575\n",
      "Training loss after  1000  iterations is :  1.5040458645123296  | validation loss is :  1.604393707165128\n",
      "Training loss after  1500  iterations is :  1.5040293617354445  | validation loss is :  1.6043412980482514\n",
      "Training loss after  2000  iterations is :  1.504028787729917  | validation loss is :  1.6043352953988645\n",
      "Training loss after  2500  iterations is :  1.5040287675781456  | validation loss is :  1.6043343044777416\n",
      "Training loss after  3000  iterations is :  1.504028766870433  | validation loss is :  1.6043341235084612\n",
      "Training loss after  3500  iterations is :  1.5040287668455787  | validation loss is :  1.6043340897615426\n",
      "Training loss after  4000  iterations is :  1.5040287668447059  | validation loss is :  1.6043340834432\n",
      "Training loss after  4500  iterations is :  1.5040287668446752  | validation loss is :  1.6043340822593377\n",
      "[[ 0.33954109]\n",
      " [-0.05772857]]\n",
      "0.5308863625655242\n",
      "For fold :  7 / 9\n",
      "Training loss after  0  iterations is :  1.6180925180008847  | validation loss is :  1.7834824018341553\n",
      "Training loss after  500  iterations is :  1.4974192401759843  | validation loss is :  1.658173431407902\n",
      "Training loss after  1000  iterations is :  1.4968030456213117  | validation loss is :  1.657694044740013\n",
      "Training loss after  1500  iterations is :  1.4967854378532313  | validation loss is :  1.6577744714895877\n",
      "Training loss after  2000  iterations is :  1.496784816612086  | validation loss is :  1.6577947041619405\n",
      "Training loss after  2500  iterations is :  1.4967847945140944  | validation loss is :  1.6577986968243272\n",
      "Training loss after  3000  iterations is :  1.4967847937278311  | validation loss is :  1.6577994561668983\n",
      "Training loss after  3500  iterations is :  1.4967847936998553  | validation loss is :  1.6577995996204815\n",
      "Training loss after  4000  iterations is :  1.4967847936988599  | validation loss is :  1.6577996266878026\n",
      "Training loss after  4500  iterations is :  1.4967847936988246  | validation loss is :  1.6577996317937722\n",
      "[[ 0.34073809]\n",
      " [-0.06517668]]\n",
      "0.5289671317075815\n",
      "For fold :  8 / 9\n",
      "Training loss after  0  iterations is :  1.663021511913847  | validation loss is :  1.4148266551735422\n",
      "Training loss after  500  iterations is :  1.5417442829663344  | validation loss is :  1.2922353781025269\n",
      "Training loss after  1000  iterations is :  1.5410941127226714  | validation loss is :  1.2926975988418823\n",
      "Training loss after  1500  iterations is :  1.5410753068881764  | validation loss is :  1.292820777537572\n",
      "Training loss after  2000  iterations is :  1.5410746114872034  | validation loss is :  1.292844916587086\n",
      "Training loss after  2500  iterations is :  1.5410745854832584  | validation loss is :  1.2928495953370276\n",
      "Training loss after  3000  iterations is :  1.5410745845104266  | validation loss is :  1.2928505005655564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  3500  iterations is :  1.5410745844740315  | validation loss is :  1.2928506756607256\n",
      "Training loss after  4000  iterations is :  1.5410745844726699  | validation loss is :  1.2928507095277877\n",
      "Training loss after  4500  iterations is :  1.5410745844726188  | validation loss is :  1.2928507160783769\n",
      "[[ 0.34547272]\n",
      " [-0.05930132]]\n",
      "0.5367297071365704\n",
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  1.6470572589143202  | validation loss is :  1.5467800379939498\n",
      "Training loss after  500  iterations is :  1.5285713702657713  | validation loss is :  1.4007510260274405\n",
      "Training loss after  1000  iterations is :  1.5279456614097926  | validation loss is :  1.3989255317389624\n",
      "Training loss after  1500  iterations is :  1.5279273229559778  | validation loss is :  1.3987383152489201\n",
      "Training loss after  2000  iterations is :  1.5279266439480843  | validation loss is :  1.398706858039867\n",
      "Training loss after  2500  iterations is :  1.5279266185545781  | validation loss is :  1.3987009470326268\n",
      "Training loss after  3000  iterations is :  1.5279266176045545  | validation loss is :  1.3986998101796395\n",
      "Training loss after  3500  iterations is :  1.5279266175690116  | validation loss is :  1.3986995905278992\n",
      "Training loss after  4000  iterations is :  1.5279266175676818  | validation loss is :  1.3986995480511772\n",
      "Training loss after  4500  iterations is :  1.527926617567632  | validation loss is :  1.3986995398355238\n",
      "[[ 0.33497687]\n",
      " [-0.05690447]]\n",
      "0.5316650433456727\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  1.6695344554893319  | validation loss is :  1.3117960424440507\n",
      "Training loss after  500  iterations is :  1.5491381390002266  | validation loss is :  1.178632167215613\n",
      "Training loss after  1000  iterations is :  1.5484559348132112  | validation loss is :  1.1783432923382338\n",
      "Training loss after  1500  iterations is :  1.5484340345555183  | validation loss is :  1.1783513002364048\n",
      "Training loss after  2000  iterations is :  1.5484331442553914  | validation loss is :  1.1783548167023115\n",
      "Training loss after  2500  iterations is :  1.548433107697684  | validation loss is :  1.1783556158074937\n",
      "Training loss after  3000  iterations is :  1.548433106195977  | validation loss is :  1.1783557815956802\n",
      "Training loss after  3500  iterations is :  1.5484331061342897  | validation loss is :  1.1783558153643372\n",
      "Training loss after  4000  iterations is :  1.5484331061317556  | validation loss is :  1.1783558222157349\n",
      "Training loss after  4500  iterations is :  1.5484331061316516  | validation loss is :  1.1783558236046734\n",
      "[[ 0.34414496]\n",
      " [-0.05548524]]\n",
      "0.5364499010244349\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  1.5024806297584101  | validation loss is :  2.548103917172718\n",
      "Training loss after  500  iterations is :  1.3751910572895916  | validation loss is :  2.443088774916381\n",
      "Training loss after  1000  iterations is :  1.3747379261685697  | validation loss is :  2.4419657586623944\n",
      "Training loss after  1500  iterations is :  1.3747279407185737  | validation loss is :  2.4418947570298317\n",
      "Training loss after  2000  iterations is :  1.3747276794873524  | validation loss is :  2.4418857951246813\n",
      "Training loss after  2500  iterations is :  1.3747276726202031  | validation loss is :  2.4418844076802144\n",
      "Training loss after  3000  iterations is :  1.3747276724396598  | validation loss is :  2.4418841844229706\n",
      "Training loss after  3500  iterations is :  1.3747276724349131  | validation loss is :  2.4418841482675204\n",
      "Training loss after  4000  iterations is :  1.3747276724347883  | validation loss is :  2.4418841424062605\n",
      "Training loss after  4500  iterations is :  1.3747276724347848  | validation loss is :  2.4418841414559167\n",
      "[[ 0.33047889]\n",
      " [-0.06285355]]\n",
      "0.5250279589701747\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  1.6275176875700899  | validation loss is :  1.7229938420588136\n",
      "Training loss after  500  iterations is :  1.5051810886749646  | validation loss is :  1.6115344334170323\n",
      "Training loss after  1000  iterations is :  1.5045686921968517  | validation loss is :  1.6107829692942206\n",
      "Training loss after  1500  iterations is :  1.5045511770995705  | validation loss is :  1.6107440977011103\n",
      "Training loss after  2000  iterations is :  1.5045505539473008  | validation loss is :  1.610739741747093\n",
      "Training loss after  2500  iterations is :  1.5045505315836434  | validation loss is :  1.6107390237291372\n",
      "Training loss after  3000  iterations is :  1.50455053078081  | validation loss is :  1.6107388915352727\n",
      "Training loss after  3500  iterations is :  1.5045505307519889  | validation loss is :  1.6107388666264022\n",
      "Training loss after  4000  iterations is :  1.5045505307509541  | validation loss is :  1.610738861911835\n",
      "Training loss after  4500  iterations is :  1.504550530750917  | validation loss is :  1.610738861018737\n",
      "[[ 0.34155578]\n",
      " [-0.05647615]]\n",
      "0.5329198964623338\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  1.685869600600222  | validation loss is :  1.1080154082066997\n",
      "Training loss after  500  iterations is :  1.5650292153577126  | validation loss is :  0.9700693911494145\n",
      "Training loss after  1000  iterations is :  1.5643238517628677  | validation loss is :  0.9713803798010547\n",
      "Training loss after  1500  iterations is :  1.5643009274702389  | validation loss is :  0.9715753061186471\n",
      "Training loss after  2000  iterations is :  1.5642999718574406  | validation loss is :  0.9716111106920278\n",
      "Training loss after  2500  iterations is :  1.5642999315769288  | validation loss is :  0.9716183049673459\n",
      "Training loss after  3000  iterations is :  1.5642999298783065  | validation loss is :  0.9716197764366179\n",
      "Training loss after  3500  iterations is :  1.5642999298066746  | validation loss is :  0.9716200783906737\n",
      "Training loss after  4000  iterations is :  1.564299929803654  | validation loss is :  0.9716201403903387\n",
      "Training loss after  4500  iterations is :  1.5642999298035265  | validation loss is :  0.9716201531219745\n",
      "[[ 0.34540262]\n",
      " [-0.05842747]]\n",
      "0.5407072701796471\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  1.6470579630181719  | validation loss is :  1.5467632852774293\n",
      "Training loss after  500  iterations is :  1.5286500013112718  | validation loss is :  1.4000640460304987\n",
      "Training loss after  1000  iterations is :  1.5280484050583505  | validation loss is :  1.3980203540915381\n",
      "Training loss after  1500  iterations is :  1.528031584253843  | validation loss is :  1.3977843799828427\n",
      "Training loss after  2000  iterations is :  1.5280309817377074  | validation loss is :  1.3977439029069503\n",
      "Training loss after  2500  iterations is :  1.5280309599160207  | validation loss is :  1.397736359136832\n",
      "Training loss after  3000  iterations is :  1.5280309591253505  | validation loss is :  1.3977349290867906\n",
      "Training loss after  3500  iterations is :  1.5280309590967014  | validation loss is :  1.3977346570918645\n",
      "Training loss after  4000  iterations is :  1.5280309590956633  | validation loss is :  1.3977346053251178\n",
      "Training loss after  4500  iterations is :  1.5280309590956256  | validation loss is :  1.3977345954715108\n",
      "[[ 0.33401903]\n",
      " [-0.05511057]]\n",
      "0.5313087502870785\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  1.687170952537904  | validation loss is :  1.0900258590644005\n",
      "Training loss after  500  iterations is :  1.5653210915973712  | validation loss is :  0.9645870311999685\n",
      "Training loss after  1000  iterations is :  1.564636863379363  | validation loss is :  0.9668053784975266\n",
      "Training loss after  1500  iterations is :  1.564616271299632  | validation loss is :  0.9671345824768823\n",
      "Training loss after  2000  iterations is :  1.5646154703045658  | validation loss is :  0.9671943353863878\n",
      "Training loss after  2500  iterations is :  1.5646154387604114  | validation loss is :  0.9672059699284893\n",
      "Training loss after  3000  iterations is :  1.5646154375175216  | validation loss is :  0.9672082700726888\n",
      "Training loss after  3500  iterations is :  1.5646154374685484  | validation loss is :  0.9672087262674105\n",
      "Training loss after  4000  iterations is :  1.564615437466619  | validation loss is :  0.9672088168064062\n",
      "Training loss after  4500  iterations is :  1.5646154374665426  | validation loss is :  0.9672088347777931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3483224 ]\n",
      " [-0.05951399]]\n",
      "0.5405220379862234\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  1.5879701485009319  | validation loss is :  2.0282265780885114\n",
      "Training loss after  500  iterations is :  1.4650830717398053  | validation loss is :  1.9157288064182283\n",
      "Training loss after  1000  iterations is :  1.4645294869031877  | validation loss is :  1.9145677790591034\n",
      "Training loss after  1500  iterations is :  1.4645149966018551  | validation loss is :  1.9144929118123488\n",
      "Training loss after  2000  iterations is :  1.4645145295792297  | validation loss is :  1.914483224531159\n",
      "Training loss after  2500  iterations is :  1.46451451441252  | validation loss is :  1.9144816013243442\n",
      "Training loss after  3000  iterations is :  1.464514513919854  | validation loss is :  1.9144813127573554\n",
      "Training loss after  3500  iterations is :  1.4645145139038505  | validation loss is :  1.914481260877888\n",
      "Training loss after  4000  iterations is :  1.4645145139033307  | validation loss is :  1.9144812515317504\n",
      "Training loss after  4500  iterations is :  1.4645145139033138  | validation loss is :  1.9144812498474122\n",
      "[[ 0.33748291]\n",
      " [-0.05801657]]\n",
      "0.5286227259145092\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  1.6533529895924597  | validation loss is :  1.484959705228396\n",
      "Training loss after  500  iterations is :  1.5337811488491921  | validation loss is :  1.3464524429185545\n",
      "Training loss after  1000  iterations is :  1.5331180214821136  | validation loss is :  1.3465946712211176\n",
      "Training loss after  1500  iterations is :  1.5330979815760633  | validation loss is :  1.3467972849015426\n",
      "Training loss after  2000  iterations is :  1.5330972221457873  | validation loss is :  1.3468423084673058\n",
      "Training loss after  2500  iterations is :  1.5330971930884434  | validation loss is :  1.3468513199070646\n",
      "Training loss after  3000  iterations is :  1.5330971919762466  | validation loss is :  1.3468530906716913\n",
      "Training loss after  3500  iterations is :  1.5330971919336756  | validation loss is :  1.3468534374057868\n",
      "Training loss after  4000  iterations is :  1.5330971919320462  | validation loss is :  1.3468535052534039\n",
      "Training loss after  4500  iterations is :  1.5330971919319838  | validation loss is :  1.346853518527795\n",
      "[[ 0.34417942]\n",
      " [-0.06469791]]\n",
      "0.5316578467243209\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  1.6562967830960804  | validation loss is :  1.4548657706237422\n",
      "Training loss after  500  iterations is :  1.53518547246788  | validation loss is :  1.3318324178397163\n",
      "Training loss after  1000  iterations is :  1.534548915103153  | validation loss is :  1.3319286107926962\n",
      "Training loss after  1500  iterations is :  1.5345307551567804  | validation loss is :  1.3319781417431855\n",
      "Training loss after  2000  iterations is :  1.5345300932854486  | validation loss is :  1.331987776110355\n",
      "Training loss after  2500  iterations is :  1.5345300688951606  | validation loss is :  1.3319896274655776\n",
      "Training loss after  3000  iterations is :  1.5345300679959752  | validation loss is :  1.3319899828832913\n",
      "Training loss after  3500  iterations is :  1.5345300679628249  | validation loss is :  1.331990051119713\n",
      "Training loss after  4000  iterations is :  1.5345300679616023  | validation loss is :  1.3319900642212472\n",
      "Training loss after  4500  iterations is :  1.5345300679615574  | validation loss is :  1.3319900667368283\n",
      "[[ 0.34267186]\n",
      " [-0.05846609]]\n",
      "0.5365366202721291\n",
      "{2: (1.5128465554619095, 1.5129411073792254), 3: (1.5104759993792245, 1.4951161066004357), 4: (1.5142599191765818, 1.5063851029349744), 5: (1.51287203041841, 1.4839413774937251), 6: (1.5130574456788002, 1.4699168337071662), 7: (1.5140970070754494, 1.4742793679498507), 8: (1.5144919267438042, 1.474297176929585), 9: (1.5147533157694322, 1.4794465646425081), 10: (1.5144726027047515, 1.4559566789845473)}\n"
     ]
    }
   ],
   "source": [
    "k_models_video_game = {}\n",
    "for i in range(2,11):\n",
    "    models_rmse,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = i, epochs = 5000, learning_rate = 0.01, loss = \"rmse\")\n",
    "#     print(\"The average loss with k = \", i, \" is \", \" train loss = \", avg_train_loss[-1], \"\")\n",
    "    k_models_video_game[i] = (avg_train_loss[-1], avg_val_loss[-1])\n",
    "print(k_models_video_game)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average loss with k =  2  is   train loss =  1.5128465554619095  validation loss =  1.5129411073792254\n",
      "The average loss with k =  3  is   train loss =  1.5104759993792245  validation loss =  1.4951161066004357\n",
      "The average loss with k =  4  is   train loss =  1.5142599191765818  validation loss =  1.5063851029349744\n",
      "The average loss with k =  5  is   train loss =  1.51287203041841  validation loss =  1.4839413774937251\n",
      "The average loss with k =  6  is   train loss =  1.5130574456788002  validation loss =  1.4699168337071662\n",
      "The average loss with k =  7  is   train loss =  1.5140970070754494  validation loss =  1.4742793679498507\n",
      "The average loss with k =  8  is   train loss =  1.5144919267438042  validation loss =  1.474297176929585\n",
      "The average loss with k =  9  is   train loss =  1.5147533157694322  validation loss =  1.4794465646425081\n",
      "The average loss with k =  10  is   train loss =  1.5144726027047515  validation loss =  1.4559566789845473\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,11):\n",
    "    print(\"The average loss with k = \", i, \" is \", \" train loss = \", k_models_video_game[i][0], \" validation loss = \", k_models_video_game[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  1.6470572589143202  | validation loss is :  1.5467800379939498\n",
      "Training loss after  500  iterations is :  1.5344107383686658  | validation loss is :  1.4102451445596718\n",
      "Training loss after  1000  iterations is :  1.528574156034646  | validation loss is :  1.4007574648375705\n",
      "Training loss after  1500  iterations is :  1.5280297801408154  | validation loss is :  1.3993144903871912\n",
      "Training loss after  2000  iterations is :  1.5279457651984567  | validation loss is :  1.3989262782484062\n",
      "Training loss after  2500  iterations is :  1.5279302947889168  | validation loss is :  1.3987914213921482\n",
      "Training loss after  3000  iterations is :  1.5279273286058863  | validation loss is :  1.3987384779396728\n",
      "[[ 0.33433124]\n",
      " [-0.05625463]]\n",
      "0.5316522604804466\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  1.6695344554893319  | validation loss is :  1.3117960424440507\n",
      "Training loss after  500  iterations is :  1.5552400221977012  | validation loss is :  1.1837251070288528\n",
      "Training loss after  1000  iterations is :  1.5491410686379379  | validation loss is :  1.178632247455201\n",
      "Training loss after  1500  iterations is :  1.5485511187789696  | validation loss is :  1.1783609804715829\n",
      "Training loss after  2000  iterations is :  1.5484560523211732  | validation loss is :  1.1783431702892795\n",
      "Training loss after  2500  iterations is :  1.5484377233076578  | validation loss is :  1.1783471497552318\n",
      "Training loss after  3000  iterations is :  1.5484340415709212  | validation loss is :  1.1783512785315684\n",
      "[[ 0.34337303]\n",
      " [-0.05471403]]\n",
      "0.5364406871643694\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  1.5024806297584101  | validation loss is :  2.548103917172718\n",
      "Training loss after  500  iterations is :  1.380094903466959  | validation loss is :  2.4498126277711068\n",
      "Training loss after  1000  iterations is :  1.3751932558205773  | validation loss is :  2.4430946131273874\n",
      "Training loss after  1500  iterations is :  1.3747928888931047  | validation loss is :  2.4421623427924684\n",
      "Training loss after  2000  iterations is :  1.3747379935328616  | validation loss is :  2.4419661774680894\n",
      "Training loss after  2500  iterations is :  1.3747293416386932  | validation loss is :  2.4419124962101577\n",
      "Training loss after  3000  iterations is :  1.374727943350793  | validation loss is :  2.4418948148659356\n",
      "[[ 0.33012965]\n",
      " [-0.06250671]]\n",
      "0.5250270991851944\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  1.6275176875700899  | validation loss is :  1.7229938420588136\n",
      "Training loss after  500  iterations is :  1.510957210784648  | validation loss is :  1.6172195621118004\n",
      "Training loss after  1000  iterations is :  1.5051838179134065  | validation loss is :  1.6115373054830306\n",
      "Training loss after  1500  iterations is :  1.504650555241197  | validation loss is :  1.6109042882676021\n",
      "Training loss after  2000  iterations is :  1.5045687932192446  | validation loss is :  1.6107831596331934\n",
      "Training loss after  2500  iterations is :  1.5045539701535495  | validation loss is :  1.6107531195905356\n",
      "Training loss after  3000  iterations is :  1.5045511824042874  | validation loss is :  1.610744122989169\n",
      "[[ 0.34095207]\n",
      " [-0.05586002]]\n",
      "0.5329081488862357\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  1.685869600600222  | validation loss is :  1.1080154082066997\n",
      "Training loss after  500  iterations is :  1.5713145349811355  | validation loss is :  0.9715793017315357\n",
      "Training loss after  1000  iterations is :  1.5650322486941157  | validation loss is :  0.9700637513159321\n",
      "Training loss after  1500  iterations is :  1.5644223966174895  | validation loss is :  0.9710155661908679\n",
      "Training loss after  2000  iterations is :  1.5643239732685084  | validation loss is :  0.9713793624674368\n",
      "Training loss after  2500  iterations is :  1.5643048271975535  | validation loss is :  0.9715178564693006\n",
      "Training loss after  3000  iterations is :  1.5643009348859587  | validation loss is :  0.9715751141857873\n",
      "[[ 0.34459701]\n",
      " [-0.05761638]]\n",
      "0.5407011338380171\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  1.6470579630181719  | validation loss is :  1.5467632852774293\n",
      "Training loss after  500  iterations is :  1.5344521747735518  | validation loss is :  1.4096865395988953\n",
      "Training loss after  1000  iterations is :  1.528652759362162  | validation loss is :  1.4000708177675019\n",
      "Training loss after  1500  iterations is :  1.5281273495395746  | validation loss is :  1.3984849939979864\n",
      "Training loss after  2000  iterations is :  1.5280485022615877  | validation loss is :  1.398021277697906\n",
      "Training loss after  2500  iterations is :  1.5280342717850648  | validation loss is :  1.397852333844016\n",
      "Training loss after  3000  iterations is :  1.5280315893608623  | validation loss is :  1.3977845909517588\n",
      "[[ 0.33341099]\n",
      " [-0.05451446]]\n",
      "0.5313051592651662\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  1.687170952537904  | validation loss is :  1.0900258590644005\n",
      "Training loss after  500  iterations is :  1.571617741806722  | validation loss is :  0.9637194699650238\n",
      "Training loss after  1000  iterations is :  1.5653241271244362  | validation loss is :  0.9645794080097614\n",
      "Training loss after  1500  iterations is :  1.5647293022575572  | validation loss is :  0.9662009660086046\n",
      "Training loss after  2000  iterations is :  1.5646369771757351  | validation loss is :  0.966803901714989\n",
      "Training loss after  2500  iterations is :  1.5646196742848064  | validation loss is :  0.9670374504646776\n",
      "Training loss after  3000  iterations is :  1.5646162777689785  | validation loss is :  0.9671342687415097\n",
      "[[ 0.34760279]\n",
      " [-0.05879553]]\n",
      "0.5405221336716407\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  1.5879701485009319  | validation loss is :  2.0282265780885114\n",
      "Training loss after  500  iterations is :  1.4705690415157373  | validation loss is :  1.922706270863344\n",
      "Training loss after  1000  iterations is :  1.4650856250433613  | validation loss is :  1.9157340663572138\n",
      "Training loss after  1500  iterations is :  1.464600956967355  | validation loss is :  1.9147743104224326\n",
      "Training loss after  2000  iterations is :  1.4645295749602734  | validation loss is :  1.9145681954322737\n",
      "Training loss after  2500  iterations is :  1.464517214673711  | validation loss is :  1.9145116134964326\n",
      "Training loss after  3000  iterations is :  1.4645150008050716  | validation loss is :  1.9144929705475893\n",
      "[[ 0.33697616]\n",
      " [-0.05750981]]\n",
      "0.5286181923553358\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  1.6533529895924597  | validation loss is :  1.484959705228396\n",
      "Training loss after  500  iterations is :  1.5398028640551535  | validation loss is :  1.352855414725992\n",
      "Training loss after  1000  iterations is :  1.5337840384046686  | validation loss is :  1.3464556332379234\n",
      "Training loss after  1500  iterations is :  1.5332085669874886  | validation loss is :  1.3463753627002315\n",
      "Training loss after  2000  iterations is :  1.5331181332929675  | validation loss is :  1.3465941571039186\n",
      "Training loss after  2500  iterations is :  1.5331012610256396  | validation loss is :  1.3467298563334684\n",
      "Training loss after  3000  iterations is :  1.533097987811313  | validation loss is :  1.3467970723059555\n",
      "[[ 0.34348862]\n",
      " [-0.06400535]]\n",
      "0.531653838937839\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  1.6562967830960804  | validation loss is :  1.4548657706237422\n",
      "Training loss after  500  iterations is :  1.5412216063486561  | validation loss is :  1.3357315452062182\n",
      "Training loss after  1000  iterations is :  1.5351883519984781  | validation loss is :  1.331832231105729\n",
      "Training loss after  1500  iterations is :  1.5346332549610517  | validation loss is :  1.3318558413234904\n",
      "Training loss after  2000  iterations is :  1.5345490189570588  | validation loss is :  1.331928351178937\n",
      "Training loss after  2500  iterations is :  1.5345336777136258  | validation loss is :  1.3319627980091124\n",
      "Training loss after  3000  iterations is :  1.534530760709808  | validation loss is :  1.3319780881187289\n",
      "[[ 0.3420344 ]\n",
      " [-0.05783226]]\n",
      "0.5365337103233256\n"
     ]
    }
   ],
   "source": [
    "video_game_model_rmse,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y,k=10, epochs = 3500, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZwU9bnv8c8zCwz7MowBQRncUFlmgAH0gIpHg2iI4kKERJG4oCQ3N5orRzzJFYMnNxtRjlEhqIgmRNyCKyqKIm6AA4IBAQHFI6IyQtgXmZnn/lE14wCzwlT3DPV9v17NVFf9+lffrlfTT1dV96/M3RERkfhKSXYAERFJLhUCEZGYUyEQEYk5FQIRkZhTIRARibm0ZAeoqTZt2nh2dnayY4iI1CuLFi362t2zyltW7wpBdnY2+fn5yY4hIlKvmNmnFS3ToSERkZhTIRARibnICoGZTTWzjWa2rILlY8xsSXhbZmZFZtY6qjwiIlK+KM8RTAPuAR4pb6G7/xH4I4CZfR+4yd03R5hH5Iiwb98+1q9fz549e5IdReqgjIwMOnToQHp6erUfE1khcPd5ZpZdzebDgUejyiJyJFm/fj3NmjUjOzsbM0t2HKlD3J1Nmzaxfv16OnXqVO3HJf0cgZk1BgYBT1XSZpSZ5ZtZfkFBQeLCidRBe/bsITMzU0VADmJmZGZm1nhvMemFAPg+8HZlh4XcfYq757l7XlZWuV+DFYkVFQGpyKG8NupCIRhGAg4LffL1Tn793HL2FRVHvSoRkXolqYXAzFoAZwHPRL2udV/v5KG31/Hskg1Rr0rkiJeamkpubi45OTn07NmTd95555D6mThxIrt27Tpo/sUXX0xubi4nnHACLVq0IDc3l9zc3Bqt595772X69OmVtlmwYAE33XRTjXOX51e/+hUTJ06slb4SLbKTxWb2KDAAaGNm64FxQDqAu08Om10MzHb3nVHlKDGgcxYnt23G5DfWcnGP9qSkaNda5FA1atSIJUuWAPDyyy9z66238sYbb9S4n4kTJ3LFFVfQuHHj/ebPnDkTgLlz5zJhwgSef/75ch9fWFhIWlr5b2M//elPq1x/37596du3bw1TH3ki2yNw9+Hu3s7d0929g7s/6O6TyxQB3H2auw+LKkNZZsboAcezeuMOXl3xVSJWKRIL27Zto1WrVqX3//jHP9K7d2+6d+/OuHHjANi5cyff+973yMnJoWvXrjz22GPcfffdbNiwgbPPPpuzzz672uvr0KEDd9xxB/369WPmzJlMnjyZ3r17k5OTw9ChQ9m9ezew/yf0/v37M3bsWPr06UPnzp1L9yxeffVVhgwZUtr+mmuu4ayzzuK4447j3nvvLV3nuHHjOPnkk/nud7/L5ZdfXqNP/n/4wx/o2rUrXbt25c9//jMA27dv5/zzzy/dHk8++SQAY8aM4dRTT6V79+7ccsst1V7H4ap3Yw0dju91a8eE2au4b+5avnvqd3TCTeq9Xz+3nA83bKvVPk89ujnjvt+l0ja7d+8mNzeXPXv28MUXX/Daa68BMHv2bFavXs3ChQtxdy688ELmzZtHQUEBRx99NC+88AIAW7dupUWLFtx55528/vrrtGnTpkYZmzRpwttvvw3Apk2buOGGGwAYO3Ys06ZNY/To0Qc9xt1ZuHAhzz77LOPHj+ell146qM1HH33EnDlz2LJlC6eccgo33HAD7733Hs8//zxLly5l79695Obmcvrpp1cr58KFC5k+fToLFy6kqKiIPn36cNZZZ7FixQqys7N58cUXS7fHV199xaxZs1i+fDlmxpYtW2q0TQ5HXThZnDBpqSmMOvN4lny2hTdXf53sOCL1VsmhoZUrV/LSSy8xYsQI3J3Zs2cze/ZsevToQc+ePVm5ciWrV6+mW7duvPrqq9xyyy28+eabtGjR4rDWf/nll5dOf/DBB5xxxhl069aNGTNmsHz58nIfc8kllwDQq1cv1q1bV26bwYMH06BBA4466ihat25NQUEBb731FkOGDKFhw4Y0b96cwYMHVzvnm2++yaWXXkrjxo1p1qwZQ4YM4a233qJ79+689NJLjB07lrfffpsWLVrQunVrUlJSuO6665g5cyZNmjSp/gY5TLHaIwD4QV4H/vLGWn7/0kr6n9BG5wqkXqvqk3sinH766Xz99dcUFBTg7tx6661cf/31B7VbtGgRs2bN4tZbb2XgwIHcdttth7zOsm+SI0aM4MUXX6Rr16488MADzJ8/v9zHNGzYEAhOdBcWFlbapmw7dz/knBU99pRTTiE/P59Zs2YxZswYBg8ezH/+53+Sn5/PK6+8wowZM5g0aRKzZ88+5HXXRKz2CAAapqXyfwaexPIN23j+n18kO45Ivbdy5UqKiorIzMzkvPPOY+rUqezYsQOAzz//nI0bN7JhwwYaN27MFVdcwc0338zixYsBaNasGdu3bz+s9e/cuZO2bduyb98+/v73vx/28zlQ//79efbZZ9m7dy/bt29n1qxZ1X7smWeeycyZM9m9ezc7duzgmWee4YwzzuDzzz+nadOmXHnllfziF79g8eLFbN++nW3btjF48GDuuusu3n///Vp/LhWJ3R4BwEU57fnLGx8z4eVVDOrSlgZpsauHIoel5BwBBJ96H374YVJTUxk4cCArVqwoPYbetGlT/va3v7FmzRrGjBlDSkoK6enpTJo0CYBRo0Zx/vnn065dO15//fVDyjJ+/Hj69OnDscceS9euXWt9DKbTTz+dQYMG0b17d7Kzs+ndu3eFh7Zuv/12JkyYAEBaWhrr1q1j+PDh9O7dG4DRo0fTrVs3Zs2axdixY0lJSaFBgwZMnjyZrVu3cskll7B3716Ki4u58847a/V5VMYOZ7cnGfLy8rw2Lkwzd9VGRj70Hv8xqDM/GXBCLSQTSYwVK1ZwyimnJDtGrOzYsYOmTZuyc+dO+vfvz8MPP0z37t2THatC5b1GzGyRu+eV1z62H4UHdD6K87p8h7vnrOazzQf/oEVEpMQ111xDbm4uvXr1Yvjw4XW6CByKWB4aKjHu+1049843+PVzy3ngqt7JjiMiddRjjz2W7AiRiu0eAcDRLRtx47kn8uqKjTyz5PNkxxERSYpYFwKAq/t1olfHVvzq6WV8vmV3suOIiCRc7AtBWmoKd/0gl+Ji5xePLaGouH6dPBcROVyxLwQAx2Y25vYLu7Dgk838afaqZMcREUkoFYLQ0LxjGN7nWO6bu5bnlmqoapHKRD0M9e23386tt96637wlS5ZU+bXZAQMGUPL18gsuuKDc8XrKfte/Ik8//TQffvhh6f3bbruNV199tdLHVMfcuXNrNERFoqgQlPHrC7uQ17EVY55cypLPEjfgk0h9UzLW0NKlS/ntb3970Jt2dVVUCIYPH37QN3VmzJjBD3/4w2r3PWvWLFq2bHlIuQ4sBOPHj+fcc889pL7qAxWCMhqkpTDpil5kNWvIyIcW8tFXh/fTd5E4iGIY6s6dO9OyZUsWLFhQOu/xxx9n2LBg1PrRo0eTl5dHly5dStdxoOzsbL7+Ohhc8je/+Q2dO3fm3HPPZdWqbw//3n///aVDWF966aXs2rWLd955h2effZYxY8aQm5vL2rVrGTlyZOlQ0XPmzKFHjx5069aNq6++mr1795aub9y4cfTs2ZNu3bqxcuXKam/DivocO3Zs6bDUN998MwBPPPEEXbt2JScnhzPPPLPa66hMrH9HUJ6sZg2Zfs1pXDb5Ha54YAGPXX86ndokbhRAkRp5cSx8+c/a7bNtNzj/d5U2ScQw1MOHD2fGjBn07duX+fPnk5mZyYknnggEb+ytW7emqKiIc845hw8++KDCH3ktWrSIGTNm8P7771NYWEjPnj3p1asXEIxIet111wHB9QgefPBBfvazn3HhhRcyePBgLrvssv362rNnDyNHjmTOnDmcdNJJjBgxgkmTJnHjjTcC0KZNGxYvXsx9993HhAkTeOCBB6rc3BX1OWLECGbOnMnKlSv3G5Z6/PjxvPzyy7Rv377WhqrWHkE5js1szF+v6UthsTN08jss+3xrsiOJ1CmJGIZ62LBhPPnkkxQXFzNjxgyGDx9euuzxxx+nZ8+e9OjRg+XLl+93GOdAb775JhdffDGNGzemefPmXHjhhaXLli1bVjqE9fTp0yscwrrEqlWr6NSpEyeddBIAV111FfPmzStdXp2hrqvbZ/PmzcnIyODaa6/lH//4R+lV3Pr168fIkSO5//77KSoqqtY6qqI9ggp0btuMx68/nREPLmD4lPlMvrIX/U6o2cUzRCJXxSf3RIhqGOpjjjmG7Oxs3njjDZ566ineffddAD755BMmTJjAe++9R6tWrRg5cmSVA81VdBGqkSNH8vTTT5OTk8O0adOYO3dupf1UNTZbdYa6rm6faWlpLFy4kDlz5jBjxgzuueceXnvtNSZPnsyCBQt44YUXyM3NZcmSJWRmZlZrXRWJbI/AzKaa2UYzW1ZJmwFmtsTMlptZzS94GrETjmrKk6P/jbYtMrjywQVMmbf2sMYmFzkSRTkM9fDhw7nppps4/vjj6dChAxCck2jSpAktWrTgq6++Kr3KV0XKDgW9fft2nnvuudJl27dvp127duzbt2+/C91XlOvkk09m3bp1rFmzBoC//vWvnHXWWdXcUuWrqM8dO3awdetWLrjgAiZOnFh6jei1a9fSt29fxo8fT5s2bfjss88Oa/0Q7R7BNOAe4JHyFppZS+A+YJC7/4+ZHRVhlkN2dMtGzPxpP8Y8sZT/N2sliz/dwm8u7kpm04ZVP1jkCJWoYaiHDh3Kz3/+89Jr/QLk5OTQo0cPunTpwnHHHUe/fv0qzdqzZ08uv/xycnNz6dixI2eccUbpsjvuuIO+ffvSsWNHunXrVvrmP2zYMK677jruvvvu0pPEABkZGTz00EMMHTqUwsJCevfuXXqZzOqaM2dOaVGD4ORveX1u3ryZiy66iD179uDu3HXXXUBwXePVq1fj7pxzzjnk5OTUaP3liXQYajPLBp53967lLPsJcLS7/6omfdbWMNQ15e5MmfcxE2avonlGOv81pCuDurbVdY8l4TQMtVSlPg1DfRLQyszmmtkiMxtRUUMzG2Vm+WaWX1BQkMCI+2Xg+rOO57mf9addywxGT1/MiKkLWfWlvmIqIvVbMgtBGtAL+B5wHvB/zeyk8hq6+xR3z3P3vKysrERmPMjJbZsz8yf9uG3wqXywfivn//c8xjyxlLUFO5KaS0TkUCXzW0Prga/dfSew08zmATnAR0nMVC3pqSlc3b8TF/doz92vrebvC/6HJxevZ1CXtow4PZvTjmutQ0YSKXfXa0zKdSiH+5NZCJ4B7jGzNKAB0Be4K4l5aqxVkwaM+34Xfnr2CTz09ic88u6nvLjsSzpmNuYHecfwvW7tyNaP0aSWZWRksGnTJjIzM1UMZD/uzqZNm8jIyKjR4yI7WWxmjwIDgDbAV8A4IB3A3SeHbcYAPwaKgQfcfWJV/SbrZHF17P6miBeXfcFj733Ggk82A3DSd5oy8NS2nHlSFjnHtKBhWmqSU0p9t2/fPtavX1/rF2mXI0NGRgYdOnQgPT19v/mVnSyO7cXro/bZ5l288uFXzP7wSxZ+splih4ZpKfQ4tiV9OmXSrX0LuhzdnHYtMvSpTkQip0KQZFt2fcPCTzYz/+PNLPhkEx9+sY2Szd6qcTqntGvOcVlNyM5sQsfMJmRnNuaY1o3JSNfeg4jUjsoKgYaYSICWjRswsEtbBnZpC8COvYWs+nIbyzds48MN21jxxTaeXbKBbXsKD3hcOkc1a8h3mmeQFf7NbNKA5o3SadkonRaN0mnROPzbKJ1G6anauxCRGlMhSIKmDdPo1bE1vTq2Lp3n7mzZtY91m3by6aZd/M/mXWzcvoeN2/by1fa9rN24g43b91JYyaU0UwwapafSqEEajRqk0Dg9jYwGqTRKT6FxgzQapafSMC2F9NQU0lKN9NQUGqSlkJYSTKenlvz9djotNYXUFEgxK3ODlJRgOjUl+I1FasmyMm3LW2YYZlBSr0ruB9Pfzueg+VY6TdjOwntla1/Qt5V5XPnrocz8b/uLTlQFOtrMEfYdVfIoM9eBz1gNUlMiOVKgQlBHmBmtmjSgVZMG9Di2Vbltioud7XsL2bZ7H1vL3LbsCv7u+qaQXd8UsXtfEbu/CW679hWx55siNm7fw65vivimsJjCImdfUTHfFH07XVmBEZG64Yazjmfs+SfXer8qBPVISoqVHgY6ppb7dnf2lRSFIg+KRHEx+wqdYneK3HF3ioqh2IN5xeH0QcuKnWIvf5m7l54fcSgzfeB8L5Pt2zYl98tr66X/lNdf+fNLOoyyDEZ1Gi7K83v1cntE023Qdx05l9q9w6Fdca0qKgQCBHskDdKMBmm6RIVI3Oh/vYhIzKkQiIjEnAqBiEjMqRCIiMScCoGISMypEIiIxJwKgYhIzKkQiIjEnAqBiEjMqRCIiMScCoGISMypEIiIxFxkhcDMpprZRjNbVsHyAWa21cyWhLfbosoiIiIVi3L00WnAPcAjlbR5090HR5hBRESqENkegbvPAzZH1b+IiNSOZJ8jON3MlprZi2bWpaJGZjbKzPLNLL+goCCR+UREjnjJLASLgY7ungP8GXi6oobuPsXd89w9LysrK2EBRUTiIGmFwN23ufuOcHoWkG5mbZKVR0QkrpJWCMysrZlZON0nzLIpWXlEROIqsm8NmdmjwACgjZmtB8YB6QDuPhm4DBhtZoXAbmCY15UrRIuIxEhkhcDdh1ex/B6Cr5eKiEgSJftbQyIikmQqBCIiMadCICIScyoEIiIxp0IgIhJzKgQiIjGnQiAiEnMqBCIiMadCICIScyoEIiIxp0IgIhJzKgQiIjGnQiAiEnMqBCIiMadCICIScyoEIiIxp0IgIhJzKgQiIjEXWSEws6lmttHMllXRrreZFZnZZVFlERGRikW5RzANGFRZAzNLBX4PvBxhDhERqURkhcDd5wGbq2j2M+ApYGNUOUREpHJJO0dgZu2Bi4HJycogIiLJPVk8EbjF3Yuqamhmo8ws38zyCwoKEhBNRCQ+0pK47jxghpkBtAEuMLNCd3/6wIbuPgWYApCXl+cJTSkicoRLWiFw904l02Y2DXi+vCIgIiLRiqwQmNmjwACgjZmtB8YB6QDurvMCIiJ1RGSFwN2H16DtyKhyiIhI5fTLYhGRmFMhEBGJORUCEZGYUyEQEYk5FQIRkZhTIRARiTkVAhGRmFMhEBGJORUCEZGYq1YhMLOfm1lzCzxoZovNbGDU4UREJHrV3SO42t23AQOBLODHwO8iSyUiIglT3UJg4d8LgIfcfWmZeSIiUo9VtxAsMrPZBIXgZTNrBhRHF0tERBKluqOPXgPkAh+7+y4za01weEhEROq56u4RnA6scvctZnYF8Ctga3SxREQkUapbCCYBu8wsB/gP4FPgkchSiYhIwlS3EBS6uwMXAf/t7v8NNIsuloiIJEp1zxFsN7NbgSuBM8wslfCykyIiUr9Vd4/gcmAvwe8JvgTaA3+MLJWIiCRMtQpB+OY/HWhhZoOBPe5e6TkCM5tqZhvNbFkFyy8ysw/MbImZ5ZtZ/xqnFxGRw1bdISZ+ACwEhgI/ABaY2WVVPGwaMKiS5XOAHHfPBa4GHqhOFhERqV3VPUfwS6C3u28EMLMs4FXgyYoe4O7zzCy7kuU7ytxtAng1s4iISC2q7jmClJIiENpUg8dWyMwuNrOVwAsEewUVtRsVHj7KLygoONzViohIGdV9M3/JzF42s5FmNpLgjXvW4a7c3We6+8nAEOCOStpNcfc8d8/Lyso63NWKiEgZ1To05O5jzOxSoB/BYHNT3H1mbYUIDyMdb2Zt3P3r2upXRESqVt1zBLj7U8BTtbViMzsBWOvubmY9gQYEh5xERCSBKi0EZrad8k/iGuDu3rySxz4KDADamNl6YBzhj9DcfTJwKTDCzPYBu4HLw18vi4hIAlVaCNz9kIeRcPfhVSz/PfD7Q+1fRERqh65ZLCIScyoEIiIxp0IgIhJzKgQiIjGnQiAiEnMqBCIiMadCICIScyoEIiIxp0IgIhJzKgQiIjGnQiAiEnMqBCIiMadCICIScyoEIiIxp0IgIhJzKgQiIjGnQiAiEnMqBCIiMRdZITCzqWa20cyWVbD8R2b2QXh7x8xyosoiIiIVi3KPYBowqJLlnwBnuXt34A5gSoRZRESkApVevP5wuPs8M8uuZPk7Ze7OBzpElUVERCpWV84RXAO8WNFCMxtlZvlmll9QUJDAWCIiR76kFwIzO5ugENxSURt3n+Luee6el5WVlbhwIiIxENmhoeows+7AA8D57r4pmVlEROIqaXsEZnYs8A/gSnf/KFk5RETiLrI9AjN7FBgAtDGz9cA4IB3A3ScDtwGZwH1mBlDo7nlR5RERkfJF+a2h4VUsvxa4Nqr1i4hI9ST9ZLGIiCSXCoGISMypEIiIxJwKgYhIzKkQiIjEnAqBiEjMqRCIiMScCoGISMypEIiIxJwKgYhIzKkQiIjEnAqBiEjMqRCIiMScCoGISMypEIiIxJwKgYhIzKkQiIjEnAqBiEjMRVYIzGyqmW00s2UVLD/ZzN41s71mdnNUOUREpHJR7hFMAwZVsnwz8L+BCRFmEBGRKkRWCNx9HsGbfUXLN7r7e8C+qDKIiEjVdI5ARCTm6kUhMLNRZpZvZvkFBQXJjiMickSpF4XA3ae4e56752VlZR1aJxtXwgs3Q+E3tRtORKSeqxeFoFZs/Qzeux8+ejHZSURE6pS0qDo2s0eBAUAbM1sPjAPSAdx9spm1BfKB5kCxmd0InOru2yIJdPy/Q7OjYfFf4dSLIlmFiEh9FFkhcPfhVSz/EugQ1foPkpIKuT+Et+6ErZ9Di/YJW7WISF0Wn0NDAD1+BF4MS/+e7CQiInVGvApB6+Mg+wx4/29QXJzsNCIidUK8CgFAzxHwr3Ww9rVkJxERqRPiVwhOHQJN28L8e5OdRESkTohfIUhrAH2uDfYINq5IdhoRkaSLXyEA6HU1pGXA/EnJTiIiknTxLARNMqH75fDBY7D9y2SnERFJqngWAoB+P4eiffDWxGQnERFJqvgWgszjIWc45E+FbV8kO42ISNLEtxAAnHkzeFHwa2MRkZiKdyFo3SkYdiL/Idi0NtlpRESSIt6FAODsX0JaQ3j5l8lOIiKSFCoEzdrCmWOC4anXvJrsNCIiCadCAHDa6GAcoln/Aft2JzuNiEhCqRBAcGho8ETYvBZe+69kpxERSSgVghLHnQW9r4V374VP3012GhGRhFEhKOvcX0PLY+Efo2DX5mSnERFJCBWCsho2haEPwY4v4alrobgo2YlERCKnQnCg9r3g/D/A2jnw2h3JTiMiErnICoGZTTWzjWa2rILlZmZ3m9kaM/vAzHpGlaXGeo0Mbm/dBQv+kuw0IiKRinKPYBowqJLl5wMnhrdRQN0ZE9oMLvgTdP4evHgLLH0s2YlERCITWSFw93lAZWdcLwIe8cB8oKWZtYsqT42lpsFlD0J2f5h5fTA4nYjIESiZ5wjaA5+Vub8+nHcQMxtlZvlmll9QUJCQcACkN4IfPQEnDoTnb4LXf6uL3ovIESeZhcDKmeflNXT3Ke6e5+55WVlZEcc6QHojGDYdcn4Ib/wOZvwQ9mxNbAYRkQglsxCsB44pc78DsCFJWSqXmg5D7gu+TbTmFZjUD9bMSXYqEZFakcxC8CwwIvz20GnAVnevu1eIMYO+18OPXwr2Ev52CcwcDVs/T3YyEZHDkhZVx2b2KDAAaGNm64FxQDqAu08GZgEXAGuAXcCPo8pSq47pDde/GRwmevdeWP4P6HsDnPYTaPadZKcTEakxcy/3sHydlZeX5/n5+cmOEfjXp8Egdf98Ijh81G0o9PoxdMgL9iBEROoIM1vk7nnlLlMhqAWb1sL8SbBkOuzbBS07QtdLgm8btc+DtAbJTigiMadCkCh7tsHKF4I9hI/nBtdDTm8CHf8tGLqiXU5wa3609hhEJKEqKwSRnSOIpYzmkDs8uO3eAuvehLWvw7q3wqufhUW3QVNolf3trelR0CQLGreBJm2gUcuggKQ3ggZNICU12tzu4MVQXBgMtFdcGBSx4vDmRWWWHXC/bLvSxxUGv7fYr5/CaNdR8hzwcqa9/PmE9w+apoL5lU1Xo8+y344+cN5+H8j8oOYHt6tpX5XNq6Svcvs/xFxVZq1KDdomvd+Isva/Ec69vfrtq0mFICqNWsIp3w9uAHt3wFfL4YulwQVw/rUOvl4dFIjCPZX3ldowKAqp6WCpQWGw1GCvonQ65ds3TC/+9lbyRlo67WXalflbJ4XPLyUtfN5pkJJS5n7q/s/fLHhMhdME97FgXlXTKWUfW432ZhW3L/nZzH57ggfOK7OsOvP226k83L4qy1XD/svb261uX1Wp0Z50kvuNos9jT6tBn9WnQpAoDZvCsX2DW1nusHc77Poadm4K/u7eAvt2BpfN/GZXMP3NrjKfhkve6Iv2f6O3A94YU1LCN8KS+SkHLCt5cy15s0359n65b7xp+7/57vcmXcH9aq2jnLaWGqxbRCKnQpBsZsEhpYzmwXWTRUQSTB+5RERiToVARCTmVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRERiToVARCTm6t2gc2ZWAHx6iA9vA3xdi3GiVp/y1qesUL/y1qesUL/y1qescHh5O7p7udf6rXeF4HCYWX5Fo+/VRfUpb33KCvUrb33KCvUrb33KCtHl1aEhEZGYUyEQEYm5uBWCKckOUEP1KW99ygr1K299ygr1K299ygoR5Y3VOQIRETlY3PYIRETkACoEIiIxF5tCYGaDzGyVma0xs7HJzgNgZuvM7J9mtsTM8sN5rc3sFTNbHf5tFc43M7s7zP+BmfVMQL6pZrbRzJaVmVfjfGZ2Vdh+tZldlcCst5vZ5+H2XWJmF5RZdmuYdZWZnVdmfkJeJ2Z2jJm9bmYrzGy5mf08nF/ntm8lWevk9jWzDDNbaGZLw7y/Dud3MrMF4XZ6zMwahPMbhvfXhMuzq3oeCcg6zcw+KbNtc16DDr8AAAXUSURBVMP50bwO3P2IvwGpwFrgOKABsBQ4tQ7kWge0OWDeH4Cx4fRY4Pfh9AXAiwQXNz0NWJCAfGcCPYFlh5oPaA18HP5tFU63SlDW24Gby2l7avgaaAh0Cl8bqYl8nQDtgJ7hdDPgozBXndu+lWStk9s33EZNw+l0YEG4zR4HhoXzJwOjw+mfAJPD6WHAY5U9jwRlnQZcVk77SF4Hcdkj6AOscfeP3f0bYAZwUZIzVeQi4OFw+mFgSJn5j3hgPtDSzNpFGcTd5wGbDzPfecAr7r7Z3f8FvAIMSlDWilwEzHD3ve7+CbCG4DWSsNeJu3/h7ovD6e3ACqA9dXD7VpK1IkndvuE22hHeTQ9vDvw78GQ4/8BtW7LNnwTOMTOr5HkkImtFInkdxKUQtAc+K3N/PZW/kBPFgdlmtsjMRoXzvuPuX0DwHxA4KpxfV55DTfMlO/f/Cnehp5YcZqkkU1KyhociehB8GqzT2/eArFBHt6+ZpZrZEmAjwZviWmCLuxeWs+7SXOHyrUBmovIemNXdS7btb8Jte5eZNTww6wGZDitrXAqBlTOvLnxvtp+79wTOB35qZmdW0rauPocSFeVLZu5JwPFALvAF8Kdwfp3JamZNgaeAG919W2VNy5mX0MzlZK2z29fdi9w9F+hA8Cn+lErWndS8B2Y1s67ArcDJQG+Cwz23RJk1LoVgPXBMmfsdgA1JylLK3TeEfzcCMwlesF+VHPIJ/24Mm9eV51DTfEnL7e5fhf/JioH7+Xa3vk5kNbN0gjfW6e7+j3B2ndy+5WWt69s3zLgFmEtwPL2lmaWVs+7SXOHyFgSHGROat0zWQeHhOHf3vcBDRLxt41II3gNODL810IDghNCzyQxkZk3MrFnJNDAQWBbmKjnjfxXwTDj9LDAi/NbAacDWkkMICVbTfC8DA82sVXjoYGA4L3IHnEO5mGD7lmQdFn5bpBNwIrCQBL5OwmPQDwIr3P3OMovq3PatKGtd3b5mlmVmLcPpRsC5BOc1XgcuC5sduG1LtvllwGsenIGt6HlEnXVlmQ8DRnAuo+y2rf3XwaGe7a5vN4Kz7R8RHCv8ZR3IcxzBNxKWAstLMhEcm5wDrA7/tvZvv11wb5j/n0BeAjI+SrDLv4/gE8c1h5IPuJrgRNsa4McJzPrXMMsH4X+gdmXa/zLMugo4P9GvE6A/wa77B8CS8HZBXdy+lWStk9sX6A68H+ZaBtxW5v/cwnA7PQE0DOdnhPfXhMuPq+p5JCDra+G2XQb8jW+/WRTJ60BDTIiIxFxcDg2JiEgFVAhERGJOhUBEJOZUCEREYk6FQEQk5lQIRCJmZgPM7Plk5xCpiAqBiEjMqRCIhMzsinBs+CVm9pdwMLAdZvYnM1tsZnPMLCtsm2tm88NBwWbat9cNOMHMXrVgfPnFZnZ82H1TM3vSzFaa2fTwF6OY2e/M7MOwnwlJeuoScyoEIoCZnQJcTjAQYC5QBPwIaAIs9mBwwDeAceFDHgFucffuBL/wLJk/HbjX3XOAfyP4tTMEI3beSDDG/XFAPzNrTTA0Q5ewn/+K9lmKlE+FQCRwDtALeC8cEvgcgjfsYuCxsM3fgP5m1gJo6e5vhPMfBs4Mx45q7+4zAdx9j7vvCtssdPf1HgzQtgTIBrYBe4AHzOwSoKStSEKpEIgEDHjY3XPDW2d3v72cdpWNyVLeUMAl9paZLgLSPBj7vg/BqJ5DgJdqmFmkVqgQiATmAJeZ2VFQeu3gjgT/R0pGrPwh8Ja7bwX+ZWZnhPOvBN7wYIz+9WY2JOyjoZk1rmiF4fj+Ldx9FsFho9wonphIVdKqbiJy5HP3D83sVwRXjEshGMX0p8BOoIuZLSK4ctXl4UOuAiaHb/QfAz8O518J/MXMxod9DK1ktc2AZ8wsg2Bv4qZafloi1aLRR0UqYWY73L1psnOIREmHhkREYk57BCIiMac9AhGRmFMhEBGJORUCEZGYUyEQEYk5FQIRkZj7/7bK1JRa77tOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot([x for x in range(len(video_game_model_rmse[6][2]))],video_game_model_rmse[6][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(video_game_model_rmse[6][3]))], video_game_model_rmse[6][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  0.5308240845351233  | validation loss is :  0.5582345900658288\n",
      "Training loss after  500  iterations is :  0.45700100148196915  | validation loss is :  0.4795379001896245\n",
      "Training loss after  1000  iterations is :  0.4569978303085981  | validation loss is :  0.47944268317976707\n",
      "Training loss after  1500  iterations is :  0.45699777289358195  | validation loss is :  0.47943061217561195\n",
      "Training loss after  2000  iterations is :  0.45699777279284237  | validation loss is :  0.47943060070940163\n",
      "Training loss after  2500  iterations is :  0.4569977728276957  | validation loss is :  0.4794305500677892\n",
      "Training loss after  3000  iterations is :  0.4569977728672719  | validation loss is :  0.47943060865270953\n",
      "[[ 0.05115998]\n",
      " [-0.00779574]]\n",
      "0.1862477570279814\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  0.5368040140891872  | validation loss is :  0.5039736684619988\n",
      "Training loss after  500  iterations is :  0.46190991979422713  | validation loss is :  0.4347683341884807\n",
      "Training loss after  1000  iterations is :  0.46190693547166356  | validation loss is :  0.4347958047092254\n",
      "Training loss after  1500  iterations is :  0.4619068657720417  | validation loss is :  0.4348008538541107\n",
      "Training loss after  2000  iterations is :  0.46190686358919036  | validation loss is :  0.4348008024652787\n",
      "Training loss after  2500  iterations is :  0.4619068638362521  | validation loss is :  0.4348008491956854\n",
      "Training loss after  3000  iterations is :  0.4619068636272842  | validation loss is :  0.43480080120157316\n",
      "[[ 0.0539893]\n",
      " [-0.0088684]]\n",
      "0.1886901043397474\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  0.5250249219113444  | validation loss is :  0.6104069419509276\n",
      "Training loss after  500  iterations is :  0.4507394435633073  | validation loss is :  0.535763539378791\n",
      "Training loss after  1000  iterations is :  0.450737760870614  | validation loss is :  0.5357292446135243\n",
      "Training loss after  1500  iterations is :  0.4507377510336586  | validation loss is :  0.5357232126951974\n",
      "Training loss after  2000  iterations is :  0.4507377461855361  | validation loss is :  0.5357199537939231\n",
      "Training loss after  2500  iterations is :  0.45073774574760533  | validation loss is :  0.5357191940581163\n",
      "Training loss after  3000  iterations is :  0.4507377453863947  | validation loss is :  0.5357185027852057\n",
      "[[ 0.05192955]\n",
      " [-0.00918731]]\n",
      "0.1861892736093519\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  0.5333169402538712  | validation loss is :  0.5358767205266307\n",
      "Training loss after  500  iterations is :  0.45949059528428143  | validation loss is :  0.45704644327887933\n",
      "Training loss after  1000  iterations is :  0.45948862256779627  | validation loss is :  0.4569629884077607\n",
      "Training loss after  1500  iterations is :  0.45948860295974403  | validation loss is :  0.45695742323179195\n",
      "Training loss after  2000  iterations is :  0.45948859629088484  | validation loss is :  0.45695617785653236\n",
      "Training loss after  2500  iterations is :  0.45948859614702103  | validation loss is :  0.456955144093709\n",
      "Training loss after  3000  iterations is :  0.4594885960159642  | validation loss is :  0.4569541251646986\n",
      "[[ 0.05220438]\n",
      " [-0.00773706]]\n",
      "0.18499368644913328\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  0.5401555127267894  | validation loss is :  0.4742788749251945\n",
      "Training loss after  500  iterations is :  0.465944629217207  | validation loss is :  0.39880325452941007\n",
      "Training loss after  1000  iterations is :  0.4659420178268825  | validation loss is :  0.3988145144889291\n",
      "Training loss after  1500  iterations is :  0.465941917952458  | validation loss is :  0.39880629556344416\n",
      "Training loss after  2000  iterations is :  0.46594191801883683  | validation loss is :  0.39880631256905685\n",
      "Training loss after  2500  iterations is :  0.4659419182658854  | validation loss is :  0.39880646230895117\n",
      "Training loss after  3000  iterations is :  0.4659419179440914  | validation loss is :  0.3988062336369916\n",
      "[[ 0.05284384]\n",
      " [-0.00971793]]\n",
      "0.1883039808599671\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  0.5310440619392569  | validation loss is :  0.5561879114302812\n",
      "Training loss after  500  iterations is :  0.4566765182657163  | validation loss is :  0.48226045920080896\n",
      "Training loss after  1000  iterations is :  0.4566752318022369  | validation loss is :  0.4822506596028728\n",
      "Training loss after  1500  iterations is :  0.45667521223032703  | validation loss is :  0.4822490430501804\n",
      "Training loss after  2000  iterations is :  0.4566752027539697  | validation loss is :  0.4822490324353692\n",
      "Training loss after  2500  iterations is :  0.4566752023078166  | validation loss is :  0.4822486467495682\n",
      "Training loss after  3000  iterations is :  0.45667520199022094  | validation loss is :  0.48224834235715347\n",
      "[[ 0.05297372]\n",
      " [-0.00899012]]\n",
      "0.18670299727520964\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  0.5417252608493387  | validation loss is :  0.4600777977259126\n",
      "Training loss after  500  iterations is :  0.46717693399251486  | validation loss is :  0.3877239934043705\n",
      "Training loss after  1000  iterations is :  0.46717287060396945  | validation loss is :  0.3877859804910528\n",
      "Training loss after  1500  iterations is :  0.4671728010040781  | validation loss is :  0.3877934555908139\n",
      "Training loss after  2000  iterations is :  0.4671727998974951  | validation loss is :  0.38779483678685267\n",
      "Training loss after  2500  iterations is :  0.467172799848045  | validation loss is :  0.38779504938009146\n",
      "Training loss after  3000  iterations is :  0.4671728001448135  | validation loss is :  0.38779509077381985\n",
      "[[ 0.05418576]\n",
      " [-0.00921167]]\n",
      "0.18997541038079785\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  0.5278806406592675  | validation loss is :  0.5846080191502094\n",
      "Training loss after  500  iterations is :  0.45386264929587883  | validation loss is :  0.507655567857395\n",
      "Training loss after  1000  iterations is :  0.45386039358526925  | validation loss is :  0.5075857097810159\n",
      "Training loss after  1500  iterations is :  0.45386038449324034  | validation loss is :  0.5075830431817899\n",
      "Training loss after  2000  iterations is :  0.4538603840581459  | validation loss is :  0.5075830328520081\n",
      "Training loss after  2500  iterations is :  0.4538603849527464  | validation loss is :  0.5075828773640527\n",
      "Training loss after  3000  iterations is :  0.453860384635514  | validation loss is :  0.5075831100566547\n",
      "[[ 0.05092704]\n",
      " [-0.00869035]]\n",
      "0.18648235528676957\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  0.5323114242041602  | validation loss is :  0.5445002992220227\n",
      "Training loss after  500  iterations is :  0.4577124763542545  | validation loss is :  0.47264208164259003\n",
      "Training loss after  1000  iterations is :  0.4577071424521441  | validation loss is :  0.4727397741221356\n",
      "Training loss after  1500  iterations is :  0.4577072122776315  | validation loss is :  0.4727387659278922\n",
      "Training loss after  2000  iterations is :  0.4577071615026447  | validation loss is :  0.4727394143758305\n",
      "Training loss after  2500  iterations is :  0.4577072292737767  | validation loss is :  0.4727386722804038\n",
      "Training loss after  3000  iterations is :  0.45770717288088814  | validation loss is :  0.4727392030607163\n",
      "[[ 0.05450344]\n",
      " [-0.00861333]]\n",
      "0.1900019937529169\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  0.5363255133913737  | validation loss is :  0.508001196888091\n",
      "Training loss after  500  iterations is :  0.46157600926190123  | validation loss is :  0.4375283719365609\n",
      "Training loss after  1000  iterations is :  0.4615709230413058  | validation loss is :  0.4376185825575507\n",
      "Training loss after  1500  iterations is :  0.46157094992966313  | validation loss is :  0.4376179845804506\n",
      "Training loss after  2000  iterations is :  0.4615709785641247  | validation loss is :  0.4376173873920499\n",
      "Training loss after  2500  iterations is :  0.46157091636162584  | validation loss is :  0.4376187633847281\n",
      "Training loss after  3000  iterations is :  0.46157094234178686  | validation loss is :  0.43761813901802216\n",
      "[[ 0.05444914]\n",
      " [-0.0096142 ]]\n",
      "0.18994550408721278\n"
     ]
    }
   ],
   "source": [
    "video_game_model_mae,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y,k=10, epochs = 3500,learning_rate = 0.005,loss = \"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot([x for x in range(len(video_game_model_mae[6][2]))],video_game_model_mae[6][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(video_game_model_mae[6][3]))], video_game_model_mae[6][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for LR MAE Loss\n",
      "For CV number  0 the train loss =  0.45699777280210174  and the val loss =  0.47943058759985263\n",
      "For CV number  1 the train loss =  0.46190686364648736  and the val loss =  0.4348008539326736\n",
      "For CV number  2 the train loss =  0.4507377454538373  and the val loss =  0.5357182538972228\n",
      "For CV number  3 the train loss =  0.4594885958767579  and the val loss =  0.45695310333692807\n",
      "For CV number  4 the train loss =  0.46594191784987354  and the val loss =  0.39880624409400794\n",
      "For CV number  5 the train loss =  0.45667520159297104  and the val loss =  0.48224799944922686\n",
      "For CV number  6 the train loss =  0.4671727997697697  and the val loss =  0.3877950464553649\n",
      "For CV number  7 the train loss =  0.4538603841330389  and the val loss =  0.5075830230387078\n",
      "For CV number  8 the train loss =  0.45770713510567584  and the val loss =  0.4727399062916709\n",
      "For CV number  9 the train loss =  0.4615709789592122  and the val loss =  0.43761736361008263\n",
      "Stats for LR RMSE Loss\n",
      "For CV number  0 the train loss =  1.5279267556929685  and the val loss =  1.3987164153298155\n",
      "For CV number  1 the train loss =  1.5484332965112981  and the val loss =  1.1783536368466818\n",
      "For CV number  2 the train loss =  1.3747277165905787  and the val loss =  2.4418883289997884\n",
      "For CV number  3 the train loss =  1.5045506547703018  and the val loss =  1.610740976830723\n",
      "For CV number  4 the train loss =  1.5643001370387803  and the val loss =  0.9715999612618064\n",
      "For CV number  5 the train loss =  1.5280310795873362  and the val loss =  1.39775618857003\n",
      "For CV number  6 the train loss =  1.564615604952  and the val loss =  0.9671759118931997\n",
      "For CV number  7 the train loss =  1.4645146020700524  and the val loss =  1.914486024992358\n",
      "For CV number  8 the train loss =  1.5330973483086257  and the val loss =  1.3468281615026443\n",
      "For CV number  9 the train loss =  1.5345302015463846  and the val loss =  1.3319848041491635\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats for LR MAE Loss\")\n",
    "for i in range(len(video_game_model_mae)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", video_game_model_mae[i][0], \" and the val loss = \", video_game_model_mae[i][1] )\n",
    "print(\"Stats for LR RMSE Loss\")\n",
    "for i in range(len(video_game_model_rmse)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", video_game_model_rmse[i][0], \" and the val loss = \", video_game_model_rmse[i][1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>User_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>40.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Global_Sales  Critic_Score User_Score\n",
       "0         82.53          76.0          8\n",
       "1         40.24           NaN        NaN\n",
       "2         35.52          82.0        8.3\n",
       "3         32.77          80.0          8\n",
       "4         31.37           NaN        NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"LR_dataset/VideoGameDataset - Video_Games_Sales_as_at_22_Dec_2016.csv\",  usecols=['Critic_Score','User_Score','Global_Sales'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Global_Sales       0\n",
       "Critic_Score    8582\n",
       "User_Score      6704\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0    256\n",
       "71.0    254\n",
       "75.0    245\n",
       "78.0    240\n",
       "73.0    238\n",
       "       ... \n",
       "20.0      3\n",
       "17.0      1\n",
       "22.0      1\n",
       "13.0      1\n",
       "21.0      1\n",
       "Name: Critic_Score, Length: 82, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Critic_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tbd    2425\n",
       "7.8     324\n",
       "8       290\n",
       "8.2     282\n",
       "8.3     254\n",
       "       ... \n",
       "0.5       2\n",
       "1         2\n",
       "0.6       2\n",
       "9.7       1\n",
       "0         1\n",
       "Name: User_Score, Length: 96, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"User_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical_sol(X,y):\n",
    "#     bias = np.zeros((X.shape[0],1))\n",
    "#     bias.fill(1)\n",
    "#     X = np.append(X,bias, axis = 1)\n",
    "#     print(X)\n",
    "    W = np.dot(np.linalg.inv(np.dot(X.T,X)), np.dot(X.T,y))\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = MyPreProcessor()\n",
    "X,y = preproc.pre_process(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]  #number of examples\n",
    "fold_size = int(m/10)\n",
    "start = fold_size\n",
    "end = 2*fold_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_i = np.concatenate((X[0:start], X[end+1:]))\n",
    "ytrain_i = np.concatenate((y[0:start],y[end+1:]))\n",
    "X_test =  X[start:end]\n",
    "y_test = y[start:end]\n",
    "\n",
    "bias = np.zeros((Xtrain_i.shape[0],1))\n",
    "bias.fill(1)\n",
    "Xtrain_i = np.append(Xtrain_i,bias, axis = 1)\n",
    "\n",
    "bias = np.zeros((X_test.shape[0],1))\n",
    "bias.fill(1)\n",
    "X_test = np.append(X_test,bias, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.15466873 -1.11591862 -1.08720356 ... -1.05935132 -1.10529227\n",
      "   1.        ]\n",
      " [-1.15466873  1.04915733  1.02892884 ...  0.82008506  1.51691176\n",
      "   1.        ]\n",
      " [-1.15466873 -0.61628571 -0.63374662 ... -0.62142439 -0.74608624\n",
      "   1.        ]\n",
      " ...\n",
      " [ 1.26171644 -1.36573508 -1.33912408 ... -1.2691913  -1.21305408\n",
      "   1.        ]\n",
      " [-1.15466873 -0.90773824 -1.08720356 ... -1.02285741 -1.03345107\n",
      "   1.        ]\n",
      " [-1.15466873  1.34060986  1.02892884 ...  1.18502416  0.96014241\n",
      "   1.        ]]\n",
      "[[ 0.05352386  0.50788834  0.42431958 ...  0.24530597 -0.09951538\n",
      "   1.        ]\n",
      " [ 1.26171644 -0.07501672 -0.18028967 ... -0.61230092 -0.11029156\n",
      "   1.        ]\n",
      " [ 1.26171644 -0.15828888 -0.18028967 ... -0.35228181 -0.48027378\n",
      "   1.        ]\n",
      " ...\n",
      " [-1.15466873  0.71606872  0.67624011 ...  0.43233726  0.19503356\n",
      "   1.        ]\n",
      " [ 0.05352386 -0.36646926 -0.38182609 ... -0.27473225 -0.57007528\n",
      "   1.        ]\n",
      " [-1.15466873  0.84097695  0.67624011 ...  0.50076334  0.51472693\n",
      "   1.        ]]\n",
      "(3758, 9)\n",
      "(417, 9)\n",
      "(3758, 1)\n",
      "(417, 1)\n",
      "[[ 6.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " ...\n",
      " [10.]\n",
      " [15.]\n",
      " [10.]]\n",
      "[[ 9.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [12.]\n",
      " [12.]\n",
      " [ 4.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 6.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [11.]\n",
      " [11.]\n",
      " [20.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [18.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [14.]\n",
      " [13.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [18.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]\n",
      " [11.]\n",
      " [10.]\n",
      " [10.]\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [14.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [16.]\n",
      " [11.]\n",
      " [ 6.]\n",
      " [13.]\n",
      " [13.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [ 3.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [13.]\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [16.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [26.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [14.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [11.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [17.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [19.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [17.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [14.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [18.]\n",
      " [10.]\n",
      " [17.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [ 6.]\n",
      " [14.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [12.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [15.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [10.]\n",
      " [10.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [12.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [18.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 6.]\n",
      " [11.]\n",
      " [11.]\n",
      " [13.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [13.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [ 5.]\n",
      " [16.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 5.]\n",
      " [14.]\n",
      " [10.]\n",
      " [10.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 6.]\n",
      " [15.]\n",
      " [15.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [15.]\n",
      " [12.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [18.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [13.]\n",
      " [12.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [15.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 4.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [ 5.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 3.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [11.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 6.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [12.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [12.]\n",
      " [12.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [18.]\n",
      " [ 4.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [10.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [11.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [13.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 3.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [13.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [15.]\n",
      " [16.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [13.]\n",
      " [14.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [10.]\n",
      " [11.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [11.]\n",
      " [11.]\n",
      " [13.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 9.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(Xtrain_i)\n",
    "print(Xtrain_i.shape)\n",
    "print(X_test.shape)\n",
    "print(ytrain_i.shape)\n",
    "print(y_test.shape)\n",
    "print(ytrain_i)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.16573902e-01]\n",
      " [-9.35381651e-03]\n",
      " [ 1.10142143e+00]\n",
      " [ 4.35909882e-01]\n",
      " [ 4.66595525e+00]\n",
      " [-4.63952803e+00]\n",
      " [-1.09014505e+00]\n",
      " [ 1.19578816e+00]\n",
      " [ 9.95246476e+00]]\n"
     ]
    }
   ],
   "source": [
    "W = get_analytical_sol(Xtrain_i,ytrain_i)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5972868051385163\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = np.dot(Xtrain_i,W)\n",
    "error = (np.sum(abs(ytrain_i-y_pred_train))/y_pred_train.shape[0])\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4951636472577468\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.dot(X_test,W)\n",
    "error = (np.sum(abs(y_test-y_pred_test))/y_pred_test.shape[0])\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_i = Xtrain_i[:,:-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
