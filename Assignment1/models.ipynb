{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Training loss after  0  iterations is  1.5811388300841898\n",
      "Training loss after  500  iterations is  0.11515089871796896\n",
      "Training loss after  1000  iterations is  0.11515089872154656\n",
      "Training loss after  1500  iterations is  0.11515089872154524\n",
      "Training loss after  2000  iterations is  0.11515089872154423\n",
      "Training loss after  2500  iterations is  0.11515089872154423\n",
      "Training loss after  3000  iterations is  0.11515089872154423\n",
      "Training loss after  3500  iterations is  0.11515089872154423\n",
      "Training loss after  4000  iterations is  0.11515089872154423\n",
      "Training loss after  4500  iterations is  0.11515089872154423\n",
      "Predicted Values: [[2.76462962]\n",
      " [2.76462962]]\n",
      "True Values: [3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scratch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Running Linear Regression on Alabone dataset '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Running Linear Regression on Alabone dataset \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.13626242  0.00849443 -0.19450817 ... -0.33811371 -0.55916099\n",
      "  -0.53284584]\n",
      " [ 3.63998538 -0.09676618 -0.28473155 ... -0.57043891 -0.70051095\n",
      "  -0.53510142]\n",
      " [ 3.63998538 -0.11180341 -0.28473155 ... -0.46968947 -0.65765484\n",
      "  -0.61254316]\n",
      " ...\n",
      " [ 0.63253946  0.0460875  -0.1569151  ... -0.18473397 -0.52833467\n",
      "  -0.4711932 ]\n",
      " [ 2.13626242 -0.14939648 -0.31480601 ... -0.49901207 -0.6448732\n",
      "  -0.63133969]\n",
      " [ 0.63253946  0.06864335 -0.1569151  ... -0.11857016 -0.51705675\n",
      "  -0.40427753]]\n",
      "[[ 6.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " ...\n",
      " [10.]\n",
      " [15.]\n",
      " [10.]]\n"
     ]
    }
   ],
   "source": [
    "pp = MyPreProcessor()\n",
    "X,Y = pp.pre_process(0)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k=5, loss = \"rmse\", epochs = 8000, learning_rate = 0.01):\n",
    "    \"\"\" Performs K fold cross validation\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : instance of the model to be used\n",
    "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as data.\n",
    "    y : 1-dimensional numpy array of shape (n_samples,) which acts as labels.\n",
    "    k : number of folds, default = 5\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : instance of model\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  #number of examples\n",
    "    fold_size = int(m/k)\n",
    "    start = 0\n",
    "    end = fold_size\n",
    "    models = {}\n",
    "    for i in range(k):\n",
    "        Xtrain_i = np.concatenate((X[0:start], X[end+1:]))\n",
    "        ytrain_i = np.concatenate((y[0:start],y[end+1:]))\n",
    "        X_test =  X[start:end]\n",
    "        y_test = y[start:end]\n",
    "        model = MyLinearRegression()\n",
    "        print(\"For fold : \", i , \"/\", k)\n",
    "        model.fit(Xtrain_i,ytrain_i,X_test,y_test,epochs,learning_rate, loss)\n",
    "        if(loss == \"rmse\"):\n",
    "            models[i] = (model.rmse_train_history[-1], model.rmse_val_history[-1], np.array(model.rmse_train_history), np.array(model.rmse_val_history))\n",
    "        if(loss == \"mae\"):\n",
    "            models[i] = (model.mae_train_history[-1], model.mae_val_history[-1], np.array(model.mae_train_history), np.array(model.mae_val_history))\n",
    "        print(model.W)\n",
    "        print(model.b)\n",
    "        start+=fold_size\n",
    "        end+=fold_size\n",
    "        \n",
    "    avg_train = 0\n",
    "    avg_val  = 0\n",
    "    for i in range(len(models)):\n",
    "        avg_train+=models[i][2]\n",
    "        avg_val+=models[i][3]\n",
    "            \n",
    "    \n",
    "    avg_train = avg_train/k\n",
    "    avg_val = avg_val/k\n",
    "    return models, avg_train, avg_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 2\n",
      "Training loss after  0  iterations is :  10.467738559547147  | validation loss is :  10.414953882174231\n",
      "Training loss after  500  iterations is :  3.841039001569982  | validation loss is :  3.778977462957609\n",
      "Training loss after  1000  iterations is :  2.967457632220936  | validation loss is :  2.936820687609525\n",
      "Training loss after  1500  iterations is :  2.7664703397094854  | validation loss is :  2.7550645843104262\n",
      "Training loss after  2000  iterations is :  2.7078007460558164  | validation loss is :  2.7034511340964533\n",
      "Training loss after  2500  iterations is :  2.682143017140535  | validation loss is :  2.6803968035580255\n",
      "Training loss after  3000  iterations is :  2.6657526713970072  | validation loss is :  2.664895462974421\n",
      "Training loss after  3500  iterations is :  2.652204085334951  | validation loss is :  2.6514729064013633\n",
      "Training loss after  4000  iterations is :  2.639669327148181  | validation loss is :  2.6386938413068455\n",
      "Training loss after  4500  iterations is :  2.6276268217922354  | validation loss is :  2.6262200971858034\n",
      "[[-0.34147535]\n",
      " [ 0.72785826]\n",
      " [-0.14440358]\n",
      " [-2.55582885]\n",
      " [ 3.93650805]\n",
      " [-2.44586173]\n",
      " [-2.55774501]\n",
      " [-0.64401652]]\n",
      "4.831575137858414\n",
      "For fold :  1 / 2\n",
      "Training loss after  0  iterations is :  10.414953882174231  | validation loss is :  10.4697154941639\n",
      "Training loss after  500  iterations is :  3.7940633993565585  | validation loss is :  3.859075591655608\n",
      "Training loss after  1000  iterations is :  2.966223921233998  | validation loss is :  2.999810688704222\n",
      "Training loss after  1500  iterations is :  2.770770563519085  | validation loss is :  2.785306417069818\n",
      "Training loss after  2000  iterations is :  2.7093100137639046  | validation loss is :  2.716605813485206\n",
      "Training loss after  2500  iterations is :  2.681383863875234  | validation loss is :  2.685913152166103\n",
      "Training loss after  3000  iterations is :  2.6633264639567815  | validation loss is :  2.6668902592145685\n",
      "Training loss after  3500  iterations is :  2.6483720838083067  | validation loss is :  2.651796982546445\n",
      "Training loss after  4000  iterations is :  2.634536198527916  | validation loss is :  2.6382308338108578\n",
      "Training loss after  4500  iterations is :  2.6212396404772913  | validation loss is :  2.6254126053741316\n",
      "[[-0.30042318]\n",
      " [ 0.68471781]\n",
      " [-0.19534638]\n",
      " [-2.55923493]\n",
      " [ 3.95122655]\n",
      " [-2.52740843]\n",
      " [-2.49415383]\n",
      " [-0.51134817]]\n",
      "4.755301767668879\n",
      "For fold :  0 / 3\n",
      "Training loss after  0  iterations is :  10.547089964505261  | validation loss is :  10.228566039205633\n",
      "Training loss after  500  iterations is :  3.8979565617565832  | validation loss is :  3.6455345906567733\n",
      "Training loss after  1000  iterations is :  3.0237886987583344  | validation loss is :  2.8214450647894447\n",
      "Training loss after  1500  iterations is :  2.815722615353504  | validation loss is :  2.6536700683774623\n",
      "Training loss after  2000  iterations is :  2.753150706916241  | validation loss is :  2.607966884806899\n",
      "Training loss after  2500  iterations is :  2.725529108076572  | validation loss is :  2.587620909023214\n",
      "Training loss after  3000  iterations is :  2.7079956311330857  | validation loss is :  2.5736117588440828\n",
      "Training loss after  3500  iterations is :  2.693617431567396  | validation loss is :  2.561073387858812\n",
      "Training loss after  4000  iterations is :  2.6803673566580635  | validation loss is :  2.548826619629382\n",
      "Training loss after  4500  iterations is :  2.667651895331942  | validation loss is :  2.536679902150911\n",
      "[[-0.34679396]\n",
      " [ 0.66402781]\n",
      " [-0.21281145]\n",
      " [-2.59010989]\n",
      " [ 4.00930825]\n",
      " [-2.5206953 ]\n",
      " [-2.54289168]\n",
      " [-0.56073324]]\n",
      "4.842911114267944\n",
      "For fold :  1 / 3\n",
      "Training loss after  0  iterations is :  10.404468483631323  | validation loss is :  10.519788414025353\n",
      "Training loss after  500  iterations is :  3.7875676298848986  | validation loss is :  3.846394080080963\n",
      "Training loss after  1000  iterations is :  2.9486959681929585  | validation loss is :  2.999493801791506\n",
      "Training loss after  1500  iterations is :  2.756395200937134  | validation loss is :  2.7961091889707927\n",
      "Training loss after  2000  iterations is :  2.697911910426018  | validation loss is :  2.734336710148331\n",
      "Training loss after  2500  iterations is :  2.6718077260912585  | validation loss is :  2.7071938552632977\n",
      "Training loss after  3000  iterations is :  2.654958250617141  | validation loss is :  2.6898016401903417\n",
      "Training loss after  3500  iterations is :  2.640958477797999  | validation loss is :  2.6753869329868616\n",
      "Training loss after  4000  iterations is :  2.6279834287051647  | validation loss is :  2.6620419777325015\n",
      "Training loss after  4500  iterations is :  2.6155145476522104  | validation loss is :  2.649220898487934\n",
      "[[-0.31652142]\n",
      " [ 0.74723491]\n",
      " [-0.12308581]\n",
      " [-2.51561208]\n",
      " [ 3.90098699]\n",
      " [-2.46545541]\n",
      " [-2.53290917]\n",
      " [-0.64087946]]\n",
      "4.783169007234633\n",
      "For fold :  2 / 3\n",
      "Training loss after  0  iterations is :  10.375199070240843  | validation loss is :  10.57543374290505\n",
      "Training loss after  500  iterations is :  3.7658082590897517  | validation loss is :  3.9617534319731114\n",
      "Training loss after  1000  iterations is :  2.9263687861960954  | validation loss is :  3.081729587418066\n",
      "Training loss after  1500  iterations is :  2.7317396503167153  | validation loss is :  2.8625044410661338\n",
      "Training loss after  2000  iterations is :  2.6726586505676395  | validation loss is :  2.792005546633901\n",
      "Training loss after  2500  iterations is :  2.6461450561344484  | validation loss is :  2.760056701548986\n",
      "Training loss after  3000  iterations is :  2.628968465798005  | validation loss is :  2.74020060614227\n",
      "Training loss after  3500  iterations is :  2.6146820365986647  | validation loss is :  2.7245994814806\n",
      "Training loss after  4000  iterations is :  2.601430179308478  | validation loss is :  2.7107387506436753\n",
      "Training loss after  4500  iterations is :  2.588679760079877  | validation loss is :  2.6977527013981857\n",
      "[[-0.29747213]\n",
      " [ 0.71018478]\n",
      " [-0.17320962]\n",
      " [-2.56733457]\n",
      " [ 3.92336637]\n",
      " [-2.46859791]\n",
      " [-2.5004653 ]\n",
      " [-0.53008537]]\n",
      "4.7538807060112465\n",
      "For fold :  0 / 4\n",
      "Training loss after  0  iterations is :  10.562599512608134  | validation loss is :  10.074530687946485\n",
      "Training loss after  500  iterations is :  3.907522571314596  | validation loss is :  3.516758891626533\n",
      "Training loss after  1000  iterations is :  3.0422580232610974  | validation loss is :  2.708048303653943\n",
      "Training loss after  1500  iterations is :  2.8357262039054594  | validation loss is :  2.545641902378148\n",
      "Training loss after  2000  iterations is :  2.772509068033582  | validation loss is :  2.501180557359102\n",
      "Training loss after  2500  iterations is :  2.7443339119466814  | validation loss is :  2.4814135871718697\n",
      "Training loss after  3000  iterations is :  2.7264293919845266  | validation loss is :  2.4678854398151797\n",
      "Training loss after  3500  iterations is :  2.711772796097525  | validation loss is :  2.455780462538649\n",
      "Training loss after  4000  iterations is :  2.6982852640196824  | validation loss is :  2.4439249814275814\n",
      "Training loss after  4500  iterations is :  2.6853515543946944  | validation loss is :  2.432137940201614\n",
      "[[-0.33799607]\n",
      " [ 0.67604119]\n",
      " [-0.19485231]\n",
      " [-2.58474694]\n",
      " [ 4.00821175]\n",
      " [-2.52536438]\n",
      " [-2.54555531]\n",
      " [-0.5349785 ]]\n",
      "4.845319526997509\n",
      "For fold :  1 / 4\n",
      "Training loss after  0  iterations is :  10.338289508350968  | validation loss is :  10.74459678877047\n",
      "Training loss after  500  iterations is :  3.7406060808477286  | validation loss is :  4.040470138997425\n",
      "Training loss after  1000  iterations is :  2.888935412427375  | validation loss is :  3.174616783082725\n",
      "Training loss after  1500  iterations is :  2.6973003154543074  | validation loss is :  2.969446445476474\n",
      "Training loss after  2000  iterations is :  2.6413145648026264  | validation loss is :  2.907468080599888\n",
      "Training loss after  2500  iterations is :  2.6166777092217175  | validation loss is :  2.8797612889348545\n",
      "Training loss after  3000  iterations is :  2.6007199034775623  | validation loss is :  2.8616542291928355\n",
      "Training loss after  3500  iterations is :  2.5873802033277884  | validation loss is :  2.846519517851866\n",
      "Training loss after  4000  iterations is :  2.5749736390420708  | validation loss is :  2.8324876381754076\n",
      "Training loss after  4500  iterations is :  2.5630315526340737  | validation loss is :  2.8190101065583986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.31745122]\n",
      " [ 0.75559578]\n",
      " [-0.12476459]\n",
      " [-2.52866453]\n",
      " [ 3.87064376]\n",
      " [-2.41688382]\n",
      " [-2.52806934]\n",
      " [-0.67065944]]\n",
      "4.768118234895041\n",
      "For fold :  2 / 4\n",
      "Training loss after  0  iterations is :  10.487053051221666  | validation loss is :  10.304418202050934\n",
      "Training loss after  500  iterations is :  3.8394354423066988  | validation loss is :  3.7181893599762126\n",
      "Training loss after  1000  iterations is :  3.0047666157049204  | validation loss is :  2.874355581692504\n",
      "Training loss after  1500  iterations is :  2.8067449446837798  | validation loss is :  2.66673727718813\n",
      "Training loss after  2000  iterations is :  2.7440139327075883  | validation loss is :  2.603783864142225\n",
      "Training loss after  2500  iterations is :  2.7156044966467667  | validation loss is :  2.5772282488474474\n",
      "Training loss after  3000  iterations is :  2.6974296431599383  | validation loss is :  2.560987516293593\n",
      "Training loss after  3500  iterations is :  2.682506137754445  | validation loss is :  2.547770279081051\n",
      "Training loss after  4000  iterations is :  2.668759491675113  | validation loss is :  2.5355266751160914\n",
      "Training loss after  4500  iterations is :  2.6555767441660647  | validation loss is :  2.5237058830614747\n",
      "[[-0.32413673]\n",
      " [ 0.70085963]\n",
      " [-0.17405881]\n",
      " [-2.55444903]\n",
      " [ 3.96241575]\n",
      " [-2.53817931]\n",
      " [-2.52307762]\n",
      " [-0.56240395]]\n",
      "4.795081114013907\n",
      "For fold :  3 / 4\n",
      "Training loss after  0  iterations is :  10.3782394656382  | validation loss is :  10.632443302210111\n",
      "Training loss after  500  iterations is :  3.7816368814299386  | validation loss is :  3.9890354225064293\n",
      "Training loss after  1000  iterations is :  2.9291320480009384  | validation loss is :  3.0999862966407212\n",
      "Training loss after  1500  iterations is :  2.7322152073222883  | validation loss is :  2.886456803192796\n",
      "Training loss after  2000  iterations is :  2.6739731848443715  | validation loss is :  2.819125313054901\n",
      "Training loss after  2500  iterations is :  2.6481380820350946  | validation loss is :  2.7882312593580005\n",
      "Training loss after  3000  iterations is :  2.6313868304525228  | validation loss is :  2.7686368565529156\n",
      "Training loss after  3500  iterations is :  2.6174054315506794  | validation loss is :  2.7530189717041065\n",
      "Training loss after  4000  iterations is :  2.6044072021304903  | validation loss is :  2.7390411673764063\n",
      "Training loss after  4500  iterations is :  2.5918861171892424  | validation loss is :  2.72590123257948\n",
      "[[-0.30295257]\n",
      " [ 0.6976628 ]\n",
      " [-0.18323289]\n",
      " [-2.56234725]\n",
      " [ 3.93114395]\n",
      " [-2.46038745]\n",
      " [-2.50651972]\n",
      " [-0.54758252]]\n",
      "4.76690614497602\n",
      "For fold :  0 / 5\n",
      "Training loss after  0  iterations is :  10.536193679195014  | validation loss is :  10.058868640326967\n",
      "Training loss after  500  iterations is :  3.891438129249002  | validation loss is :  3.500262862731633\n",
      "Training loss after  1000  iterations is :  3.0304417326909006  | validation loss is :  2.680896808741341\n",
      "Training loss after  1500  iterations is :  2.82558999429007  | validation loss is :  2.5156933516130895\n",
      "Training loss after  2000  iterations is :  2.7631191613037553  | validation loss is :  2.47095439053567\n",
      "Training loss after  2500  iterations is :  2.7352252717453367  | validation loss is :  2.451275279369472\n",
      "Training loss after  3000  iterations is :  2.717346787676429  | validation loss is :  2.4379277220024504\n",
      "Training loss after  3500  iterations is :  2.7026024865091345  | validation loss is :  2.4260918556163764\n",
      "Training loss after  4000  iterations is :  2.68898371211899  | validation loss is :  2.4145826080512993\n",
      "Training loss after  4500  iterations is :  2.675904385481728  | validation loss is :  2.403196585235139\n",
      "[[-0.33093179]\n",
      " [ 0.67434504]\n",
      " [-0.19355807]\n",
      " [-2.57310677]\n",
      " [ 4.00159125]\n",
      " [-2.53414552]\n",
      " [-2.54083318]\n",
      " [-0.53664193]]\n",
      "4.825881881265161\n",
      "For fold :  1 / 5\n",
      "Training loss after  0  iterations is :  10.404110196553857  | validation loss is :  10.5797796067284\n",
      "Training loss after  500  iterations is :  3.7871888352357876  | validation loss is :  3.914783862779078\n",
      "Training loss after  1000  iterations is :  2.9285544962445518  | validation loss is :  3.0814197476915144\n",
      "Training loss after  1500  iterations is :  2.731734875442034  | validation loss is :  2.8918488489412595\n",
      "Training loss after  2000  iterations is :  2.673313864544158  | validation loss is :  2.834051151069531\n",
      "Training loss after  2500  iterations is :  2.6476164994093  | validation loss is :  2.8074923701815426\n",
      "Training loss after  3000  iterations is :  2.631200960498166  | validation loss is :  2.7897766535741635\n",
      "Training loss after  3500  iterations is :  2.6176357629364135  | validation loss is :  2.774748565527812\n",
      "Training loss after  4000  iterations is :  2.6050847105903516  | validation loss is :  2.760670754910345\n",
      "Training loss after  4500  iterations is :  2.5930237157635756  | validation loss is :  2.7470603906088535\n",
      "[[-0.32534429]\n",
      " [ 0.72203899]\n",
      " [-0.16475974]\n",
      " [-2.55986492]\n",
      " [ 3.92616459]\n",
      " [-2.44206251]\n",
      " [-2.51971422]\n",
      " [-0.62635798]]\n",
      "4.788917649288169\n",
      "For fold :  2 / 5\n",
      "Training loss after  0  iterations is :  10.432144588653038  | validation loss is :  10.4873462390504\n",
      "Training loss after  500  iterations is :  3.8144197730974243  | validation loss is :  3.7997938406806577\n",
      "Training loss after  1000  iterations is :  2.9688468626448232  | validation loss is :  2.944080878602855\n",
      "Training loss after  1500  iterations is :  2.7715894134361645  | validation loss is :  2.745542507298169\n",
      "Training loss after  2000  iterations is :  2.7112689292975802  | validation loss is :  2.6910255767622733\n",
      "Training loss after  2500  iterations is :  2.684325680607714  | validation loss is :  2.66908738094332\n",
      "Training loss after  3000  iterations is :  2.6670038913910052  | validation loss is :  2.654932791796891\n",
      "Training loss after  3500  iterations is :  2.6526654627219024  | validation loss is :  2.642466821755525\n",
      "Training loss after  4000  iterations is :  2.6393972119145412  | validation loss is :  2.630313101238381\n",
      "Training loss after  4500  iterations is :  2.6266492491152746  | validation loss is :  2.6182582954703535\n",
      "[[-0.33387111]\n",
      " [ 0.73411686]\n",
      " [-0.14388543]\n",
      " [-2.53605575]\n",
      " [ 3.90298029]\n",
      " [-2.50253414]\n",
      " [-2.54591998]\n",
      " [-0.62073059]]\n",
      "4.8085925721384\n",
      "For fold :  3 / 5\n",
      "Training loss after  0  iterations is :  10.463008692446145  | validation loss is :  10.361027854590143\n",
      "Training loss after  500  iterations is :  3.805477025982693  | validation loss is :  3.8682618877419417\n",
      "Training loss after  1000  iterations is :  2.9738223557998205  | validation loss is :  2.982642758653268\n",
      "Training loss after  1500  iterations is :  2.7784484330141033  | validation loss is :  2.754514223682133\n",
      "Training loss after  2000  iterations is :  2.717474023142718  | validation loss is :  2.682870523032349\n",
      "Training loss after  2500  iterations is :  2.689885471760399  | validation loss is :  2.6520097332142094\n",
      "Training loss after  3000  iterations is :  2.6720940547167493  | validation loss is :  2.6334210155335303\n",
      "Training loss after  3500  iterations is :  2.6573851180845254  | validation loss is :  2.618890514579274\n",
      "Training loss after  4000  iterations is :  2.6437921272283416  | validation loss is :  2.605913827712124\n",
      "Training loss after  4500  iterations is :  2.6307413755908926  | validation loss is :  2.593687015000019\n",
      "[[-0.30162343]\n",
      " [ 0.71377844]\n",
      " [-0.16030091]\n",
      " [-2.55300476]\n",
      " [ 3.93182311]\n",
      " [-2.50453901]\n",
      " [-2.52126522]\n",
      " [-0.5444386 ]]\n",
      "4.7831437805463315\n",
      "For fold :  4 / 5\n",
      "Training loss after  0  iterations is :  10.373619235729842  | validation loss is :  10.713470171413764\n",
      "Training loss after  500  iterations is :  3.785596229046982  | validation loss is :  3.999478000781372\n",
      "Training loss after  1000  iterations is :  2.9273665638969306  | validation loss is :  3.136824056116211\n",
      "Training loss after  1500  iterations is :  2.7307692054764927  | validation loss is :  2.9320040669456318\n",
      "Training loss after  2000  iterations is :  2.6729462968531066  | validation loss is :  2.8635797826302274\n",
      "Training loss after  2500  iterations is :  2.647351952470832  | validation loss is :  2.8303861523975837\n",
      "Training loss after  3000  iterations is :  2.6307942588868913  | validation loss is :  2.809049230972283\n",
      "Training loss after  3500  iterations is :  2.6169985766480584  | validation loss is :  2.792258564270894\n",
      "Training loss after  4000  iterations is :  2.604185998074389  | validation loss is :  2.7774689643328188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  4500  iterations is :  2.591851400343507  | validation loss is :  2.7637207296869746\n",
      "[[-0.30670066]\n",
      " [ 0.68906901]\n",
      " [-0.18794464]\n",
      " [-2.56522074]\n",
      " [ 3.96169342]\n",
      " [-2.44618311]\n",
      " [-2.49736782]\n",
      " [-0.55173881]]\n",
      "4.759391086954489\n",
      "For fold :  0 / 6\n",
      "Training loss after  0  iterations is :  10.50492978894445  | validation loss is :  10.124512192023712\n",
      "Training loss after  500  iterations is :  3.8598806834360353  | validation loss is :  3.5983936476758536\n",
      "Training loss after  1000  iterations is :  3.004708602096431  | validation loss is :  2.764419403801102\n",
      "Training loss after  1500  iterations is :  2.802115226000241  | validation loss is :  2.587277713479322\n",
      "Training loss after  2000  iterations is :  2.7404129901133096  | validation loss is :  2.5382504209507784\n",
      "Training loss after  2500  iterations is :  2.713000439924967  | validation loss is :  2.5169393422407715\n",
      "Training loss after  3000  iterations is :  2.6954946072717503  | validation loss is :  2.5026781006993444\n",
      "Training loss after  3500  iterations is :  2.6810727458266106  | validation loss is :  2.4901548995913885\n",
      "Training loss after  4000  iterations is :  2.6677534484369896  | validation loss is :  2.4780538687935456\n",
      "Training loss after  4500  iterations is :  2.6549620323759013  | validation loss is :  2.466125242249427\n",
      "[[-0.32667167]\n",
      " [ 0.68453934]\n",
      " [-0.18131505]\n",
      " [-2.56676009]\n",
      " [ 3.97108356]\n",
      " [-2.50827321]\n",
      " [-2.54238199]\n",
      " [-0.55653485]]\n",
      "4.814953503460169\n",
      "For fold :  1 / 6\n",
      "Training loss after  0  iterations is :  10.463915740501609  | validation loss is :  10.331571967050552\n",
      "Training loss after  500  iterations is :  3.841216280082923  | validation loss is :  3.691993440628752\n",
      "Training loss after  1000  iterations is :  2.976020362119339  | validation loss is :  2.884350714980542\n",
      "Training loss after  1500  iterations is :  2.774393374197892  | validation loss is :  2.719695445166824\n",
      "Training loss after  2000  iterations is :  2.7139897894837453  | validation loss is :  2.6727175623134816\n",
      "Training loss after  2500  iterations is :  2.687182222067965  | validation loss is :  2.6511970482810314\n",
      "Training loss after  3000  iterations is :  2.6700147065346185  | validation loss is :  2.6363835111251066\n",
      "Training loss after  3500  iterations is :  2.6558482646622394  | validation loss is :  2.6232898928846202\n",
      "Training loss after  4000  iterations is :  2.6427574526210282  | validation loss is :  2.610639873768273\n",
      "Training loss after  4500  iterations is :  2.630183450987877  | validation loss is :  2.5981797519867307\n",
      "[[-0.33481138]\n",
      " [ 0.69418662]\n",
      " [-0.19358899]\n",
      " [-2.57472538]\n",
      " [ 3.97067977]\n",
      " [-2.49344291]\n",
      " [-2.52094396]\n",
      " [-0.58331488]]\n",
      "4.811887194537424\n",
      "For fold :  2 / 6\n",
      "Training loss after  0  iterations is :  10.372703914137855  | validation loss is :  10.778064142816495\n",
      "Training loss after  500  iterations is :  3.7661000266033233  | validation loss is :  4.052061653203636\n",
      "Training loss after  1000  iterations is :  2.9197450539501575  | validation loss is :  3.184741056312655\n",
      "Training loss after  1500  iterations is :  2.7274905665814244  | validation loss is :  2.9698756060576366\n",
      "Training loss after  2000  iterations is :  2.6702403350937307  | validation loss is :  2.902366715190012\n",
      "Training loss after  2500  iterations is :  2.6448353098232613  | validation loss is :  2.8718966250059252\n",
      "Training loss after  3000  iterations is :  2.628405696442369  | validation loss is :  2.852350477192284\n",
      "Training loss after  3500  iterations is :  2.6147198811212964  | validation loss is :  2.8364124314799226\n",
      "Training loss after  4000  iterations is :  2.6020170972760392  | validation loss is :  2.821891918847461\n",
      "Training loss after  4500  iterations is :  2.5898001710150442  | validation loss is :  2.808085417573435\n",
      "[[-0.3130393 ]\n",
      " [ 0.7558008 ]\n",
      " [-0.11856469]\n",
      " [-2.53104554]\n",
      " [ 3.88652568]\n",
      " [-2.43188091]\n",
      " [-2.53157404]\n",
      " [-0.63299715]]\n",
      "4.778037243093444\n",
      "For fold :  3 / 6\n",
      "Training loss after  0  iterations is :  10.480274796780812  | validation loss is :  10.255009989515246\n",
      "Training loss after  500  iterations is :  3.844917603386606  | validation loss is :  3.6458421411146995\n",
      "Training loss after  1000  iterations is :  2.9990422535374885  | validation loss is :  2.8062734793000437\n",
      "Training loss after  1500  iterations is :  2.799641958992781  | validation loss is :  2.6119641450487\n",
      "Training loss after  2000  iterations is :  2.737981620844429  | validation loss is :  2.556314844183592\n",
      "Training loss after  2500  iterations is :  2.7103323549387737  | validation loss is :  2.5330267311747257\n",
      "Training loss after  3000  iterations is :  2.6926090053928093  | validation loss is :  2.5182742985530155\n",
      "Training loss after  3500  iterations is :  2.6779937826048266  | validation loss is :  2.5057736482549577\n",
      "Training loss after  4000  iterations is :  2.6644966522088143  | validation loss is :  2.4938981951157038\n",
      "Training loss after  4500  iterations is :  2.6515385987609053  | validation loss is :  2.482281892124806\n",
      "[[-0.32595338]\n",
      " [ 0.69066405]\n",
      " [-0.18264243]\n",
      " [-2.55126564]\n",
      " [ 3.96530605]\n",
      " [-2.52564067]\n",
      " [-2.5256013 ]\n",
      " [-0.57395332]]\n",
      "4.801955732227884\n",
      "For fold :  4 / 6\n",
      "Training loss after  0  iterations is :  10.456496335194371  | validation loss is :  10.365448795668819\n",
      "Training loss after  500  iterations is :  3.8016954802361456  | validation loss is :  3.9131959061169237\n",
      "Training loss after  1000  iterations is :  2.966568847920621  | validation loss is :  3.0128229438734024\n",
      "Training loss after  1500  iterations is :  2.7707006504637164  | validation loss is :  2.7813893989787526\n",
      "Training loss after  2000  iterations is :  2.7101421272455313  | validation loss is :  2.70986565176881\n",
      "Training loss after  2500  iterations is :  2.682896167565417  | validation loss is :  2.679497257800162\n",
      "Training loss after  3000  iterations is :  2.6653666005485794  | validation loss is :  2.6612027589533755\n",
      "Training loss after  3500  iterations is :  2.6508798468969617  | validation loss is :  2.6467633951857694\n",
      "Training loss after  4000  iterations is :  2.637488294606526  | validation loss is :  2.6337487571313964\n",
      "Training loss after  4500  iterations is :  2.624625330137795  | validation loss is :  2.62141268177684\n",
      "[[-0.30622595]\n",
      " [ 0.7229256 ]\n",
      " [-0.15753691]\n",
      " [-2.55765589]\n",
      " [ 3.91109111]\n",
      " [-2.49602647]\n",
      " [-2.51781302]\n",
      " [-0.56549131]]\n",
      "4.791516932119626\n",
      "For fold :  5 / 6\n",
      "Training loss after  0  iterations is :  10.373249748503483  | validation loss is :  10.78132964738196\n",
      "Training loss after  500  iterations is :  3.792904076470087  | validation loss is :  3.9938508220899216\n",
      "Training loss after  1000  iterations is :  2.9349066949116502  | validation loss is :  3.1350672765005227\n",
      "Training loss after  1500  iterations is :  2.7369377675724986  | validation loss is :  2.93384194077642\n",
      "Training loss after  2000  iterations is :  2.678118764753464  | validation loss is :  2.8682774313745356\n",
      "Training loss after  2500  iterations is :  2.6520322400825815  | validation loss is :  2.8369983152410976\n",
      "Training loss after  3000  iterations is :  2.6351913481625275  | validation loss is :  2.8168562098341656\n",
      "Training loss after  3500  iterations is :  2.621186454637859  | validation loss is :  2.8008157624499597\n",
      "Training loss after  4000  iterations is :  2.608192119537408  | validation loss is :  2.7865229402553773\n",
      "Training loss after  4500  iterations is :  2.5956883176478853  | validation loss is :  2.773132127744787\n",
      "[[-0.31771288]\n",
      " [ 0.69149696]\n",
      " [-0.18691331]\n",
      " [-2.56655876]\n",
      " [ 3.96081423]\n",
      " [-2.46332463]\n",
      " [-2.51353686]\n",
      " [-0.55217176]]\n",
      "4.764813817682043\n",
      "For fold :  0 / 7\n",
      "Training loss after  0  iterations is :  10.500362558436718  | validation loss is :  10.079882283322439\n",
      "Training loss after  500  iterations is :  3.8510175397499258  | validation loss is :  3.6217358678795932\n",
      "Training loss after  1000  iterations is :  2.9959246247217233  | validation loss is :  2.769031866693867\n",
      "Training loss after  1500  iterations is :  2.7930586679866254  | validation loss is :  2.592559527090889\n",
      "Training loss after  2000  iterations is :  2.7316330981992105  | validation loss is :  2.5475163414679765\n",
      "Training loss after  2500  iterations is :  2.7045172874484815  | validation loss is :  2.5288659745602993\n",
      "Training loss after  3000  iterations is :  2.68725750009198  | validation loss is :  2.5158860549498234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  3500  iterations is :  2.673051202807358  | validation loss is :  2.5037825464048753\n",
      "Training loss after  4000  iterations is :  2.659934576690827  | validation loss is :  2.4916204255597\n",
      "Training loss after  4500  iterations is :  2.647341188700917  | validation loss is :  2.4793699329434027\n",
      "[[-0.33015305]\n",
      " [ 0.68673173]\n",
      " [-0.18091324]\n",
      " [-2.57202813]\n",
      " [ 3.95008698]\n",
      " [-2.50087518]\n",
      " [-2.54546967]\n",
      " [-0.57677659]]\n",
      "4.816020959266428\n",
      "For fold :  1 / 7\n",
      "Training loss after  0  iterations is :  10.489433739398555  | validation loss is :  10.15228013533456\n",
      "Training loss after  500  iterations is :  3.8730275259320717  | validation loss is :  3.4398321207519227\n",
      "Training loss after  1000  iterations is :  3.0110577133245404  | validation loss is :  2.664139156862276\n",
      "Training loss after  1500  iterations is :  2.80912730479297  | validation loss is :  2.5093166321327818\n",
      "Training loss after  2000  iterations is :  2.7473028273285762  | validation loss is :  2.462893856193078\n",
      "Training loss after  2500  iterations is :  2.719302924806391  | validation loss is :  2.4414012644630527\n",
      "Training loss after  3000  iterations is :  2.701197527042805  | validation loss is :  2.427464600690988\n",
      "Training loss after  3500  iterations is :  2.6862280097721434  | validation loss is :  2.415869976284227\n",
      "Training loss after  4000  iterations is :  2.672396357212272  | validation loss is :  2.4050534173589218\n",
      "Training loss after  4500  iterations is :  2.659111997484436  | validation loss is :  2.394594383461214\n",
      "[[-0.32981227]\n",
      " [ 0.70583533]\n",
      " [-0.17888419]\n",
      " [-2.56847034]\n",
      " [ 4.00021698]\n",
      " [-2.53153458]\n",
      " [-2.52738702]\n",
      " [-0.53101444]]\n",
      "4.819658363972803\n",
      "For fold :  2 / 7\n",
      "Training loss after  0  iterations is :  10.361838662852893  | validation loss is :  10.914170949585612\n",
      "Training loss after  500  iterations is :  3.7574557193011158  | validation loss is :  4.159504431936395\n",
      "Training loss after  1000  iterations is :  2.9018435104436495  | validation loss is :  3.305649364197991\n",
      "Training loss after  1500  iterations is :  2.7061578307443894  | validation loss is :  3.1074638711911535\n",
      "Training loss after  2000  iterations is :  2.6485921267783534  | validation loss is :  3.0487171811491818\n",
      "Training loss after  2500  iterations is :  2.623393524451076  | validation loss is :  3.022194757394257\n",
      "Training loss after  3000  iterations is :  2.6071874180809194  | validation loss is :  3.0042943047060024\n",
      "Training loss after  3500  iterations is :  2.5936893938725554  | validation loss is :  2.9889081149175767\n",
      "Training loss after  4000  iterations is :  2.5811501887614345  | validation loss is :  2.9744050344873716\n",
      "Training loss after  4500  iterations is :  2.569083272191557  | validation loss is :  2.9603490689516727\n",
      "[[-0.32719228]\n",
      " [ 0.72541056]\n",
      " [-0.15529559]\n",
      " [-2.54884431]\n",
      " [ 3.90190713]\n",
      " [-2.44204165]\n",
      " [-2.52735658]\n",
      " [-0.63766707]]\n",
      "4.772471975671287\n",
      "For fold :  3 / 7\n",
      "Training loss after  0  iterations is :  10.451179762311087  | validation loss is :  10.395210564705447\n",
      "Training loss after  500  iterations is :  3.8178986006280953  | validation loss is :  3.7750153257758123\n",
      "Training loss after  1000  iterations is :  2.9759233660527604  | validation loss is :  2.9196485439194686\n",
      "Training loss after  1500  iterations is :  2.7809109989527454  | validation loss is :  2.7072271111775317\n",
      "Training loss after  2000  iterations is :  2.7213285208719573  | validation loss is :  2.642090708245859\n",
      "Training loss after  2500  iterations is :  2.6948170570785512  | validation loss is :  2.6136231814758077\n",
      "Training loss after  3000  iterations is :  2.6779162921207953  | validation loss is :  2.5954484857790674\n",
      "Training loss after  3500  iterations is :  2.6640150422145905  | validation loss is :  2.580326830323783\n",
      "Training loss after  4000  iterations is :  2.6511872742456415  | validation loss is :  2.566240749019501\n",
      "Training loss after  4500  iterations is :  2.6388731931975227  | validation loss is :  2.552638040165849\n",
      "[[-0.31769443]\n",
      " [ 0.7068482 ]\n",
      " [-0.16171903]\n",
      " [-2.54052923]\n",
      " [ 3.92331441]\n",
      " [-2.4654618 ]\n",
      " [-2.52376576]\n",
      " [-0.62076722]]\n",
      "4.794741689785512\n",
      "For fold :  4 / 7\n",
      "Training loss after  0  iterations is :  10.475186556296498  | validation loss is :  10.233597122572009\n",
      "Training loss after  500  iterations is :  3.8319723400305006  | validation loss is :  3.7155094707735024\n",
      "Training loss after  1000  iterations is :  2.991135377668045  | validation loss is :  2.850881232563874\n",
      "Training loss after  1500  iterations is :  2.7933444802533627  | validation loss is :  2.6376607636577294\n",
      "Training loss after  2000  iterations is :  2.7319142443871023  | validation loss is :  2.5720450130952983\n",
      "Training loss after  2500  iterations is :  2.7041062409462486  | validation loss is :  2.5440286173045568\n",
      "Training loss after  3000  iterations is :  2.686167643158918  | validation loss is :  2.5272240511052315\n",
      "Training loss after  3500  iterations is :  2.6713455541428432  | validation loss is :  2.5140201828164668\n",
      "Training loss after  4000  iterations is :  2.6576541584456046  | validation loss is :  2.5021290138804155\n",
      "Training loss after  4500  iterations is :  2.64451106499451  | validation loss is :  2.4908504909998075\n",
      "[[-0.31381345]\n",
      " [ 0.7026628 ]\n",
      " [-0.17711549]\n",
      " [-2.55597063]\n",
      " [ 3.9596044 ]\n",
      " [-2.52253436]\n",
      " [-2.52274307]\n",
      " [-0.54087415]]\n",
      "4.792683451750612\n",
      "For fold :  5 / 7\n",
      "Training loss after  0  iterations is :  10.42449133332346  | validation loss is :  10.544329561714214\n",
      "Training loss after  500  iterations is :  3.7934839437613297  | validation loss is :  4.003042907873363\n",
      "Training loss after  1000  iterations is :  2.9530939771023754  | validation loss is :  3.0983564302105897\n",
      "Training loss after  1500  iterations is :  2.7560315122501944  | validation loss is :  2.869256754024027\n",
      "Training loss after  2000  iterations is :  2.69566554724015  | validation loss is :  2.7990225541030846\n",
      "Training loss after  2500  iterations is :  2.668735402631414  | validation loss is :  2.768851430993658\n",
      "Training loss after  3000  iterations is :  2.6514792430052054  | validation loss is :  2.750297775320853\n",
      "Training loss after  3500  iterations is :  2.637228104387131  | validation loss is :  2.735440546226162\n",
      "Training loss after  4000  iterations is :  2.6240499982777608  | validation loss is :  2.721959730960205\n",
      "Training loss after  4500  iterations is :  2.6113868625436054  | validation loss is :  2.709147889307534\n",
      "[[-0.31205953]\n",
      " [ 0.72339253]\n",
      " [-0.1553353 ]\n",
      " [-2.55805875]\n",
      " [ 3.91074281]\n",
      " [-2.47322309]\n",
      " [-2.52206724]\n",
      " [-0.57278125]]\n",
      "4.792669577964033\n",
      "For fold :  6 / 7\n",
      "Training loss after  0  iterations is :  10.391753667505473  | validation loss is :  10.744828629712025\n",
      "Training loss after  500  iterations is :  3.799845255775547  | validation loss is :  3.9809321725076594\n",
      "Training loss after  1000  iterations is :  2.940378711842641  | validation loss is :  3.1265268030884488\n",
      "Training loss after  1500  iterations is :  2.7428939352873565  | validation loss is :  2.927329271570233\n",
      "Training loss after  2000  iterations is :  2.6846258195890766  | validation loss is :  2.860671421877171\n",
      "Training loss after  2500  iterations is :  2.6587834451372863  | validation loss is :  2.828043830159699\n",
      "Training loss after  3000  iterations is :  2.642032750206541  | validation loss is :  2.806958242753401\n",
      "Training loss after  3500  iterations is :  2.6280612274495123  | validation loss is :  2.790334366984157\n",
      "Training loss after  4000  iterations is :  2.6150822081830967  | validation loss is :  2.7756849758191384\n",
      "Training loss after  4500  iterations is :  2.602589330277038  | validation loss is :  2.762065552242348\n",
      "[[-0.31107681]\n",
      " [ 0.6966185 ]\n",
      " [-0.18037542]\n",
      " [-2.56139264]\n",
      " [ 3.96221653]\n",
      " [-2.46857503]\n",
      " [-2.50630891]\n",
      " [-0.56162097]]\n",
      "4.767644211665191\n",
      "For fold :  0 / 8\n",
      "Training loss after  0  iterations is :  10.503867602064357  | validation loss is :  10.007181329471328\n",
      "Training loss after  500  iterations is :  3.855443559702166  | validation loss is :  3.542950569380124\n",
      "Training loss after  1000  iterations is :  3.0020552918951036  | validation loss is :  2.69663806346938\n",
      "Training loss after  1500  iterations is :  2.8003901937394167  | validation loss is :  2.5213912919558608\n",
      "Training loss after  2000  iterations is :  2.739176353119378  | validation loss is :  2.4760622741011376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  2500  iterations is :  2.712053216496475  | validation loss is :  2.4572654758521426\n",
      "Training loss after  3000  iterations is :  2.6947824996690177  | validation loss is :  2.4442849311811\n",
      "Training loss after  3500  iterations is :  2.6805838216509916  | validation loss is :  2.4322100370838586\n",
      "Training loss after  4000  iterations is :  2.6674848141515786  | validation loss is :  2.420061920767902\n",
      "Training loss after  4500  iterations is :  2.6549120776468342  | validation loss is :  2.4078053112249713\n",
      "[[-0.3281169 ]\n",
      " [ 0.68002517]\n",
      " [-0.18333199]\n",
      " [-2.56752266]\n",
      " [ 3.95304809]\n",
      " [-2.49563597]\n",
      " [-2.54821362]\n",
      " [-0.57327621]]\n",
      "4.812687967700471\n",
      "For fold :  1 / 8\n",
      "Training loss after  0  iterations is :  10.485034242218678  | validation loss is :  10.14143278850831\n",
      "Training loss after  500  iterations is :  3.8584625253601894  | validation loss is :  3.4912553241452855\n",
      "Training loss after  1000  iterations is :  2.997931210943036  | validation loss is :  2.7209755715269863\n",
      "Training loss after  1500  iterations is :  2.796038879317714  | validation loss is :  2.5671079333326303\n",
      "Training loss after  2000  iterations is :  2.7344413368092675  | validation loss is :  2.520948448660828\n",
      "Training loss after  2500  iterations is :  2.7068258013332827  | validation loss is :  2.4990551133151695\n",
      "Training loss after  3000  iterations is :  2.68908849145487  | validation loss is :  2.484389569776269\n",
      "Training loss after  3500  iterations is :  2.67445179290701  | validation loss is :  2.4719727478643256\n",
      "Training loss after  4000  iterations is :  2.6609311213865072  | validation loss is :  2.460329337484269\n",
      "Training loss after  4500  iterations is :  2.6479459782506516  | validation loss is :  2.4490606714549603\n",
      "[[-0.32701778]\n",
      " [ 0.70596295]\n",
      " [-0.1788712 ]\n",
      " [-2.57136068]\n",
      " [ 3.99182074]\n",
      " [-2.51202749]\n",
      " [-2.51861883]\n",
      " [-0.54258565]]\n",
      "4.818786717371851\n",
      "For fold :  2 / 8\n",
      "Training loss after  0  iterations is :  10.405177805179731  | validation loss is :  10.703062905334882\n",
      "Training loss after  500  iterations is :  3.7886251393856663  | validation loss is :  4.03267438405046\n",
      "Training loss after  1000  iterations is :  2.9370794677598355  | validation loss is :  3.1637586859840705\n",
      "Training loss after  1500  iterations is :  2.7411608211798417  | validation loss is :  2.9560663312388793\n",
      "Training loss after  2000  iterations is :  2.6828261708542116  | validation loss is :  2.8919594835015263\n",
      "Training loss after  2500  iterations is :  2.657052929744122  | validation loss is :  2.8628044063417684\n",
      "Training loss after  3000  iterations is :  2.6404780211409546  | validation loss is :  2.84375101504813\n",
      "Training loss after  3500  iterations is :  2.6267173929095615  | validation loss is :  2.827932751081358\n",
      "Training loss after  4000  iterations is :  2.613958397412916  | validation loss is :  2.8133436309008295\n",
      "Training loss after  4500  iterations is :  2.6016865915087584  | validation loss is :  2.799372921356279\n",
      "[[-0.3163678 ]\n",
      " [ 0.71935733]\n",
      " [-0.1655717 ]\n",
      " [-2.5502608 ]\n",
      " [ 3.92409419]\n",
      " [-2.45858021]\n",
      " [-2.5187108 ]\n",
      " [-0.62326845]]\n",
      "4.7742258622475795\n",
      "For fold :  3 / 8\n",
      "Training loss after  0  iterations is :  10.391198448745193  | validation loss is :  10.78597073753484\n",
      "Training loss after  500  iterations is :  3.7821168379694208  | validation loss is :  4.045365304505845\n",
      "Training loss after  1000  iterations is :  2.9313007833248683  | validation loss is :  3.186727295609666\n",
      "Training loss after  1500  iterations is :  2.736394327569164  | validation loss is :  2.9796749702892087\n",
      "Training loss after  2000  iterations is :  2.6781421335911144  | validation loss is :  2.916082039571172\n",
      "Training loss after  2500  iterations is :  2.6522433567009838  | validation loss is :  2.8876774898017112\n",
      "Training loss after  3000  iterations is :  2.635505607288637  | validation loss is :  2.869326568421497\n",
      "Training loss after  3500  iterations is :  2.6215810245574565  | validation loss is :  2.8541220566482575\n",
      "Training loss after  4000  iterations is :  2.608665519210981  | validation loss is :  2.8400793298318265\n",
      "Training loss after  4500  iterations is :  2.5962466818278727  | validation loss is :  2.8266085039318845\n",
      "[[-0.32091759]\n",
      " [ 0.73505317]\n",
      " [-0.1366236 ]\n",
      " [-2.54042995]\n",
      " [ 3.90365083]\n",
      " [-2.45553172]\n",
      " [-2.53361991]\n",
      " [-0.60822483]]\n",
      "4.790806065095801\n",
      "For fold :  4 / 8\n",
      "Training loss after  0  iterations is :  10.472102603194683  | validation loss is :  10.226271478524831\n",
      "Training loss after  500  iterations is :  3.8434288181266263  | validation loss is :  3.6173222125001843\n",
      "Training loss after  1000  iterations is :  2.991330949665966  | validation loss is :  2.7771091676370405\n",
      "Training loss after  1500  iterations is :  2.7907290087102554  | validation loss is :  2.5962708136063157\n",
      "Training loss after  2000  iterations is :  2.7295261337313836  | validation loss is :  2.5493626625608568\n",
      "Training loss after  2500  iterations is :  2.702304216254767  | validation loss is :  2.530482691164395\n",
      "Training loss after  3000  iterations is :  2.6848978012642237  | validation loss is :  2.5177936701305885\n",
      "Training loss after  3500  iterations is :  2.6705422369383154  | validation loss is :  2.506153290131137\n",
      "Training loss after  4000  iterations is :  2.6572779124102848  | validation loss is :  2.4945204919635118\n",
      "Training loss after  4500  iterations is :  2.644538306404648  | validation loss is :  2.482828448719503\n",
      "[[-0.33024773]\n",
      " [ 0.69679784]\n",
      " [-0.18229279]\n",
      " [-2.55380419]\n",
      " [ 3.95497365]\n",
      " [-2.51446138]\n",
      " [-2.5303817 ]\n",
      " [-0.59485716]]\n",
      "4.807036067888686\n",
      "For fold :  5 / 8\n",
      "Training loss after  0  iterations is :  10.450186873637723  | validation loss is :  10.381976719923177\n",
      "Training loss after  500  iterations is :  3.8114506406809525  | validation loss is :  3.826450540406774\n",
      "Training loss after  1000  iterations is :  2.975411538247594  | validation loss is :  2.964966888522861\n",
      "Training loss after  1500  iterations is :  2.779836274134065  | validation loss is :  2.7346449300045554\n",
      "Training loss after  2000  iterations is :  2.718716316787726  | validation loss is :  2.6593168443000748\n",
      "Training loss after  2500  iterations is :  2.6910342133430802  | validation loss is :  2.62650906397153\n",
      "Training loss after  3000  iterations is :  2.6732218045904728  | validation loss is :  2.607216913071719\n",
      "Training loss after  3500  iterations is :  2.6585299966684715  | validation loss is :  2.5926450800805454\n",
      "Training loss after  4000  iterations is :  2.6449683583245425  | validation loss is :  2.579952200151868\n",
      "Training loss after  4500  iterations is :  2.6319522305232756  | validation loss is :  2.5681619219744465\n",
      "[[-0.31282982]\n",
      " [ 0.71118162]\n",
      " [-0.16146758]\n",
      " [-2.55847504]\n",
      " [ 3.95059602]\n",
      " [-2.50258321]\n",
      " [-2.51733787]\n",
      " [-0.54558741]]\n",
      "4.7808393727047385\n",
      "For fold :  6 / 8\n",
      "Training loss after  0  iterations is :  10.445313378975216  | validation loss is :  10.420654409113482\n",
      "Training loss after  500  iterations is :  3.8108694698672836  | validation loss is :  3.904279418736257\n",
      "Training loss after  1000  iterations is :  2.964209610352818  | validation loss is :  3.013926626148014\n",
      "Training loss after  1500  iterations is :  2.7659165175239515  | validation loss is :  2.8000724002800035\n",
      "Training loss after  2000  iterations is :  2.705783785351059  | validation loss is :  2.7360292387820793\n",
      "Training loss after  2500  iterations is :  2.6790333237102213  | validation loss is :  2.708179399273556\n",
      "Training loss after  3000  iterations is :  2.6618507573478682  | validation loss is :  2.6906083466104946\n",
      "Training loss after  3500  iterations is :  2.6476224840350437  | validation loss is :  2.67623443557599\n",
      "Training loss after  4000  iterations is :  2.6344455773954896  | validation loss is :  2.663030518074759\n",
      "Training loss after  4500  iterations is :  2.621773479240202  | validation loss is :  2.6504054123952057\n",
      "[[-0.31273381]\n",
      " [ 0.70769009]\n",
      " [-0.16757089]\n",
      " [-2.56037388]\n",
      " [ 3.9252638 ]\n",
      " [-2.47634959]\n",
      " [-2.52889813]\n",
      " [-0.56205542]]\n",
      "4.79969565186125\n",
      "For fold :  7 / 8\n",
      "Training loss after  0  iterations is :  10.38430935010645  | validation loss is :  10.840095148809766\n",
      "Training loss after  500  iterations is :  3.7940746934581866  | validation loss is :  4.053269506437809\n",
      "Training loss after  1000  iterations is :  2.93794830848185  | validation loss is :  3.1781173872342845\n",
      "Training loss after  1500  iterations is :  2.7409527271145695  | validation loss is :  2.968501983021017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  2000  iterations is :  2.6825180340683468  | validation loss is :  2.8986742835329524\n",
      "Training loss after  2500  iterations is :  2.656529698168971  | validation loss is :  2.865255410173884\n",
      "Training loss after  3000  iterations is :  2.6396989185151267  | validation loss is :  2.84399728420742\n",
      "Training loss after  3500  iterations is :  2.62568296561578  | validation loss is :  2.8273414349514057\n",
      "Training loss after  4000  iterations is :  2.612674758198694  | validation loss is :  2.8126856724459843\n",
      "Training loss after  4500  iterations is :  2.6001588980882167  | validation loss is :  2.799061433292529\n",
      "[[-0.31230349]\n",
      " [ 0.69794351]\n",
      " [-0.18388903]\n",
      " [-2.5586956 ]\n",
      " [ 3.95283316]\n",
      " [-2.47374633]\n",
      " [-2.50509202]\n",
      " [-0.56627554]]\n",
      "4.764185272731771\n",
      "For fold :  0 / 9\n",
      "Training loss after  0  iterations is :  10.495690817392795  | validation loss is :  10.006678804161508\n",
      "Training loss after  500  iterations is :  3.8547513522068684  | validation loss is :  3.519750587960143\n",
      "Training loss after  1000  iterations is :  2.9984856156557287  | validation loss is :  2.677141303845773\n",
      "Training loss after  1500  iterations is :  2.7958859186262197  | validation loss is :  2.514174770894709\n",
      "Training loss after  2000  iterations is :  2.7346074700689007  | validation loss is :  2.4757915838297393\n",
      "Training loss after  2500  iterations is :  2.70751282068329  | validation loss is :  2.460412674735069\n",
      "Training loss after  3000  iterations is :  2.6902543991056524  | validation loss is :  2.4491468616965646\n",
      "Training loss after  3500  iterations is :  2.676051488409161  | validation loss is :  2.4379619208328474\n",
      "Training loss after  4000  iterations is :  2.6629396838585833  | validation loss is :  2.4262929995533735\n",
      "Training loss after  4500  iterations is :  2.6503501421937514  | validation loss is :  2.4143074547771177\n",
      "[[-0.33183097]\n",
      " [ 0.68192831]\n",
      " [-0.18106219]\n",
      " [-2.57235357]\n",
      " [ 3.95117237]\n",
      " [-2.49921113]\n",
      " [-2.54779504]\n",
      " [-0.57430418]]\n",
      "4.815871105802393\n",
      "For fold :  1 / 9\n",
      "Training loss after  0  iterations is :  10.482421671861411  | validation loss is :  10.12114123999858\n",
      "Training loss after  500  iterations is :  3.8522728961160864  | validation loss is :  3.5000623878460986\n",
      "Training loss after  1000  iterations is :  2.9955486991163554  | validation loss is :  2.718470205170112\n",
      "Training loss after  1500  iterations is :  2.79469075952581  | validation loss is :  2.5538508687971864\n",
      "Training loss after  2000  iterations is :  2.7332913526370666  | validation loss is :  2.5032890989504977\n",
      "Training loss after  2500  iterations is :  2.705707215323834  | validation loss is :  2.479734295844838\n",
      "Training loss after  3000  iterations is :  2.6879327797505974  | validation loss is :  2.4645541994711633\n",
      "Training loss after  3500  iterations is :  2.6732291524577563  | validation loss is :  2.4521738785708544\n",
      "Training loss after  4000  iterations is :  2.659630171899143  | validation loss is :  2.4408532529947475\n",
      "Training loss after  4500  iterations is :  2.6465630464794163  | validation loss is :  2.4300597548988483\n",
      "[[-0.3226135 ]\n",
      " [ 0.69527703]\n",
      " [-0.18528389]\n",
      " [-2.56016541]\n",
      " [ 3.99317379]\n",
      " [-2.52274929]\n",
      " [-2.51463724]\n",
      " [-0.54429362]]\n",
      "4.808231431308173\n",
      "For fold :  2 / 9\n",
      "Training loss after  0  iterations is :  10.428401097368102  | validation loss is :  10.549861087498604\n",
      "Training loss after  500  iterations is :  3.808165628016427  | validation loss is :  3.9063233990649397\n",
      "Training loss after  1000  iterations is :  2.951283615115387  | validation loss is :  3.0695246302342687\n",
      "Training loss after  1500  iterations is :  2.7525640722741604  | validation loss is :  2.8836261832785475\n",
      "Training loss after  2000  iterations is :  2.693077722088915  | validation loss is :  2.8280593165015255\n",
      "Training loss after  2500  iterations is :  2.666772372372938  | validation loss is :  2.8026703720476416\n",
      "Training loss after  3000  iterations is :  2.649944285083371  | validation loss is :  2.785575766748537\n",
      "Training loss after  3500  iterations is :  2.6360463741879183  | validation loss is :  2.77084915012605\n",
      "Training loss after  4000  iterations is :  2.6231951722946576  | validation loss is :  2.7568816720373093\n",
      "Training loss after  4500  iterations is :  2.610848954526901  | validation loss is :  2.743270351384898\n",
      "[[-0.32638191]\n",
      " [ 0.70964446]\n",
      " [-0.17682351]\n",
      " [-2.56561687]\n",
      " [ 3.93877584]\n",
      " [-2.46406044]\n",
      " [-2.52558414]\n",
      " [-0.59971116]]\n",
      "4.794478273941763\n",
      "For fold :  3 / 9\n",
      "Training loss after  0  iterations is :  10.37448884826815  | validation loss is :  10.971063035910925\n",
      "Training loss after  500  iterations is :  3.782416318949456  | validation loss is :  4.10886605159056\n",
      "Training loss after  1000  iterations is :  2.931764603336837  | validation loss is :  3.22876382076572\n",
      "Training loss after  1500  iterations is :  2.73711787272844  | validation loss is :  3.0130254866863964\n",
      "Training loss after  2000  iterations is :  2.6792883617471523  | validation loss is :  2.944862064221476\n",
      "Training loss after  2500  iterations is :  2.6535939324327846  | validation loss is :  2.9137321289786797\n",
      "Training loss after  3000  iterations is :  2.6369278969983783  | validation loss is :  2.8937808259619175\n",
      "Training loss after  3500  iterations is :  2.6230194577475308  | validation loss is :  2.87765672667494\n",
      "Training loss after  4000  iterations is :  2.6100987488084355  | validation loss is :  2.863088423764193\n",
      "Training loss after  4500  iterations is :  2.597666238028322  | validation loss is :  2.849313368542292\n",
      "[[-0.3165255 ]\n",
      " [ 0.74188305]\n",
      " [-0.13691146]\n",
      " [-2.54181065]\n",
      " [ 3.91254336]\n",
      " [-2.45958683]\n",
      " [-2.51868487]\n",
      " [-0.61364736]]\n",
      "4.77369982319406\n",
      "For fold :  4 / 9\n",
      "Training loss after  0  iterations is :  10.477832009612738  | validation loss is :  10.159079513013182\n",
      "Training loss after  500  iterations is :  3.828441516323374  | validation loss is :  3.6314038238556448\n",
      "Training loss after  1000  iterations is :  2.9804013436664083  | validation loss is :  2.8276485140375796\n",
      "Training loss after  1500  iterations is :  2.7826396004821494  | validation loss is :  2.6447345736009273\n",
      "Training loss after  2000  iterations is :  2.7220336237414213  | validation loss is :  2.5946697276764215\n",
      "Training loss after  2500  iterations is :  2.695049116133243  | validation loss is :  2.5741630373795563\n",
      "Training loss after  3000  iterations is :  2.677828480370285  | validation loss is :  2.5604369000617697\n",
      "Training loss after  3500  iterations is :  2.6636512241657466  | validation loss is :  2.5479589257469004\n",
      "Training loss after  4000  iterations is :  2.650564675665532  | validation loss is :  2.5355625921448746\n",
      "Training loss after  4500  iterations is :  2.638003128141864  | validation loss is :  2.5231363288932394\n",
      "[[-0.32747825]\n",
      " [ 0.69823735]\n",
      " [-0.16959472]\n",
      " [-2.55443134]\n",
      " [ 3.9369329 ]\n",
      " [-2.487208  ]\n",
      " [-2.54732155]\n",
      " [-0.58910557]]\n",
      "4.8069218842238755\n",
      "For fold :  5 / 9\n",
      "Training loss after  0  iterations is :  10.446834515190048  | validation loss is :  10.41281537999839\n",
      "Training loss after  500  iterations is :  3.8202835061001  | validation loss is :  3.8157423742537504\n",
      "Training loss after  1000  iterations is :  2.975861945062664  | validation loss is :  2.9358656536426455\n",
      "Training loss after  1500  iterations is :  2.7782401316166214  | validation loss is :  2.7180654332357213\n",
      "Training loss after  2000  iterations is :  2.71771972849806  | validation loss is :  2.6501762586115793\n",
      "Training loss after  2500  iterations is :  2.6905941655463823  | validation loss is :  2.6203215653434575\n",
      "Training loss after  3000  iterations is :  2.6731393230945315  | validation loss is :  2.601994330211224\n",
      "Training loss after  3500  iterations is :  2.658701090185591  | validation loss is :  2.5875385889014826\n",
      "Training loss after  4000  iterations is :  2.6453474653080495  | validation loss is :  2.5745894024679195\n",
      "Training loss after  4500  iterations is :  2.632518674151535  | validation loss is :  2.5623778329346045\n",
      "[[-0.31337537]\n",
      " [ 0.71055747]\n",
      " [-0.1683518 ]\n",
      " [-2.54543346]\n",
      " [ 3.95121224]\n",
      " [-2.49787979]\n",
      " [-2.51436293]\n",
      " [-0.57522748]]\n",
      "4.792109105564066\n",
      "For fold :  6 / 9\n",
      "Training loss after  0  iterations is :  10.446937692019244  | validation loss is :  10.413332799829265\n",
      "Training loss after  500  iterations is :  3.806258843419044  | validation loss is :  3.894866079356356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  1000  iterations is :  2.972496152534626  | validation loss is :  2.99061744505574\n",
      "Training loss after  1500  iterations is :  2.776858791427592  | validation loss is :  2.746721198362421\n",
      "Training loss after  2000  iterations is :  2.715891797975088  | validation loss is :  2.6701758919363585\n",
      "Training loss after  2500  iterations is :  2.688401313250153  | validation loss is :  2.6381543587787952\n",
      "Training loss after  3000  iterations is :  2.670747947352643  | validation loss is :  2.619448385118739\n",
      "Training loss after  3500  iterations is :  2.656189186555341  | validation loss is :  2.6050906050374767\n",
      "Training loss after  4000  iterations is :  2.642746191934339  | validation loss is :  2.592369137520899\n",
      "Training loss after  4500  iterations is :  2.6298405234133755  | validation loss is :  2.580420768212571\n",
      "[[-0.31243334]\n",
      " [ 0.71616683]\n",
      " [-0.15816753]\n",
      " [-2.55439824]\n",
      " [ 3.92880619]\n",
      " [-2.49874495]\n",
      " [-2.52740844]\n",
      " [-0.56332849]]\n",
      "4.779111348243007\n",
      "For fold :  7 / 9\n",
      "Training loss after  0  iterations is :  10.430080557528788  | validation loss is :  10.538926203235384\n",
      "Training loss after  500  iterations is :  3.807696766585193  | validation loss is :  3.9249929167189737\n",
      "Training loss after  1000  iterations is :  2.95202337073547  | validation loss is :  3.0732006112909014\n",
      "Training loss after  1500  iterations is :  2.752887644731558  | validation loss is :  2.8835819770809277\n",
      "Training loss after  2000  iterations is :  2.693349336635777  | validation loss is :  2.82791057667906\n",
      "Training loss after  2500  iterations is :  2.667020070485554  | validation loss is :  2.802776409377203\n",
      "Training loss after  3000  iterations is :  2.650100170894408  | validation loss is :  2.7860018703124325\n",
      "Training loss after  3500  iterations is :  2.6360634969052397  | validation loss is :  2.771692658528438\n",
      "Training loss after  4000  iterations is :  2.6230498250695504  | validation loss is :  2.758240498671138\n",
      "Training loss after  4500  iterations is :  2.610529268674012  | validation loss is :  2.7452226939769693\n",
      "[[-0.32201066]\n",
      " [ 0.71001157]\n",
      " [-0.17160777]\n",
      " [-2.56523459]\n",
      " [ 3.92930264]\n",
      " [-2.46871596]\n",
      " [-2.52414249]\n",
      " [-0.57314276]]\n",
      "4.805629300943693\n",
      "For fold :  8 / 9\n",
      "Training loss after  0  iterations is :  10.400570897792102  | validation loss is :  10.7709299057455\n",
      "Training loss after  500  iterations is :  3.8018888174568786  | validation loss is :  4.03040333063165\n",
      "Training loss after  1000  iterations is :  2.9474736644327595  | validation loss is :  3.1501264627648227\n",
      "Training loss after  1500  iterations is :  2.7500708267171743  | validation loss is :  2.9362741032613315\n",
      "Training loss after  2000  iterations is :  2.691144759585499  | validation loss is :  2.8633922159038807\n",
      "Training loss after  2500  iterations is :  2.6648751734332876  | validation loss is :  2.828218402506256\n",
      "Training loss after  3000  iterations is :  2.647884024636584  | validation loss is :  2.8060926211092063\n",
      "Training loss after  3500  iterations is :  2.63376105468351  | validation loss is :  2.7890695692193566\n",
      "Training loss after  4000  iterations is :  2.6206686745201195  | validation loss is :  2.7743091843448533\n",
      "Training loss after  4500  iterations is :  2.608079676258624  | validation loss is :  2.760713511309599\n",
      "[[-0.30843715]\n",
      " [ 0.69617178]\n",
      " [-0.18309126]\n",
      " [-2.56081059]\n",
      " [ 3.96065719]\n",
      " [-2.47769598]\n",
      " [-2.50458631]\n",
      " [-0.55921845]]\n",
      "4.766470066693365\n",
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  10.486870576920118  | validation loss is :  10.035787282465531\n",
      "Training loss after  500  iterations is :  3.8475568308027923  | validation loss is :  3.529724382293449\n",
      "Training loss after  1000  iterations is :  2.990674479288378  | validation loss is :  2.707752947737866\n",
      "Training loss after  1500  iterations is :  2.7893201859558316  | validation loss is :  2.5492809103070586\n",
      "Training loss after  2000  iterations is :  2.728604937257921  | validation loss is :  2.509490501010646\n",
      "Training loss after  2500  iterations is :  2.7017182786184017  | validation loss is :  2.492397822622198\n",
      "Training loss after  3000  iterations is :  2.684545666589665  | validation loss is :  2.4799451053680874\n",
      "Training loss after  3500  iterations is :  2.6703885721039007  | validation loss is :  2.468008134487672\n",
      "Training loss after  4000  iterations is :  2.657309099291977  | validation loss is :  2.4558559892156167\n",
      "Training loss after  4500  iterations is :  2.644746721673703  | validation loss is :  2.443540713200838\n",
      "[[-0.32994345]\n",
      " [ 0.68478379]\n",
      " [-0.1802029 ]\n",
      " [-2.56873633]\n",
      " [ 3.95289619]\n",
      " [-2.48952606]\n",
      " [-2.54803564]\n",
      " [-0.57237274]]\n",
      "4.8109044828225445\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  10.481705596353182  | validation loss is :  10.082037592391158\n",
      "Training loss after  500  iterations is :  3.8541754584121293  | validation loss is :  3.4736678013034803\n",
      "Training loss after  1000  iterations is :  3.0007943769903727  | validation loss is :  2.6573838006456665\n",
      "Training loss after  1500  iterations is :  2.799820359448486  | validation loss is :  2.4818341485306608\n",
      "Training loss after  2000  iterations is :  2.7382102716145407  | validation loss is :  2.4294732468071967\n",
      "Training loss after  2500  iterations is :  2.710517091375423  | validation loss is :  2.4059737410082267\n",
      "Training loss after  3000  iterations is :  2.692664740668664  | validation loss is :  2.3912212387217404\n",
      "Training loss after  3500  iterations is :  2.6778921387972714  | validation loss is :  2.3793492120586706\n",
      "Training loss after  4000  iterations is :  2.664228527638036  | validation loss is :  2.3685520852128072\n",
      "Training loss after  4500  iterations is :  2.6511004211907583  | validation loss is :  2.3582798438370105\n",
      "[[-0.31965699]\n",
      " [ 0.69936185]\n",
      " [-0.18120791]\n",
      " [-2.56048205]\n",
      " [ 3.98764667]\n",
      " [-2.52661622]\n",
      " [-2.51566683]\n",
      " [-0.54448687]]\n",
      "4.805006442619137\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  10.443555680042602  | validation loss is :  10.40268366038925\n",
      "Training loss after  500  iterations is :  3.8204120018147667  | validation loss is :  3.7748041218071178\n",
      "Training loss after  1000  iterations is :  2.967654898937553  | validation loss is :  2.9224013341681867\n",
      "Training loss after  1500  iterations is :  2.7685844136008  | validation loss is :  2.7330958342009133\n",
      "Training loss after  2000  iterations is :  2.7082076273825115  | validation loss is :  2.680153931578145\n",
      "Training loss after  2500  iterations is :  2.681251926876256  | validation loss is :  2.658118816695299\n",
      "Training loss after  3000  iterations is :  2.6639430823458667  | validation loss is :  2.643884900825542\n",
      "Training loss after  3500  iterations is :  2.6496443074664677  | validation loss is :  2.6314909446268633\n",
      "Training loss after  4000  iterations is :  2.6364280345655464  | validation loss is :  2.619496156026733\n",
      "Training loss after  4500  iterations is :  2.6237348905041995  | validation loss is :  2.6076406005715937\n",
      "[[-0.32777921]\n",
      " [ 0.7130472 ]\n",
      " [-0.16941963]\n",
      " [-2.56586791]\n",
      " [ 3.9334839 ]\n",
      " [-2.49365521]\n",
      " [-2.53082278]\n",
      " [-0.57583983]]\n",
      "4.803761591746101\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  10.409599696092537  | validation loss is :  10.739894028388138\n",
      "Training loss after  500  iterations is :  3.7920200948386755  | validation loss is :  4.034746883708614\n",
      "Training loss after  1000  iterations is :  2.9365775343013003  | validation loss is :  3.213063978328534\n",
      "Training loss after  1500  iterations is :  2.7401235952286105  | validation loss is :  3.0186894148889154\n",
      "Training loss after  2000  iterations is :  2.681735700606817  | validation loss is :  2.9547583946979414\n",
      "Training loss after  2500  iterations is :  2.656104501245643  | validation loss is :  2.923430539613398\n",
      "Training loss after  3000  iterations is :  2.6397458393080804  | validation loss is :  2.9022171017165603\n",
      "Training loss after  3500  iterations is :  2.626223560273579  | validation loss is :  2.88454199979609\n",
      "Training loss after  4000  iterations is :  2.61370666577314  | validation loss is :  2.8683270237814003\n",
      "Training loss after  4500  iterations is :  2.6016746519484384  | validation loss is :  2.8528721208055963\n",
      "[[-0.31830463]\n",
      " [ 0.71545644]\n",
      " [-0.16611898]\n",
      " [-2.55427195]\n",
      " [ 3.93702164]\n",
      " [-2.4375718 ]\n",
      " [-2.51537914]\n",
      " [-0.62819306]]\n",
      "4.779727680449261\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  10.40397435011291  | validation loss is :  10.78534844162182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  500  iterations is :  3.7858860026695793  | validation loss is :  4.077096962863754\n",
      "Training loss after  1000  iterations is :  2.937338975968636  | validation loss is :  3.2090291103348916\n",
      "Training loss after  1500  iterations is :  2.7424874400886994  | validation loss is :  2.9947617251241185\n",
      "Training loss after  2000  iterations is :  2.684042122192298  | validation loss is :  2.928635509528761\n",
      "Training loss after  2500  iterations is :  2.6580178676065787  | validation loss is :  2.8993844499751433\n",
      "Training loss after  3000  iterations is :  2.6412024872500885  | validation loss is :  2.880702118485843\n",
      "Training loss after  3500  iterations is :  2.627221133735846  | validation loss is :  2.8653402808325654\n",
      "Training loss after  4000  iterations is :  2.6142566485794254  | validation loss is :  2.851209688045866\n",
      "Training loss after  4500  iterations is :  2.601791468990883  | validation loss is :  2.8376826786901415\n",
      "[[-0.31795229]\n",
      " [ 0.73384202]\n",
      " [-0.1400171 ]\n",
      " [-2.53882166]\n",
      " [ 3.90679598]\n",
      " [-2.46016259]\n",
      " [-2.53446339]\n",
      " [-0.60522819]]\n",
      "4.789159657999841\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  10.468597711831103  | validation loss is :  10.20403362196898\n",
      "Training loss after  500  iterations is :  3.8441221110401704  | validation loss is :  3.537746660607545\n",
      "Training loss after  1000  iterations is :  2.995268321305056  | validation loss is :  2.6922415606696224\n",
      "Training loss after  1500  iterations is :  2.7944582980708192  | validation loss is :  2.5104292577935805\n",
      "Training loss after  2000  iterations is :  2.7324775663774585  | validation loss is :  2.4684911886191228\n",
      "Training loss after  2500  iterations is :  2.7047876714613857  | validation loss is :  2.4544257686340805\n",
      "Training loss after  3000  iterations is :  2.687100810803679  | validation loss is :  2.445208388459966\n",
      "Training loss after  3500  iterations is :  2.672541693631976  | validation loss is :  2.435966994794725\n",
      "Training loss after  4000  iterations is :  2.659102938289127  | validation loss is :  2.4260592411510733\n",
      "Training loss after  4500  iterations is :  2.6462003480415466  | validation loss is :  2.4157016957121984\n",
      "[[-0.33626651]\n",
      " [ 0.70280561]\n",
      " [-0.17676849]\n",
      " [-2.55695362]\n",
      " [ 3.94579939]\n",
      " [-2.52476686]\n",
      " [-2.53477595]\n",
      " [-0.58901975]]\n",
      "4.8125083481622175\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  10.461757825345039  | validation loss is :  10.269276195192214\n",
      "Training loss after  500  iterations is :  3.818298929404089  | validation loss is :  3.79450691585762\n",
      "Training loss after  1000  iterations is :  2.973330646256048  | validation loss is :  2.9430298739495417\n",
      "Training loss after  1500  iterations is :  2.7764214713055044  | validation loss is :  2.7295259313360853\n",
      "Training loss after  2000  iterations is :  2.7161101080374888  | validation loss is :  2.6591552559187344\n",
      "Training loss after  2500  iterations is :  2.6890122993574193  | validation loss is :  2.626961012380595\n",
      "Training loss after  3000  iterations is :  2.6715477154034684  | validation loss is :  2.607255433229824\n",
      "Training loss after  3500  iterations is :  2.6570961557513697  | validation loss is :  2.592111855774201\n",
      "Training loss after  4000  iterations is :  2.6437319786150457  | validation loss is :  2.578874790474577\n",
      "Training loss after  4500  iterations is :  2.630896148561213  | validation loss is :  2.5665905547164862\n",
      "[[-0.3101512 ]\n",
      " [ 0.69267242]\n",
      " [-0.18175918]\n",
      " [-2.55694502]\n",
      " [ 3.96534206]\n",
      " [-2.49788981]\n",
      " [-2.51500152]\n",
      " [-0.54779262]]\n",
      "4.783351715030489\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  10.440459433754144  | validation loss is :  10.464740386980772\n",
      "Training loss after  500  iterations is :  3.80659803020629  | validation loss is :  3.9439709383079333\n",
      "Training loss after  1000  iterations is :  2.9675762438249045  | validation loss is :  3.0174298801989425\n",
      "Training loss after  1500  iterations is :  2.770673364279499  | validation loss is :  2.778180345103819\n",
      "Training loss after  2000  iterations is :  2.7100674364558985  | validation loss is :  2.7080429878449377\n",
      "Training loss after  2500  iterations is :  2.6829053514703984  | validation loss is :  2.67983339912071\n",
      "Training loss after  3000  iterations is :  2.6654378328066315  | validation loss is :  2.663065694035844\n",
      "Training loss after  3500  iterations is :  2.650988804197217  | validation loss is :  2.6496252380487038\n",
      "Training loss after  4000  iterations is :  2.6376232146161867  | validation loss is :  2.6372969333487837\n",
      "Training loss after  4500  iterations is :  2.624781918807389  | validation loss is :  2.6254746347670572\n",
      "[[-0.3136797 ]\n",
      " [ 0.72890773]\n",
      " [-0.14789026]\n",
      " [-2.55327364]\n",
      " [ 3.9112436 ]\n",
      " [-2.49244328]\n",
      " [-2.53087387]\n",
      " [-0.57795934]]\n",
      "4.7944240326508885\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  10.42861386893045  | validation loss is :  10.570648471721583\n",
      "Training loss after  500  iterations is :  3.8125748467031575  | validation loss is :  3.8875356404750034\n",
      "Training loss after  1000  iterations is :  2.9573998488826554  | validation loss is :  3.052227620087822\n",
      "Training loss after  1500  iterations is :  2.7595068704903696  | validation loss is :  2.860121489813156\n",
      "Training loss after  2000  iterations is :  2.700569723767137  | validation loss is :  2.796226268277063\n",
      "Training loss after  2500  iterations is :  2.6745977890434687  | validation loss is :  2.7644632691336564\n",
      "Training loss after  3000  iterations is :  2.657975602975481  | validation loss is :  2.7432268733983842\n",
      "Training loss after  3500  iterations is :  2.6442206098817937  | validation loss is :  2.72590893733229\n",
      "Training loss after  4000  iterations is :  2.631479470757402  | validation loss is :  2.7102895643692144\n",
      "Training loss after  4500  iterations is :  2.6192224274931806  | validation loss is :  2.695573261444523\n",
      "[[-0.31733785]\n",
      " [ 0.69726998]\n",
      " [-0.17647869]\n",
      " [-2.56343292]\n",
      " [ 3.95020974]\n",
      " [-2.44605962]\n",
      " [-2.51566388]\n",
      " [-0.58120448]]\n",
      "4.78838867257028\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  10.398691427070192  | validation loss is :  10.831165619685407\n",
      "Training loss after  500  iterations is :  3.7962565199391007  | validation loss is :  4.083414054654952\n",
      "Training loss after  1000  iterations is :  2.9432081151640435  | validation loss is :  3.200699881377132\n",
      "Training loss after  1500  iterations is :  2.7461878244244704  | validation loss is :  2.983761782130368\n",
      "Training loss after  2000  iterations is :  2.687073750025459  | validation loss is :  2.9112838015373432\n",
      "Training loss after  2500  iterations is :  2.6606028689453627  | validation loss is :  2.8773493984867473\n",
      "Training loss after  3000  iterations is :  2.643431027500802  | validation loss is :  2.8563567960695297\n",
      "Training loss after  3500  iterations is :  2.6291401708139435  | validation loss is :  2.84025659709372\n",
      "Training loss after  4000  iterations is :  2.6158891895523624  | validation loss is :  2.8262651308717306\n",
      "Training loss after  4500  iterations is :  2.603149805032821  | validation loss is :  2.8133403120578744\n",
      "[[-0.31105149]\n",
      " [ 0.69966281]\n",
      " [-0.1804032 ]\n",
      " [-2.55901183]\n",
      " [ 3.95361352]\n",
      " [-2.49073294]\n",
      " [-2.50921899]\n",
      " [-0.55270727]]\n",
      "4.768920585636032\n",
      "{2: (2.612144577361343, 2.6135570905179266), 3: (2.6116837273367985, 2.6156052463190758), 4: (2.611696186273858, 2.6129862969808997), 5: (2.6113593850326624, 2.6129494802267663), 6: (2.6121954304520307, 2.6126436070823256), 7: (2.612428760806144, 2.609077512675159), 8: (2.6126324169953046, 2.61069115768441), 9: (2.6126643989316816, 2.610974447617996), 10: (2.612464223152074, 2.6094793832843246)}\n"
     ]
    }
   ],
   "source": [
    "k_models = {}\n",
    "for i in range(2,11):\n",
    "    models_rmse,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = i, epochs = 5000, learning_rate = 0.01, loss = \"rmse\")\n",
    "#     print(\"The average loss with k = \", i, \" is \", \" train loss = \", avg_train_loss[-1], \"\")\n",
    "    k_models[i] = (avg_train_loss[-1], avg_val_loss[-1])\n",
    "print(k_models)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average loss with k =  2  is   train loss =  2.612144577361343  validation loss =  2.6135570905179266\n",
      "The average loss with k =  3  is   train loss =  2.6116837273367985  validation loss =  2.6156052463190758\n",
      "The average loss with k =  4  is   train loss =  2.611696186273858  validation loss =  2.6129862969808997\n",
      "The average loss with k =  5  is   train loss =  2.6113593850326624  validation loss =  2.6129494802267663\n",
      "The average loss with k =  6  is   train loss =  2.6121954304520307  validation loss =  2.6126436070823256\n",
      "The average loss with k =  7  is   train loss =  2.612428760806144  validation loss =  2.609077512675159\n",
      "The average loss with k =  8  is   train loss =  2.6126324169953046  validation loss =  2.61069115768441\n",
      "The average loss with k =  9  is   train loss =  2.6126643989316816  validation loss =  2.610974447617996\n",
      "The average loss with k =  10  is   train loss =  2.612464223152074  validation loss =  2.6094793832843246\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,11):\n",
    "    print(\"The average loss with k = \", i, \" is \", \" train loss = \", k_models[i][0], \" validation loss = \", k_models[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  9.973922299095264  | validation loss is :  9.56115107913669\n",
      "Training loss after  500  iterations is :  2.962320415058596  | validation loss is :  2.7473166876086546\n",
      "Training loss after  1000  iterations is :  2.0833946955401403  | validation loss is :  1.9356651946181729\n",
      "Training loss after  1500  iterations is :  1.945036973722756  | validation loss is :  1.8272117300606823\n",
      "Training loss after  2000  iterations is :  1.9109231822590862  | validation loss is :  1.7952513185869328\n",
      "Training loss after  2500  iterations is :  1.896077065441792  | validation loss is :  1.7813066164182183\n",
      "Training loss after  3000  iterations is :  1.8849150488108957  | validation loss is :  1.7707498174027059\n",
      "Training loss after  3500  iterations is :  1.8748375260194394  | validation loss is :  1.761598179180337\n",
      "Training loss after  4000  iterations is :  1.8653497711514704  | validation loss is :  1.7528039853883042\n",
      "Training loss after  4500  iterations is :  1.8563597312802027  | validation loss is :  1.7443318393294156\n",
      "[[-0.26399924]\n",
      " [ 0.79478446]\n",
      " [-0.07326202]\n",
      " [-2.43636374]\n",
      " [ 3.56021631]\n",
      " [-1.9993367 ]\n",
      " [-2.27723292]\n",
      " [-0.80745345]]\n",
      "4.50104843001621\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  9.960883448642894  | validation loss is :  9.676258992805755\n",
      "Training loss after  500  iterations is :  2.969315110260521  | validation loss is :  2.703003582260705\n",
      "Training loss after  1000  iterations is :  2.0999443685255987  | validation loss is :  1.8512214634384319\n",
      "Training loss after  1500  iterations is :  1.9596260040950866  | validation loss is :  1.7166940870597893\n",
      "Training loss after  2000  iterations is :  1.9245041215956076  | validation loss is :  1.6770452691225581\n",
      "Training loss after  2500  iterations is :  1.9092512959201096  | validation loss is :  1.6620025089453387\n",
      "Training loss after  3000  iterations is :  1.8978161704081842  | validation loss is :  1.6516885173194789\n",
      "Training loss after  3500  iterations is :  1.887636423432816  | validation loss is :  1.6429260603122875\n",
      "Training loss after  4000  iterations is :  1.87811096358754  | validation loss is :  1.6347243569235808\n",
      "Training loss after  4500  iterations is :  1.8690389434073653  | validation loss is :  1.626636947692088\n",
      "[[-0.27567087]\n",
      " [ 0.78770783]\n",
      " [-0.09121256]\n",
      " [-2.43787611]\n",
      " [ 3.57135971]\n",
      " [-2.01509701]\n",
      " [-2.27619118]\n",
      " [-0.81555125]]\n",
      "4.512480042575901\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  9.935870143693453  | validation loss is :  9.880095923261392\n",
      "Training loss after  500  iterations is :  2.9471860289248513  | validation loss is :  2.8981009975038226\n",
      "Training loss after  1000  iterations is :  2.0736899287608366  | validation loss is :  2.0409394178187896\n",
      "Training loss after  1500  iterations is :  1.9355068887316382  | validation loss is :  1.905660419309713\n",
      "Training loss after  2000  iterations is :  1.8991599915425816  | validation loss is :  1.880882607901415\n",
      "Training loss after  2500  iterations is :  1.8835249686509603  | validation loss is :  1.872257235622759\n",
      "Training loss after  3000  iterations is :  1.8721482786910941  | validation loss is :  1.8640632060463662\n",
      "Training loss after  3500  iterations is :  1.8619018147717095  | validation loss is :  1.8564740355301075\n",
      "Training loss after  4000  iterations is :  1.852451865682345  | validation loss is :  1.8488067269337163\n",
      "Training loss after  4500  iterations is :  1.8434237378042546  | validation loss is :  1.8411338063753224\n",
      "[[-0.27707501]\n",
      " [ 0.81910613]\n",
      " [-0.0693703 ]\n",
      " [-2.44685726]\n",
      " [ 3.54442967]\n",
      " [-1.99721422]\n",
      " [-2.29365204]\n",
      " [-0.824731  ]]\n",
      "4.513650878126809\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  9.905268759978712  | validation loss is :  10.184652278177458\n",
      "Training loss after  500  iterations is :  2.9278098377436232  | validation loss is :  3.1045590499851143\n",
      "Training loss after  1000  iterations is :  2.0629793169271093  | validation loss is :  2.211782231795219\n",
      "Training loss after  1500  iterations is :  1.9254826625652934  | validation loss is :  2.047987802094045\n",
      "Training loss after  2000  iterations is :  1.8897711127398165  | validation loss is :  2.0027564883308924\n",
      "Training loss after  2500  iterations is :  1.8752086229092348  | validation loss is :  1.981828947817242\n",
      "Training loss after  3000  iterations is :  1.8643558335880523  | validation loss is :  1.9667173980271877\n",
      "Training loss after  3500  iterations is :  1.854816914065548  | validation loss is :  1.9544200643894643\n",
      "Training loss after  4000  iterations is :  1.8456543077133956  | validation loss is :  1.9429367547081786\n",
      "Training loss after  4500  iterations is :  1.8368242092689369  | validation loss is :  1.9324073128669337\n",
      "[[-0.2571426 ]\n",
      " [ 0.81442747]\n",
      " [-0.06422106]\n",
      " [-2.4299641 ]\n",
      " [ 3.55018254]\n",
      " [-1.95977495]\n",
      " [-2.26620911]\n",
      " [-0.84599308]]\n",
      "4.490734433209311\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  9.90367216604577  | validation loss is :  10.194244604316546\n",
      "Training loss after  500  iterations is :  2.9221281892892  | validation loss is :  3.1126167224004706\n",
      "Training loss after  1000  iterations is :  2.0490039855553874  | validation loss is :  2.263905996552852\n",
      "Training loss after  1500  iterations is :  1.9130633983649639  | validation loss is :  2.124085767829835\n",
      "Training loss after  2000  iterations is :  1.879288154690121  | validation loss is :  2.0885528090177794\n",
      "Training loss after  2500  iterations is :  1.865126168997203  | validation loss is :  2.070375326619271\n",
      "Training loss after  3000  iterations is :  1.8543199080183743  | validation loss is :  2.0578202611486094\n",
      "Training loss after  3500  iterations is :  1.8447946784881188  | validation loss is :  2.0469707844552545\n",
      "Training loss after  4000  iterations is :  1.8356358033757427  | validation loss is :  2.0359354098377387\n",
      "Training loss after  4500  iterations is :  1.8270130709296692  | validation loss is :  2.02563182079688\n",
      "[[-0.26168823]\n",
      " [ 0.81735459]\n",
      " [-0.05670118]\n",
      " [-2.426992  ]\n",
      " [ 3.50705417]\n",
      " [-1.94758447]\n",
      " [-2.28897109]\n",
      " [-0.84254565]]\n",
      "4.488052155401756\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  9.952368281000533  | validation loss is :  9.752997601918466\n",
      "Training loss after  500  iterations is :  2.9570025844968604  | validation loss is :  2.7432512508745077\n",
      "Training loss after  1000  iterations is :  2.090253832325397  | validation loss is :  1.8984178283524324\n",
      "Training loss after  1500  iterations is :  1.949211238327307  | validation loss is :  1.7800384358959027\n",
      "Training loss after  2000  iterations is :  1.9143771015418745  | validation loss is :  1.755963362757666\n",
      "Training loss after  2500  iterations is :  1.899065077300278  | validation loss is :  1.7459605648291967\n",
      "Training loss after  3000  iterations is :  1.887666373728765  | validation loss is :  1.7391501660576651\n",
      "Training loss after  3500  iterations is :  1.8774039652652337  | validation loss is :  1.7323993885702984\n",
      "Training loss after  4000  iterations is :  1.86797546863875  | validation loss is :  1.7249419608781758\n",
      "Training loss after  4500  iterations is :  1.8589791746905473  | validation loss is :  1.7175758048044325\n",
      "[[-0.2782574 ]\n",
      " [ 0.79998391]\n",
      " [-0.08140022]\n",
      " [-2.42910498]\n",
      " [ 3.5123762 ]\n",
      " [-2.01441931]\n",
      " [-2.29619645]\n",
      " [-0.82333518]]\n",
      "4.501293241085857\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  9.948642895156999  | validation loss is :  9.788968824940047\n",
      "Training loss after  500  iterations is :  2.944376015443496  | validation loss is :  2.9305000179805476\n",
      "Training loss after  1000  iterations is :  2.0771889072789733  | validation loss is :  2.064226101327561\n",
      "Training loss after  1500  iterations is :  1.9387759347200326  | validation loss is :  1.9148419948866613\n",
      "Training loss after  2000  iterations is :  1.9029724647506912  | validation loss is :  1.8768208209536512\n",
      "Training loss after  2500  iterations is :  1.8879153293404183  | validation loss is :  1.8603991983942165\n",
      "Training loss after  3000  iterations is :  1.8767610703441397  | validation loss is :  1.848273937952695\n",
      "Training loss after  3500  iterations is :  1.8668760603422474  | validation loss is :  1.8379320724481596\n",
      "Training loss after  4000  iterations is :  1.8573699460767326  | validation loss is :  1.8283226299806825\n",
      "Training loss after  4500  iterations is :  1.8483161634645608  | validation loss is :  1.8195199688239863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2580753 ]\n",
      " [ 0.79847922]\n",
      " [-0.07404254]\n",
      " [-2.43563009]\n",
      " [ 3.56240773]\n",
      " [-1.99448507]\n",
      " [-2.27002943]\n",
      " [-0.80826853]]\n",
      "4.487551889303039\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  9.934007450771688  | validation loss is :  9.92326139088729\n",
      "Training loss after  500  iterations is :  2.9393393028849832  | validation loss is :  2.9912361736920023\n",
      "Training loss after  1000  iterations is :  2.082676545030294  | validation loss is :  2.0381586483103433\n",
      "Training loss after  1500  iterations is :  1.941665552030257  | validation loss is :  1.882513967579168\n",
      "Training loss after  2000  iterations is :  1.906077500072077  | validation loss is :  1.8465931810001042\n",
      "Training loss after  2500  iterations is :  1.8901541420082217  | validation loss is :  1.835969983024558\n",
      "Training loss after  3000  iterations is :  1.878817808701835  | validation loss is :  1.8277391043532831\n",
      "Training loss after  3500  iterations is :  1.8687196288927628  | validation loss is :  1.819664781292943\n",
      "Training loss after  4000  iterations is :  1.8593198205888897  | validation loss is :  1.8118954994124854\n",
      "Training loss after  4500  iterations is :  1.850325298116253  | validation loss is :  1.804256053060567\n",
      "[[-0.27360878]\n",
      " [ 0.8209999 ]\n",
      " [-0.06082098]\n",
      " [-2.43965385]\n",
      " [ 3.50571189]\n",
      " [-1.99030692]\n",
      " [-2.28506152]\n",
      " [-0.84230345]]\n",
      "4.505023948908989\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  9.918839808408729  | validation loss is :  10.059952038369305\n",
      "Training loss after  500  iterations is :  2.9343913267184303  | validation loss is :  3.045403699136124\n",
      "Training loss after  1000  iterations is :  2.0606434283911694  | validation loss is :  2.1873979964344232\n",
      "Training loss after  1500  iterations is :  1.9217473341030225  | validation loss is :  2.0525157921647526\n",
      "Training loss after  2000  iterations is :  1.8872505403417472  | validation loss is :  2.0177203924938603\n",
      "Training loss after  2500  iterations is :  1.8728242924286624  | validation loss is :  2.001082373255941\n",
      "Training loss after  3000  iterations is :  1.8621842596072546  | validation loss is :  1.9881758458501235\n",
      "Training loss after  3500  iterations is :  1.8526993863127605  | validation loss is :  1.9767878547090698\n",
      "Training loss after  4000  iterations is :  1.8436119099238435  | validation loss is :  1.9660534886996237\n",
      "Training loss after  4500  iterations is :  1.8350695969539799  | validation loss is :  1.9556243717674893\n",
      "[[-0.26073569]\n",
      " [ 0.80680465]\n",
      " [-0.06929063]\n",
      " [-2.44322656]\n",
      " [ 3.53202544]\n",
      " [-1.950017  ]\n",
      " [-2.26450246]\n",
      " [-0.85227447]]\n",
      "4.491873336881501\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  9.892229909526344  | validation loss is :  10.29736211031175\n",
      "Training loss after  500  iterations is :  2.929797567808123  | validation loss is :  3.1773653819731105\n",
      "Training loss after  1000  iterations is :  2.0559105978815846  | validation loss is :  2.253535674276982\n",
      "Training loss after  1500  iterations is :  1.9173343060050463  | validation loss is :  2.1075959087482325\n",
      "Training loss after  2000  iterations is :  1.8820137925454994  | validation loss is :  2.068709075664247\n",
      "Training loss after  2500  iterations is :  1.8670945662641514  | validation loss is :  2.0524494395035133\n",
      "Training loss after  3000  iterations is :  1.8561625289854908  | validation loss is :  2.0394541430668807\n",
      "Training loss after  3500  iterations is :  1.8464745278644976  | validation loss is :  2.0282202856026172\n",
      "Training loss after  4000  iterations is :  1.8373492718636266  | validation loss is :  2.0176778555239796\n",
      "Training loss after  4500  iterations is :  1.8286724443856088  | validation loss is :  2.0071773767009544\n",
      "[[-0.2622177 ]\n",
      " [ 0.7950808 ]\n",
      " [-0.08686672]\n",
      " [-2.45319363]\n",
      " [ 3.55610407]\n",
      " [-1.96701535]\n",
      " [-2.26949518]\n",
      " [-0.81891744]]\n",
      "4.48739222990983\n"
     ]
    }
   ],
   "source": [
    "models_mae,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = 10, epochs = 5000, learning_rate = 0.01, loss = \"mae\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x for x in range(len(models_mae[1][2]))],models_mae[1][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(models_mae[1][3]))], models_mae[1][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  10.486870576920118  | validation loss is :  10.035787282465531\n",
      "Training loss after  500  iterations is :  3.8475568308027923  | validation loss is :  3.529724382293449\n",
      "Training loss after  1000  iterations is :  2.990674479288378  | validation loss is :  2.707752947737866\n",
      "Training loss after  1500  iterations is :  2.7893201859558316  | validation loss is :  2.5492809103070586\n",
      "Training loss after  2000  iterations is :  2.728604937257921  | validation loss is :  2.509490501010646\n",
      "Training loss after  2500  iterations is :  2.7017182786184017  | validation loss is :  2.492397822622198\n",
      "Training loss after  3000  iterations is :  2.684545666589665  | validation loss is :  2.4799451053680874\n",
      "Training loss after  3500  iterations is :  2.6703885721039007  | validation loss is :  2.468008134487672\n",
      "Training loss after  4000  iterations is :  2.657309099291977  | validation loss is :  2.4558559892156167\n",
      "Training loss after  4500  iterations is :  2.644746721673703  | validation loss is :  2.443540713200838\n",
      "[[-0.32994345]\n",
      " [ 0.68478379]\n",
      " [-0.1802029 ]\n",
      " [-2.56873633]\n",
      " [ 3.95289619]\n",
      " [-2.48952606]\n",
      " [-2.54803564]\n",
      " [-0.57237274]]\n",
      "4.8109044828225445\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  10.481705596353182  | validation loss is :  10.082037592391158\n",
      "Training loss after  500  iterations is :  3.8541754584121293  | validation loss is :  3.4736678013034803\n",
      "Training loss after  1000  iterations is :  3.0007943769903727  | validation loss is :  2.6573838006456665\n",
      "Training loss after  1500  iterations is :  2.799820359448486  | validation loss is :  2.4818341485306608\n",
      "Training loss after  2000  iterations is :  2.7382102716145407  | validation loss is :  2.4294732468071967\n",
      "Training loss after  2500  iterations is :  2.710517091375423  | validation loss is :  2.4059737410082267\n",
      "Training loss after  3000  iterations is :  2.692664740668664  | validation loss is :  2.3912212387217404\n",
      "Training loss after  3500  iterations is :  2.6778921387972714  | validation loss is :  2.3793492120586706\n",
      "Training loss after  4000  iterations is :  2.664228527638036  | validation loss is :  2.3685520852128072\n",
      "Training loss after  4500  iterations is :  2.6511004211907583  | validation loss is :  2.3582798438370105\n",
      "[[-0.31965699]\n",
      " [ 0.69936185]\n",
      " [-0.18120791]\n",
      " [-2.56048205]\n",
      " [ 3.98764667]\n",
      " [-2.52661622]\n",
      " [-2.51566683]\n",
      " [-0.54448687]]\n",
      "4.805006442619137\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  10.443555680042602  | validation loss is :  10.40268366038925\n",
      "Training loss after  500  iterations is :  3.8204120018147667  | validation loss is :  3.7748041218071178\n",
      "Training loss after  1000  iterations is :  2.967654898937553  | validation loss is :  2.9224013341681867\n",
      "Training loss after  1500  iterations is :  2.7685844136008  | validation loss is :  2.7330958342009133\n",
      "Training loss after  2000  iterations is :  2.7082076273825115  | validation loss is :  2.680153931578145\n",
      "Training loss after  2500  iterations is :  2.681251926876256  | validation loss is :  2.658118816695299\n",
      "Training loss after  3000  iterations is :  2.6639430823458667  | validation loss is :  2.643884900825542\n",
      "Training loss after  3500  iterations is :  2.6496443074664677  | validation loss is :  2.6314909446268633\n",
      "Training loss after  4000  iterations is :  2.6364280345655464  | validation loss is :  2.619496156026733\n",
      "Training loss after  4500  iterations is :  2.6237348905041995  | validation loss is :  2.6076406005715937\n",
      "[[-0.32777921]\n",
      " [ 0.7130472 ]\n",
      " [-0.16941963]\n",
      " [-2.56586791]\n",
      " [ 3.9334839 ]\n",
      " [-2.49365521]\n",
      " [-2.53082278]\n",
      " [-0.57583983]]\n",
      "4.803761591746101\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  10.409599696092537  | validation loss is :  10.739894028388138\n",
      "Training loss after  500  iterations is :  3.7920200948386755  | validation loss is :  4.034746883708614\n",
      "Training loss after  1000  iterations is :  2.9365775343013003  | validation loss is :  3.213063978328534\n",
      "Training loss after  1500  iterations is :  2.7401235952286105  | validation loss is :  3.0186894148889154\n",
      "Training loss after  2000  iterations is :  2.681735700606817  | validation loss is :  2.9547583946979414\n",
      "Training loss after  2500  iterations is :  2.656104501245643  | validation loss is :  2.923430539613398\n",
      "Training loss after  3000  iterations is :  2.6397458393080804  | validation loss is :  2.9022171017165603\n",
      "Training loss after  3500  iterations is :  2.626223560273579  | validation loss is :  2.88454199979609\n",
      "Training loss after  4000  iterations is :  2.61370666577314  | validation loss is :  2.8683270237814003\n",
      "Training loss after  4500  iterations is :  2.6016746519484384  | validation loss is :  2.8528721208055963\n",
      "[[-0.31830463]\n",
      " [ 0.71545644]\n",
      " [-0.16611898]\n",
      " [-2.55427195]\n",
      " [ 3.93702164]\n",
      " [-2.4375718 ]\n",
      " [-2.51537914]\n",
      " [-0.62819306]]\n",
      "4.779727680449261\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  10.40397435011291  | validation loss is :  10.78534844162182\n",
      "Training loss after  500  iterations is :  3.7858860026695793  | validation loss is :  4.077096962863754\n",
      "Training loss after  1000  iterations is :  2.937338975968636  | validation loss is :  3.2090291103348916\n",
      "Training loss after  1500  iterations is :  2.7424874400886994  | validation loss is :  2.9947617251241185\n",
      "Training loss after  2000  iterations is :  2.684042122192298  | validation loss is :  2.928635509528761\n",
      "Training loss after  2500  iterations is :  2.6580178676065787  | validation loss is :  2.8993844499751433\n",
      "Training loss after  3000  iterations is :  2.6412024872500885  | validation loss is :  2.880702118485843\n",
      "Training loss after  3500  iterations is :  2.627221133735846  | validation loss is :  2.8653402808325654\n",
      "Training loss after  4000  iterations is :  2.6142566485794254  | validation loss is :  2.851209688045866\n",
      "Training loss after  4500  iterations is :  2.601791468990883  | validation loss is :  2.8376826786901415\n",
      "[[-0.31795229]\n",
      " [ 0.73384202]\n",
      " [-0.1400171 ]\n",
      " [-2.53882166]\n",
      " [ 3.90679598]\n",
      " [-2.46016259]\n",
      " [-2.53446339]\n",
      " [-0.60522819]]\n",
      "4.789159657999841\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  10.468597711831103  | validation loss is :  10.20403362196898\n",
      "Training loss after  500  iterations is :  3.8441221110401704  | validation loss is :  3.537746660607545\n",
      "Training loss after  1000  iterations is :  2.995268321305056  | validation loss is :  2.6922415606696224\n",
      "Training loss after  1500  iterations is :  2.7944582980708192  | validation loss is :  2.5104292577935805\n",
      "Training loss after  2000  iterations is :  2.7324775663774585  | validation loss is :  2.4684911886191228\n",
      "Training loss after  2500  iterations is :  2.7047876714613857  | validation loss is :  2.4544257686340805\n",
      "Training loss after  3000  iterations is :  2.687100810803679  | validation loss is :  2.445208388459966\n",
      "Training loss after  3500  iterations is :  2.672541693631976  | validation loss is :  2.435966994794725\n",
      "Training loss after  4000  iterations is :  2.659102938289127  | validation loss is :  2.4260592411510733\n",
      "Training loss after  4500  iterations is :  2.6462003480415466  | validation loss is :  2.4157016957121984\n",
      "[[-0.33626651]\n",
      " [ 0.70280561]\n",
      " [-0.17676849]\n",
      " [-2.55695362]\n",
      " [ 3.94579939]\n",
      " [-2.52476686]\n",
      " [-2.53477595]\n",
      " [-0.58901975]]\n",
      "4.8125083481622175\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  10.461757825345039  | validation loss is :  10.269276195192214\n",
      "Training loss after  500  iterations is :  3.818298929404089  | validation loss is :  3.79450691585762\n",
      "Training loss after  1000  iterations is :  2.973330646256048  | validation loss is :  2.9430298739495417\n",
      "Training loss after  1500  iterations is :  2.7764214713055044  | validation loss is :  2.7295259313360853\n",
      "Training loss after  2000  iterations is :  2.7161101080374888  | validation loss is :  2.6591552559187344\n",
      "Training loss after  2500  iterations is :  2.6890122993574193  | validation loss is :  2.626961012380595\n",
      "Training loss after  3000  iterations is :  2.6715477154034684  | validation loss is :  2.607255433229824\n",
      "Training loss after  3500  iterations is :  2.6570961557513697  | validation loss is :  2.592111855774201\n",
      "Training loss after  4000  iterations is :  2.6437319786150457  | validation loss is :  2.578874790474577\n",
      "Training loss after  4500  iterations is :  2.630896148561213  | validation loss is :  2.5665905547164862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3101512 ]\n",
      " [ 0.69267242]\n",
      " [-0.18175918]\n",
      " [-2.55694502]\n",
      " [ 3.96534206]\n",
      " [-2.49788981]\n",
      " [-2.51500152]\n",
      " [-0.54779262]]\n",
      "4.783351715030489\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  10.440459433754144  | validation loss is :  10.464740386980772\n",
      "Training loss after  500  iterations is :  3.80659803020629  | validation loss is :  3.9439709383079333\n",
      "Training loss after  1000  iterations is :  2.9675762438249045  | validation loss is :  3.0174298801989425\n",
      "Training loss after  1500  iterations is :  2.770673364279499  | validation loss is :  2.778180345103819\n",
      "Training loss after  2000  iterations is :  2.7100674364558985  | validation loss is :  2.7080429878449377\n",
      "Training loss after  2500  iterations is :  2.6829053514703984  | validation loss is :  2.67983339912071\n",
      "Training loss after  3000  iterations is :  2.6654378328066315  | validation loss is :  2.663065694035844\n",
      "Training loss after  3500  iterations is :  2.650988804197217  | validation loss is :  2.6496252380487038\n",
      "Training loss after  4000  iterations is :  2.6376232146161867  | validation loss is :  2.6372969333487837\n",
      "Training loss after  4500  iterations is :  2.624781918807389  | validation loss is :  2.6254746347670572\n",
      "[[-0.3136797 ]\n",
      " [ 0.72890773]\n",
      " [-0.14789026]\n",
      " [-2.55327364]\n",
      " [ 3.9112436 ]\n",
      " [-2.49244328]\n",
      " [-2.53087387]\n",
      " [-0.57795934]]\n",
      "4.7944240326508885\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  10.42861386893045  | validation loss is :  10.570648471721583\n",
      "Training loss after  500  iterations is :  3.8125748467031575  | validation loss is :  3.8875356404750034\n",
      "Training loss after  1000  iterations is :  2.9573998488826554  | validation loss is :  3.052227620087822\n",
      "Training loss after  1500  iterations is :  2.7595068704903696  | validation loss is :  2.860121489813156\n",
      "Training loss after  2000  iterations is :  2.700569723767137  | validation loss is :  2.796226268277063\n",
      "Training loss after  2500  iterations is :  2.6745977890434687  | validation loss is :  2.7644632691336564\n",
      "Training loss after  3000  iterations is :  2.657975602975481  | validation loss is :  2.7432268733983842\n",
      "Training loss after  3500  iterations is :  2.6442206098817937  | validation loss is :  2.72590893733229\n",
      "Training loss after  4000  iterations is :  2.631479470757402  | validation loss is :  2.7102895643692144\n",
      "Training loss after  4500  iterations is :  2.6192224274931806  | validation loss is :  2.695573261444523\n",
      "[[-0.31733785]\n",
      " [ 0.69726998]\n",
      " [-0.17647869]\n",
      " [-2.56343292]\n",
      " [ 3.95020974]\n",
      " [-2.44605962]\n",
      " [-2.51566388]\n",
      " [-0.58120448]]\n",
      "4.78838867257028\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  10.398691427070192  | validation loss is :  10.831165619685407\n",
      "Training loss after  500  iterations is :  3.7962565199391007  | validation loss is :  4.083414054654952\n",
      "Training loss after  1000  iterations is :  2.9432081151640435  | validation loss is :  3.200699881377132\n",
      "Training loss after  1500  iterations is :  2.7461878244244704  | validation loss is :  2.983761782130368\n",
      "Training loss after  2000  iterations is :  2.687073750025459  | validation loss is :  2.9112838015373432\n",
      "Training loss after  2500  iterations is :  2.6606028689453627  | validation loss is :  2.8773493984867473\n",
      "Training loss after  3000  iterations is :  2.643431027500802  | validation loss is :  2.8563567960695297\n",
      "Training loss after  3500  iterations is :  2.6291401708139435  | validation loss is :  2.84025659709372\n",
      "Training loss after  4000  iterations is :  2.6158891895523624  | validation loss is :  2.8262651308717306\n",
      "Training loss after  4500  iterations is :  2.603149805032821  | validation loss is :  2.8133403120578744\n",
      "[[-0.31105149]\n",
      " [ 0.69966281]\n",
      " [-0.1804032 ]\n",
      " [-2.55901183]\n",
      " [ 3.95361352]\n",
      " [-2.49073294]\n",
      " [-2.50921899]\n",
      " [-0.55270727]]\n",
      "4.768920585636032\n"
     ]
    }
   ],
   "source": [
    "models,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = 10, epochs = 5000, learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x for x in range(len(models[1][2]))],models[1][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(models[1][3]))], models[1][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for LR MAE Loss\n",
      "For CV number  0 the train loss =  1.8477167574636362  and the val loss =  1.736364767583854\n",
      "For CV number  1 the train loss =  1.8603356449328192  and the val loss =  1.6194507487277865\n",
      "For CV number  2 the train loss =  1.8348234643751553  and the val loss =  1.833392456765055\n",
      "For CV number  3 the train loss =  1.8284658820470958  and the val loss =  1.9229871995573304\n",
      "For CV number  4 the train loss =  1.8187509108463003  and the val loss =  2.0159666237000478\n",
      "For CV number  5 the train loss =  1.8501645427054196  and the val loss =  1.7105550493875565\n",
      "For CV number  6 the train loss =  1.839663468885081  and the val loss =  1.8114407182725596\n",
      "For CV number  7 the train loss =  1.8415982087180307  and the val loss =  1.7966541926342663\n",
      "For CV number  8 the train loss =  1.8268650868936411  and the val loss =  1.9456485515974185\n",
      "For CV number  9 the train loss =  1.8203173673361615  and the val loss =  1.9974029352388079\n",
      "Stats for LR RMSE Loss\n",
      "For CV number  0 the train loss =  2.6325630850486883  and the val loss =  2.431246718787734\n",
      "For CV number  1 the train loss =  2.6383659819341414  and the val loss =  2.3483853409168307\n",
      "For CV number  2 the train loss =  2.6114265896296733  and the val loss =  2.5959558922252817\n",
      "For CV number  3 the train loss =  2.590002195577359  and the val loss =  2.837949365605624\n",
      "For CV number  4 the train loss =  2.589701594375981  and the val loss =  2.8245829363898394\n",
      "For CV number  5 the train loss =  2.6336885344725354  and the val loss =  2.405192489365533\n",
      "For CV number  6 the train loss =  2.618450697393412  and the val loss =  2.5549080837163882\n",
      "For CV number  7 the train loss =  2.6123277744478206  and the val loss =  2.614015259776798\n",
      "For CV number  8 the train loss =  2.6073213527313435  and the val loss =  2.681478174155562\n",
      "For CV number  9 the train loss =  2.5907944259097846  and the val loss =  2.801079571903656\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats for LR MAE Loss\")\n",
    "for i in range(len(models_mae)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", models_mae[i][0], \" and the val loss = \", models_mae[i][1] )\n",
    "print(\"Stats for LR RMSE Loss\")\n",
    "for i in range(len(models)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", models[i][0], \" and the val loss = \", models[i][1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LR on Video Game dataset '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"LR on Video Game dataset \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85068818 -0.91989366]\n",
      " [ 0.97591277 -0.97591234]\n",
      " [ 0.97591277 -0.97591234]\n",
      " ...\n",
      " [ 0.97591277 -0.97591234]\n",
      " [ 0.97591277 -0.97591234]\n",
      " [ 0.9453717  -0.93251812]]\n",
      "[[0.07]\n",
      " [0.01]\n",
      " [0.02]\n",
      " ...\n",
      " [0.08]\n",
      " [1.19]\n",
      " [0.29]]\n"
     ]
    }
   ],
   "source": [
    "pp = MyPreProcessor()\n",
    "X,Y = pp.pre_process(1)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 2\n",
      "Training loss after  0  iterations is :  1.5499161784774869  | validation loss is :  1.7202719733113556\n",
      "Training loss after  500  iterations is :  1.438093787910635  | validation loss is :  1.6170855094165029\n",
      "Training loss after  1000  iterations is :  1.4331338277205545  | validation loss is :  1.6123069368880993\n",
      "Training loss after  1500  iterations is :  1.4299792120068124  | validation loss is :  1.6092189870053055\n",
      "Training loss after  2000  iterations is :  1.427978748702445  | validation loss is :  1.6072212949869573\n",
      "Training loss after  2500  iterations is :  1.4267123983879193  | validation loss is :  1.605924986446124\n",
      "Training loss after  3000  iterations is :  1.4259114799576216  | validation loss is :  1.6050797882872427\n",
      "Training loss after  3500  iterations is :  1.4254050416366904  | validation loss is :  1.6045251729472298\n",
      "Training loss after  4000  iterations is :  1.425084679403224  | validation loss is :  1.6041582948757904\n",
      "Training loss after  4500  iterations is :  1.4248817985720754  | validation loss is :  1.6039132307185595\n",
      "[[0.89452346]\n",
      " [0.17555428]]\n",
      "-0.1677951307234037\n",
      "For fold :  1 / 2\n",
      "Training loss after  0  iterations is :  1.7202719733113556  | validation loss is :  1.5499129675444863\n",
      "Training loss after  500  iterations is :  1.617352649731173  | validation loss is :  1.4384181642675198\n",
      "Training loss after  1000  iterations is :  1.6126188118012925  | validation loss is :  1.4335001980965356\n",
      "Training loss after  1500  iterations is :  1.6094713304868533  | validation loss is :  1.4302791681640554\n",
      "Training loss after  2000  iterations is :  1.6073833990089974  | validation loss is :  1.4281831525127988\n",
      "Training loss after  2500  iterations is :  1.6060002600232628  | validation loss is :  1.426828100534369\n",
      "Training loss after  3000  iterations is :  1.6050846548008784  | validation loss is :  1.4259584217745667\n",
      "Training loss after  3500  iterations is :  1.604478631351971  | validation loss is :  1.425405067634885\n",
      "Training loss after  4000  iterations is :  1.6040773538541728  | validation loss is :  1.4250567717211187\n",
      "Training loss after  4500  iterations is :  1.6038113804235279  | validation loss is :  1.4248406056185274\n",
      "[[0.9104598 ]\n",
      " [0.17622033]]\n",
      "-0.17598356909464521\n",
      "For fold :  0 / 3\n",
      "Training loss after  0  iterations is :  1.4881044455063484  | validation loss is :  1.9008772187098473\n",
      "Training loss after  500  iterations is :  1.3755588630620548  | validation loss is :  1.7998972885632598\n",
      "Training loss after  1000  iterations is :  1.370492851213478  | validation loss is :  1.7952103368391494\n",
      "Training loss after  1500  iterations is :  1.3673284437599795  | validation loss is :  1.7921725216167221\n",
      "Training loss after  2000  iterations is :  1.3653581867268685  | validation loss is :  1.7901926355354385\n",
      "Training loss after  2500  iterations is :  1.3641336988253727  | validation loss is :  1.788891914621047\n",
      "Training loss after  3000  iterations is :  1.3633733512326258  | validation loss is :  1.7880287137331612\n",
      "Training loss after  3500  iterations is :  1.362901246569275  | validation loss is :  1.7874490110146326\n",
      "Training loss after  4000  iterations is :  1.3626079089221093  | validation loss is :  1.7870544521851295\n",
      "Training loss after  4500  iterations is :  1.362425350354193  | validation loss is :  1.7867819739073836\n",
      "[[0.886838 ]\n",
      " [0.1757437]]\n",
      "-0.17129744521969462\n",
      "For fold :  1 / 3\n",
      "Training loss after  0  iterations is :  1.7797438128785805  | validation loss is :  1.3066934819356062\n",
      "Training loss after  500  iterations is :  1.6751859810828622  | validation loss is :  1.1904569350247878\n",
      "Training loss after  1000  iterations is :  1.6705880101092283  | validation loss is :  1.1849869233445478\n",
      "Training loss after  1500  iterations is :  1.6674990673161738  | validation loss is :  1.181406974712515\n",
      "Training loss after  2000  iterations is :  1.6654283761028217  | validation loss is :  1.1790897829544311\n",
      "Training loss after  2500  iterations is :  1.6640421708371518  | validation loss is :  1.1776072633058159\n",
      "Training loss after  3000  iterations is :  1.6631149123991387  | validation loss is :  1.1766721685618058\n",
      "Training loss after  3500  iterations is :  1.6624948486526332  | validation loss is :  1.1760932306790726\n",
      "Training loss after  4000  iterations is :  1.662080168259275  | validation loss is :  1.1757439131856469\n",
      "Training loss after  4500  iterations is :  1.6618026970458966  | validation loss is :  1.175541007451697\n",
      "[[0.91276121]\n",
      " [0.17419488]]\n",
      "-0.16405274436356626\n",
      "For fold :  2 / 3\n",
      "Training loss after  0  iterations is :  1.6310705160021566  | validation loss is :  1.6495759477709904\n",
      "Training loss after  500  iterations is :  1.5254030353650483  | validation loss is :  1.5399568991613497\n",
      "Training loss after  1000  iterations is :  1.5204718132250636  | validation loss is :  1.535228250290922\n",
      "Training loss after  1500  iterations is :  1.5172569442217108  | validation loss is :  1.532179919381057\n",
      "Training loss after  2000  iterations is :  1.5151664840067935  | validation loss is :  1.530225388186218\n",
      "Training loss after  2500  iterations is :  1.5138092355410298  | validation loss is :  1.528978695782638\n",
      "Training loss after  3000  iterations is :  1.512928657287704  | validation loss is :  1.5281878440356773\n",
      "Training loss after  3500  iterations is :  1.512357357303008  | validation loss is :  1.5276892800934425\n",
      "Training loss after  4000  iterations is :  1.5119864698879673  | validation loss is :  1.5273773292013946\n",
      "Training loss after  4500  iterations is :  1.5117453427828662  | validation loss is :  1.5271839634545146\n",
      "[[0.90849698]\n",
      " [0.17736543]]\n",
      "-0.18130943168375255\n",
      "For fold :  0 / 4\n",
      "Training loss after  0  iterations is :  1.5376436992311158  | validation loss is :  1.9053237652417057\n",
      "Training loss after  500  iterations is :  1.4257318559582373  | validation loss is :  1.8077876526059566\n",
      "Training loss after  1000  iterations is :  1.4207679403375184  | validation loss is :  1.803082818102817\n",
      "Training loss after  1500  iterations is :  1.4176151454021682  | validation loss is :  1.7999991490195204\n",
      "Training loss after  2000  iterations is :  1.4156185787552031  | validation loss is :  1.7979691836246658\n",
      "Training loss after  2500  iterations is :  1.4143563782741297  | validation loss is :  1.7966240883388727\n",
      "Training loss after  3000  iterations is :  1.4135590805614981  | validation loss is :  1.7957252123259309\n",
      "Training loss after  3500  iterations is :  1.4130554878988228  | validation loss is :  1.795118379457663\n",
      "Training loss after  4000  iterations is :  1.4127372024435105  | validation loss is :  1.7947038937855788\n",
      "Training loss after  4500  iterations is :  1.4125357367262823  | validation loss is :  1.7944171004661351\n",
      "[[0.89318557]\n",
      " [0.17342832]]\n",
      "-0.17094178146919486\n",
      "For fold :  1 / 4\n",
      "Training loss after  0  iterations is :  1.6766999900659438  | validation loss is :  1.512920021714094\n",
      "Training loss after  500  iterations is :  1.5711368840022126  | validation loss is :  1.4011656847571066\n",
      "Training loss after  1000  iterations is :  1.5663344216358521  | validation loss is :  1.3962321158742887\n",
      "Training loss after  1500  iterations is :  1.5631799474205512  | validation loss is :  1.3930356316163481\n",
      "Training loss after  2000  iterations is :  1.5611131006697714  | validation loss is :  1.3909778206860612\n",
      "Training loss after  2500  iterations is :  1.5597609469205551  | validation loss is :  1.3896613595522285\n",
      "Training loss after  3000  iterations is :  1.558877083357106  | validation loss is :  1.3888249432967044\n",
      "Training loss after  3500  iterations is :  1.5582994825590013  | validation loss is :  1.3882978035941498\n",
      "Training loss after  4000  iterations is :  1.5579219321254358  | validation loss is :  1.3879688845242997\n",
      "Training loss after  4500  iterations is :  1.5576749496077553  | validation loss is :  1.387766263008878\n",
      "[[0.9070028 ]\n",
      " [0.17802976]]\n",
      "-0.17068611642210635\n",
      "For fold :  2 / 4\n",
      "Training loss after  0  iterations is :  1.6746489205495743  | validation loss is :  1.5197847088120833\n",
      "Training loss after  500  iterations is :  1.5692925242019515  | validation loss is :  1.4074326277570126\n",
      "Training loss after  1000  iterations is :  1.5645371764899083  | validation loss is :  1.4024120048909763\n",
      "Training loss after  1500  iterations is :  1.5614193307622202  | validation loss is :  1.3991281430498919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  2000  iterations is :  1.5593801095250202  | validation loss is :  1.3969875419653246\n",
      "Training loss after  2500  iterations is :  1.5580483203609028  | validation loss is :  1.3955956422154614\n",
      "Training loss after  3000  iterations is :  1.5571791957708623  | validation loss is :  1.3946923448979622\n",
      "Training loss after  3500  iterations is :  1.5566120942917445  | validation loss is :  1.3941070909402131\n",
      "Training loss after  4000  iterations is :  1.5562419115734794  | validation loss is :  1.3937284386602073\n",
      "Training loss after  4500  iterations is :  1.5560000184840637  | validation loss is :  1.3934837579360722\n",
      "[[0.90202002]\n",
      " [0.17393358]]\n",
      "-0.17069599180268982\n",
      "For fold :  3 / 4\n",
      "Training loss after  0  iterations is :  1.6560975814541212  | validation loss is :  1.5796022032029848\n",
      "Training loss after  500  iterations is :  1.5504027827019944  | validation loss is :  1.4686979266693685\n",
      "Training loss after  1000  iterations is :  1.5455654946774022  | validation loss is :  1.4638518994695968\n",
      "Training loss after  1500  iterations is :  1.5423877752766948  | validation loss is :  1.460727435670509\n",
      "Training loss after  2000  iterations is :  1.5403055035464777  | validation loss is :  1.458728221712654\n",
      "Training loss after  2500  iterations is :  1.5389431155244493  | validation loss is :  1.4574592925189864\n",
      "Training loss after  3000  iterations is :  1.5380524248488694  | validation loss is :  1.4566613757230182\n",
      "Training loss after  3500  iterations is :  1.5374702172085937  | validation loss is :  1.4561654056831963\n",
      "Training loss after  4000  iterations is :  1.5370894999464781  | validation loss is :  1.4558617293546552\n",
      "Training loss after  4500  iterations is :  1.5368402817239393  | validation loss is :  1.455679568086367\n",
      "[[0.90841715]\n",
      " [0.17793293]]\n",
      "-0.17604009387894773\n",
      "For fold :  0 / 5\n",
      "Training loss after  0  iterations is :  1.6843023836669495  | validation loss is :  1.4339734545596348\n",
      "Training loss after  500  iterations is :  1.5806532844278804  | validation loss is :  1.311272050235538\n",
      "Training loss after  1000  iterations is :  1.5760534930647816  | validation loss is :  1.30562679519531\n",
      "Training loss after  1500  iterations is :  1.5730035478103188  | validation loss is :  1.3019043702723123\n",
      "Training loss after  2000  iterations is :  1.5709859853881747  | validation loss is :  1.2994605293993382\n",
      "Training loss after  2500  iterations is :  1.569653306161083  | validation loss is :  1.2978620466718085\n",
      "Training loss after  3000  iterations is :  1.5687737435988813  | validation loss is :  1.2968201029421238\n",
      "Training loss after  3500  iterations is :  1.5681934213606181  | validation loss is :  1.2961432833324074\n",
      "Training loss after  4000  iterations is :  1.5678104834524358  | validation loss is :  1.2957052645967115\n",
      "Training loss after  4500  iterations is :  1.5675576417048611  | validation loss is :  1.2954229489480722\n",
      "[[0.89718547]\n",
      " [0.17576807]]\n",
      "-0.16597738419441546\n",
      "For fold :  1 / 5\n",
      "Training loss after  0  iterations is :  1.4726801912820016  | validation loss is :  2.1746726711032434\n",
      "Training loss after  500  iterations is :  1.3572787945316585  | validation loss is :  2.0823403078213123\n",
      "Training loss after  1000  iterations is :  1.3520768760235429  | validation loss is :  2.0778709356300724\n",
      "Training loss after  1500  iterations is :  1.348853142348399  | validation loss is :  2.0749473082203993\n",
      "Training loss after  2000  iterations is :  1.3468619341868913  | validation loss is :  2.073018623182225\n",
      "Training loss after  2500  iterations is :  1.3456342390726703  | validation loss is :  2.071732443332395\n",
      "Training loss after  3000  iterations is :  1.3448778218329458  | validation loss is :  2.0708637778519923\n",
      "Training loss after  3500  iterations is :  1.3444116553129624  | validation loss is :  2.0702688172985897\n",
      "Training loss after  4000  iterations is :  1.344124004557227  | validation loss is :  2.0698552422925394\n",
      "Training loss after  4500  iterations is :  1.343946056668676  | validation loss is :  2.0695633814337184\n",
      "[[0.89248992]\n",
      " [0.17193058]]\n",
      "-0.17728031047483808\n",
      "For fold :  2 / 5\n",
      "Training loss after  0  iterations is :  1.7025149621256437  | validation loss is :  1.3451820943225608\n",
      "Training loss after  500  iterations is :  1.59836559612508  | validation loss is :  1.2222925631845942\n",
      "Training loss after  1000  iterations is :  1.5937603360450334  | validation loss is :  1.2165281490031319\n",
      "Training loss after  1500  iterations is :  1.5907191178915432  | validation loss is :  1.2127022131761802\n",
      "Training loss after  2000  iterations is :  1.5887154259250829  | validation loss is :  1.2101686134949605\n",
      "Training loss after  2500  iterations is :  1.5873971950407901  | validation loss is :  1.208492190022461\n",
      "Training loss after  3000  iterations is :  1.5865306074173866  | validation loss is :  1.2073827473889533\n",
      "Training loss after  3500  iterations is :  1.5859610738599064  | validation loss is :  1.2066477603896923\n",
      "Training loss after  4000  iterations is :  1.5855866896822786  | validation loss is :  1.2061599154609282\n",
      "Training loss after  4500  iterations is :  1.5853404103895725  | validation loss is :  1.20583518536748\n",
      "[[0.89652111]\n",
      " [0.17270883]]\n",
      "-0.16324769485868146\n",
      "For fold :  3 / 5\n",
      "Training loss after  0  iterations is :  1.639642532274904  | validation loss is :  1.6279317739402497\n",
      "Training loss after  500  iterations is :  1.5309379811436261  | validation loss is :  1.526979538219409\n",
      "Training loss after  1000  iterations is :  1.5258014797315957  | validation loss is :  1.5229312971036248\n",
      "Training loss after  1500  iterations is :  1.522477208399869  | validation loss is :  1.5204206148699304\n",
      "Training loss after  2000  iterations is :  1.520331763772401  | validation loss is :  1.5188879560426924\n",
      "Training loss after  2500  iterations is :  1.5189494183239847  | validation loss is :  1.5179706444916534\n",
      "Training loss after  3000  iterations is :  1.5180595025138424  | validation loss is :  1.517436278047221\n",
      "Training loss after  3500  iterations is :  1.5174867063747344  | validation loss is :  1.5171372386324375\n",
      "Training loss after  4000  iterations is :  1.517117864524988  | validation loss is :  1.516980527201568\n",
      "Training loss after  4500  iterations is :  1.516880084899613  | validation loss is :  1.516908058408378\n",
      "[[0.92096948]\n",
      " [0.18375312]]\n",
      "-0.17957373062189985\n",
      "For fold :  4 / 5\n",
      "Training loss after  0  iterations is :  1.6765355734287186  | validation loss is :  1.4699445373433957\n",
      "Training loss after  500  iterations is :  1.5716298295527258  | validation loss is :  1.3534313636755193\n",
      "Training loss after  1000  iterations is :  1.5669044005437613  | validation loss is :  1.3482007429368699\n",
      "Training loss after  1500  iterations is :  1.5637767386182784  | validation loss is :  1.34481998764934\n",
      "Training loss after  2000  iterations is :  1.5617115201153862  | validation loss is :  1.3426552374539453\n",
      "Training loss after  2500  iterations is :  1.560349792897254  | validation loss is :  1.3412833551320325\n",
      "Training loss after  3000  iterations is :  1.5594525700815076  | validation loss is :  1.3404247174062582\n",
      "Training loss after  3500  iterations is :  1.5588614822859068  | validation loss is :  1.339895886421609\n",
      "Training loss after  4000  iterations is :  1.5584719078622502  | validation loss is :  1.3395772521476126\n",
      "Training loss after  4500  iterations is :  1.558214872978491  | validation loss is :  1.3393912588980696\n",
      "[[0.9060176 ]\n",
      " [0.17427619]]\n",
      "-0.17507634151230744\n",
      "For fold :  0 / 6\n",
      "Training loss after  0  iterations is :  1.682198400460771  | validation loss is :  1.3912806234370805\n",
      "Training loss after  500  iterations is :  1.577642498388802  | validation loss is :  1.2691589334843458\n",
      "Training loss after  1000  iterations is :  1.5730209577046141  | validation loss is :  1.2633537980968121\n",
      "Training loss after  1500  iterations is :  1.5699665506259575  | validation loss is :  1.2595210082229757\n",
      "Training loss after  2000  iterations is :  1.5679526269603439  | validation loss is :  1.2569992789103508\n",
      "Training loss after  2500  iterations is :  1.56662667290181  | validation loss is :  1.2553442234259942\n",
      "Training loss after  3000  iterations is :  1.5657543603035318  | validation loss is :  1.2542599811784927\n",
      "Training loss after  3500  iterations is :  1.565180634026  | validation loss is :  1.2535506907628349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  4000  iterations is :  1.5648032028624703  | validation loss is :  1.2530871916492172\n",
      "Training loss after  4500  iterations is :  1.5645547182102544  | validation loss is :  1.2527845394135104\n",
      "[[0.89733321]\n",
      " [0.17332226]]\n",
      "-0.16657037059708443\n",
      "For fold :  1 / 6\n",
      "Training loss after  0  iterations is :  1.4692038044878073  | validation loss is :  2.3004788118604997\n",
      "Training loss after  500  iterations is :  1.3545857367792067  | validation loss is :  2.206952498208223\n",
      "Training loss after  1000  iterations is :  1.3493039689485347  | validation loss is :  2.2026595174238643\n",
      "Training loss after  1500  iterations is :  1.3460375603353214  | validation loss is :  2.1998513710364027\n",
      "Training loss after  2000  iterations is :  1.3440244108011938  | validation loss is :  2.197998158952827\n",
      "Training loss after  2500  iterations is :  1.3427860300764147  | validation loss is :  2.1967613561741373\n",
      "Training loss after  3000  iterations is :  1.342024866715263  | validation loss is :  2.195925086460409\n",
      "Training loss after  3500  iterations is :  1.3415569824033735  | validation loss is :  2.1953514722069127\n",
      "Training loss after  4000  iterations is :  1.3412690865330095  | validation loss is :  2.194952039397299\n",
      "Training loss after  4500  iterations is :  1.3410915582576675  | validation loss is :  2.1946696049254766\n",
      "[[0.89547416]\n",
      " [0.17796885]]\n",
      "-0.17746487021081803\n",
      "For fold :  2 / 6\n",
      "Training loss after  0  iterations is :  1.6990217963874046  | validation loss is :  1.2848789541341008\n",
      "Training loss after  500  iterations is :  1.5928732543637085  | validation loss is :  1.169677844397403\n",
      "Training loss after  1000  iterations is :  1.5881127627492964  | validation loss is :  1.164270862070575\n",
      "Training loss after  1500  iterations is :  1.5849653983989822  | validation loss is :  1.160822608208721\n",
      "Training loss after  2000  iterations is :  1.5828895278333976  | validation loss is :  1.1586549424506036\n",
      "Training loss after  2500  iterations is :  1.5815223858483733  | validation loss is :  1.15731485676269\n",
      "Training loss after  3000  iterations is :  1.580622715671843  | validation loss is :  1.1565042851581335\n",
      "Training loss after  3500  iterations is :  1.58003081694145  | validation loss is :  1.1560288450325706\n",
      "Training loss after  4000  iterations is :  1.579641300636137  | validation loss is :  1.155762758640405\n",
      "Training loss after  4500  iterations is :  1.5793847595185777  | validation loss is :  1.1556252660279425\n",
      "[[0.91044071]\n",
      " [0.1756771 ]]\n",
      "-0.17026299948011972\n",
      "For fold :  3 / 6\n",
      "Training loss after  0  iterations is :  1.6923315860799133  | validation loss is :  1.3283972012136998\n",
      "Training loss after  500  iterations is :  1.5868964037435693  | validation loss is :  1.209745555604709\n",
      "Training loss after  1000  iterations is :  1.582203339717427  | validation loss is :  1.2041359174576063\n",
      "Training loss after  1500  iterations is :  1.579108034597891  | validation loss is :  1.2004728876577402\n",
      "Training loss after  2000  iterations is :  1.5770713983664704  | validation loss is :  1.198095534314111\n",
      "Training loss after  2500  iterations is :  1.5757333041230044  | validation loss is :  1.1965610742121104\n",
      "Training loss after  3000  iterations is :  1.5748548548209844  | validation loss is :  1.1955763123282304\n",
      "Training loss after  3500  iterations is :  1.5742783033712067  | validation loss is :  1.1949483777350345\n",
      "Training loss after  4000  iterations is :  1.5738998029443079  | validation loss is :  1.1945510190166926\n",
      "Training loss after  4500  iterations is :  1.5736511260946577  | validation loss is :  1.1943019233594974\n",
      "[[0.90253629]\n",
      " [0.17467548]]\n",
      "-0.16716596033808848\n",
      "For fold :  4 / 6\n",
      "Training loss after  0  iterations is :  1.5964746570958632  | validation loss is :  1.8267502891222782\n",
      "Training loss after  500  iterations is :  1.4895283577080582  | validation loss is :  1.7188800066646506\n",
      "Training loss after  1000  iterations is :  1.484579064026934  | validation loss is :  1.7144024176998331\n",
      "Training loss after  1500  iterations is :  1.4813924348363612  | validation loss is :  1.7114876069744493\n",
      "Training loss after  2000  iterations is :  1.479346322614718  | validation loss is :  1.709589409480377\n",
      "Training loss after  2500  iterations is :  1.4780346188036937  | validation loss is :  1.708351002677504\n",
      "Training loss after  3000  iterations is :  1.4771943470970612  | validation loss is :  1.7075404372305782\n",
      "Training loss after  3500  iterations is :  1.4766560913729412  | validation loss is :  1.7070074778897506\n",
      "Training loss after  4000  iterations is :  1.4763110695064543  | validation loss is :  1.7066549739137684\n",
      "Training loss after  4500  iterations is :  1.4760895802036174  | validation loss is :  1.7064201139708957\n",
      "[[0.90036331]\n",
      " [0.17655964]]\n",
      "-0.1771994559083906\n",
      "For fold :  5 / 6\n",
      "Training loss after  0  iterations is :  1.672062537981645  | validation loss is :  1.4510951999277684\n",
      "Training loss after  500  iterations is :  1.5661125394598006  | validation loss is :  1.3378550723121725\n",
      "Training loss after  1000  iterations is :  1.5613244766224095  | validation loss is :  1.332762967793641\n",
      "Training loss after  1500  iterations is :  1.5581681873069184  | validation loss is :  1.329505042906919\n",
      "Training loss after  2000  iterations is :  1.5560926582367045  | validation loss is :  1.327444436577457\n",
      "Training loss after  2500  iterations is :  1.554729850002122  | validation loss is :  1.3261581542190015\n",
      "Training loss after  3000  iterations is :  1.5538357107155243  | validation loss is :  1.3253684269232815\n",
      "Training loss after  3500  iterations is :  1.5532491768610917  | validation loss is :  1.3248942877300616\n",
      "Training loss after  4000  iterations is :  1.552864286592447  | validation loss is :  1.324618636039968\n",
      "Training loss after  4500  iterations is :  1.552611472218307  | validation loss is :  1.324466193365351\n",
      "[[0.90854728]\n",
      " [0.17597525]]\n",
      "-0.17380526831335846\n",
      "For fold :  0 / 7\n",
      "Training loss after  0  iterations is :  1.6694605707677617  | validation loss is :  1.4292777046882554\n",
      "Training loss after  500  iterations is :  1.5645819956959743  | validation loss is :  1.3077571798782717\n",
      "Training loss after  1000  iterations is :  1.559944331967836  | validation loss is :  1.3019317655122786\n",
      "Training loss after  1500  iterations is :  1.556891608440133  | validation loss is :  1.2980613520209519\n",
      "Training loss after  2000  iterations is :  1.554886994994075  | validation loss is :  1.2954924514185309\n",
      "Training loss after  2500  iterations is :  1.5535725832343716  | validation loss is :  1.2937865553003545\n",
      "Training loss after  3000  iterations is :  1.5527114233472872  | validation loss is :  1.292651772461218\n",
      "Training loss after  3500  iterations is :  1.552147368662813  | validation loss is :  1.2918947128250369\n",
      "Training loss after  4000  iterations is :  1.5517778325597595  | validation loss is :  1.2913875892005395\n",
      "Training loss after  4500  iterations is :  1.5515355507345518  | validation loss is :  1.2910460692726071\n",
      "[[0.89539641]\n",
      " [0.17329116]]\n",
      "-0.16566064301218456\n",
      "For fold :  1 / 7\n",
      "Training loss after  0  iterations is :  1.5162356723465853  | validation loss is :  2.229746616742443\n",
      "Training loss after  500  iterations is :  1.4033507759544714  | validation loss is :  2.1387595402679325\n",
      "Training loss after  1000  iterations is :  1.3983847390386415  | validation loss is :  2.1340106400097683\n",
      "Training loss after  1500  iterations is :  1.3952576972896802  | validation loss is :  2.130812218267238\n",
      "Training loss after  2000  iterations is :  1.3932945249002238  | validation loss is :  2.1286375228653553\n",
      "Training loss after  2500  iterations is :  1.3920640854766957  | validation loss is :  2.127142032692967\n",
      "Training loss after  3000  iterations is :  1.3912934210662586  | validation loss is :  2.126100543584231\n",
      "Training loss after  3500  iterations is :  1.3908106548804844  | validation loss is :  2.1253654738859313\n",
      "Training loss after  4000  iterations is :  1.390507927468686  | validation loss is :  2.1248395801547244\n",
      "Training loss after  4500  iterations is :  1.3903176971207931  | validation loss is :  2.124458285051177\n",
      "[[0.88736024]\n",
      " [0.16854982]]\n",
      "-0.17173596895997126\n",
      "For fold :  2 / 7\n",
      "Training loss after  0  iterations is :  1.6386197321701461  | validation loss is :  1.6294182815803955\n",
      "Training loss after  500  iterations is :  1.5315984702238774  | validation loss is :  1.5223671519572657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  1000  iterations is :  1.5266431919660077  | validation loss is :  1.518019262404483\n",
      "Training loss after  1500  iterations is :  1.5234159428199145  | validation loss is :  1.5152757933190844\n",
      "Training loss after  2000  iterations is :  1.5213197135996024  | validation loss is :  1.513564705795811\n",
      "Training loss after  2500  iterations is :  1.5199603239671433  | validation loss is :  1.5125120282874427\n",
      "Training loss after  3000  iterations is :  1.5190795175408895  | validation loss is :  1.511875643478033\n",
      "Training loss after  3500  iterations is :  1.518508941466564  | validation loss is :  1.5115000093469002\n",
      "Training loss after  4000  iterations is :  1.5181392076647788  | validation loss is :  1.511285881291332\n",
      "Training loss after  4500  iterations is :  1.5178993911643277  | validation loss is :  1.5111703625271824\n",
      "[[0.91100698]\n",
      " [0.18099164]]\n",
      "-0.17605629836498002\n",
      "For fold :  3 / 7\n",
      "Training loss after  0  iterations is :  1.6874454114868667  | validation loss is :  1.2963579313203406\n",
      "Training loss after  500  iterations is :  1.582446309978346  | validation loss is :  1.171805889092243\n",
      "Training loss after  1000  iterations is :  1.5778140276696646  | validation loss is :  1.1656839198275335\n",
      "Training loss after  1500  iterations is :  1.5747552613423288  | validation loss is :  1.161642427587311\n",
      "Training loss after  2000  iterations is :  1.5727402468393412  | validation loss is :  1.1589844813433214\n",
      "Training loss after  2500  iterations is :  1.5714147220680912  | validation loss is :  1.1572409466362963\n",
      "Training loss after  3000  iterations is :  1.570543419710017  | validation loss is :  1.1560994412098478\n",
      "Training loss after  3500  iterations is :  1.5699708098465308  | validation loss is :  1.1553532034986742\n",
      "Training loss after  4000  iterations is :  1.5695943841237938  | validation loss is :  1.1548659372631807\n",
      "Training loss after  4500  iterations is :  1.5693467137800319  | validation loss is :  1.154548050589103\n",
      "[[0.89819859]\n",
      " [0.17230655]]\n",
      "-0.16669004450123578\n",
      "For fold :  4 / 7\n",
      "Training loss after  0  iterations is :  1.6893335501565632  | validation loss is :  1.2811773532127626\n",
      "Training loss after  500  iterations is :  1.583076614798985  | validation loss is :  1.1648048584181416\n",
      "Training loss after  1000  iterations is :  1.57815129062953  | validation loss is :  1.1602305466747456\n",
      "Training loss after  1500  iterations is :  1.5749115104508356  | validation loss is :  1.1574611608413732\n",
      "Training loss after  2000  iterations is :  1.5727858420801135  | validation loss is :  1.1558409284810842\n",
      "Training loss after  2500  iterations is :  1.5713933357307344  | validation loss is :  1.1549392740556006\n",
      "Training loss after  3000  iterations is :  1.5704818958442737  | validation loss is :  1.1544783193539498\n",
      "Training loss after  3500  iterations is :  1.5698855049040532  | validation loss is :  1.154280962157472\n",
      "Training loss after  4000  iterations is :  1.569495179752193  | validation loss is :  1.154235767419026\n",
      "Training loss after  4500  iterations is :  1.5692395268194885  | validation loss is :  1.1542736490736032\n",
      "[[0.91817426]\n",
      " [0.18254813]]\n",
      "-0.17412586022014903\n",
      "For fold :  5 / 7\n",
      "Training loss after  0  iterations is :  1.5718127000350983  | validation loss is :  1.9854756949767438\n",
      "Training loss after  500  iterations is :  1.4646230397819633  | validation loss is :  1.8768866752826188\n",
      "Training loss after  1000  iterations is :  1.4597466205433185  | validation loss is :  1.8721176663266117\n",
      "Training loss after  1500  iterations is :  1.4566230101588296  | validation loss is :  1.868929240844069\n",
      "Training loss after  2000  iterations is :  1.4546276888099252  | validation loss is :  1.8667841704336596\n",
      "Training loss after  2500  iterations is :  1.4533551285411366  | validation loss is :  1.8653292629991547\n",
      "Training loss after  3000  iterations is :  1.4525441111467563  | validation loss is :  1.8643327215166665\n",
      "Training loss after  3500  iterations is :  1.4520272387342175  | validation loss is :  1.8636424650211314\n",
      "Training loss after  4000  iterations is :  1.4516975888761832  | validation loss is :  1.8631584902303087\n",
      "Training loss after  4500  iterations is :  1.451487009710188  | validation loss is :  1.86281475592941\n",
      "[[0.89060858]\n",
      " [0.17334063]]\n",
      "-0.17396229846310612\n",
      "For fold :  6 / 7\n",
      "Training loss after  0  iterations is :  1.6798782916236898  | validation loss is :  1.354048879041732\n",
      "Training loss after  500  iterations is :  1.573191041968777  | validation loss is :  1.2429342574838007\n",
      "Training loss after  1000  iterations is :  1.568309073040032  | validation loss is :  1.2381672190087427\n",
      "Training loss after  1500  iterations is :  1.565089418626947  | validation loss is :  1.2352429264027542\n",
      "Training loss after  2000  iterations is :  1.562971342733979  | validation loss is :  1.233499833679805\n",
      "Training loss after  2500  iterations is :  1.5615800601342096  | validation loss is :  1.2325018255783222\n",
      "Training loss after  3000  iterations is :  1.560666905524869  | validation loss is :  1.2319659277732975\n",
      "Training loss after  3500  iterations is :  1.560067690236992  | validation loss is :  1.2317106359941001\n",
      "Training loss after  4000  iterations is :  1.559674348796391  | validation loss is :  1.2316208640611999\n",
      "Training loss after  4500  iterations is :  1.5594159034941835  | validation loss is :  1.2316245783198343\n",
      "[[0.91677533]\n",
      " [0.1788626 ]]\n",
      "-0.17650394644503042\n",
      "For fold :  0 / 8\n",
      "Training loss after  0  iterations is :  1.6628917242136976  | validation loss is :  1.44554439979406\n",
      "Training loss after  500  iterations is :  1.5578249614131126  | validation loss is :  1.323244175183682\n",
      "Training loss after  1000  iterations is :  1.5531281762599636  | validation loss is :  1.3176081975792047\n",
      "Training loss after  1500  iterations is :  1.5500498710619377  | validation loss is :  1.3138670871709532\n",
      "Training loss after  2000  iterations is :  1.5480372561770994  | validation loss is :  1.311384193675841\n",
      "Training loss after  2500  iterations is :  1.5467233391382234  | validation loss is :  1.3097339807824995\n",
      "Training loss after  3000  iterations is :  1.5458662167073247  | validation loss is :  1.3086340985144376\n",
      "Training loss after  3500  iterations is :  1.5453071855226581  | validation loss is :  1.3078979981827605\n",
      "Training loss after  4000  iterations is :  1.5449424456336807  | validation loss is :  1.3074026953087792\n",
      "Training loss after  4500  iterations is :  1.5447042425486055  | validation loss is :  1.3070671661817188\n",
      "[[0.89663357]\n",
      " [0.17275057]]\n",
      "-0.1684187496113216\n",
      "For fold :  1 / 8\n",
      "Training loss after  0  iterations is :  1.5247795746875141  | validation loss is :  2.274304379734654\n",
      "Training loss after  500  iterations is :  1.411363275354664  | validation loss is :  2.1879653022225867\n",
      "Training loss after  1000  iterations is :  1.4062656063498702  | validation loss is :  2.1836098111263307\n",
      "Training loss after  1500  iterations is :  1.4030394998196756  | validation loss is :  2.1807449127379677\n",
      "Training loss after  2000  iterations is :  1.4010041085036329  | validation loss is :  2.178849405905335\n",
      "Training loss after  2500  iterations is :  1.399722233308526  | validation loss is :  2.1775851431548996\n",
      "Training loss after  3000  iterations is :  1.398915588567613  | validation loss is :  2.176733472518745\n",
      "Training loss after  3500  iterations is :  1.3984080224861164  | validation loss is :  2.1761530837959056\n",
      "Training loss after  4000  iterations is :  1.3980884232709019  | validation loss is :  2.175752461872066\n",
      "Training loss after  4500  iterations is :  1.3978868591325033  | validation loss is :  2.175472092861467\n",
      "[[0.9003407 ]\n",
      " [0.17669908]]\n",
      "-0.1748112325788576\n",
      "For fold :  2 / 8\n",
      "Training loss after  0  iterations is :  1.601332268832439  | validation loss is :  1.869967476415844\n",
      "Training loss after  500  iterations is :  1.4955486064798333  | validation loss is :  1.7556125319959532\n",
      "Training loss after  1000  iterations is :  1.4908509057323238  | validation loss is :  1.7502649031418749\n",
      "Training loss after  1500  iterations is :  1.487812367687942  | validation loss is :  1.7466238103665754\n",
      "Training loss after  2000  iterations is :  1.4858520741422032  | validation loss is :  1.7441271506455738\n",
      "Training loss after  2500  iterations is :  1.4845893282288904  | validation loss is :  1.742400218485123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  3000  iterations is :  1.4837765184330012  | validation loss is :  1.7411934511481586\n",
      "Training loss after  3500  iterations is :  1.4832533770976675  | validation loss is :  1.7403405885991896\n",
      "Training loss after  4000  iterations is :  1.4829164957754306  | validation loss is :  1.7397305538659202\n",
      "Training loss after  4500  iterations is :  1.4826992877729783  | validation loss is :  1.7392887737010738\n",
      "[[0.88472008]\n",
      " [0.17049443]]\n",
      "-0.1685193520749572\n",
      "For fold :  3 / 8\n",
      "Training loss after  0  iterations is :  1.7056142111595616  | validation loss is :  1.040100291588724\n",
      "Training loss after  500  iterations is :  1.5985516805871574  | validation loss is :  0.921078082191854\n",
      "Training loss after  1000  iterations is :  1.5936090718174278  | validation loss is :  0.9163399538261263\n",
      "Training loss after  1500  iterations is :  1.5903457756984594  | validation loss is :  0.9136636598064398\n",
      "Training loss after  2000  iterations is :  1.5881966187733842  | validation loss is :  0.912275225139202\n",
      "Training loss after  2500  iterations is :  1.586783414539884  | validation loss is :  0.9116667150880604\n",
      "Training loss after  3000  iterations is :  1.5858549435026914  | validation loss is :  0.911513651188374\n",
      "Training loss after  3500  iterations is :  1.5852451312251115  | validation loss is :  0.9116125509270014\n",
      "Training loss after  4000  iterations is :  1.5848445390084258  | validation loss is :  0.9118384198772546\n",
      "Training loss after  4500  iterations is :  1.5845812002308493  | validation loss is :  0.9121164625696124\n",
      "[[0.92382531]\n",
      " [0.18275393]]\n",
      "-0.17426933163211872\n",
      "For fold :  4 / 8\n",
      "Training loss after  0  iterations is :  1.6608173632382845  | validation loss is :  1.4621470373502032\n",
      "Training loss after  500  iterations is :  1.5561671510996073  | validation loss is :  1.3374490141461892\n",
      "Training loss after  1000  iterations is :  1.5515702071179187  | validation loss is :  1.331297335334763\n",
      "Training loss after  1500  iterations is :  1.548557366818227  | validation loss is :  1.3271154894136616\n",
      "Training loss after  2000  iterations is :  1.5465874625758638  | validation loss is :  1.3242611857792368\n",
      "Training loss after  2500  iterations is :  1.5453013194272116  | validation loss is :  1.3223013181144756\n",
      "Training loss after  3000  iterations is :  1.5444622193299564  | validation loss is :  1.320945300231973\n",
      "Training loss after  3500  iterations is :  1.5439148669531197  | validation loss is :  1.3199985764865196\n",
      "Training loss after  4000  iterations is :  1.5435576890590588  | validation loss is :  1.319330839560858\n",
      "Training loss after  4500  iterations is :  1.5433243804068324  | validation loss is :  1.3188546136292045\n",
      "[[0.88876794]\n",
      " [0.16939599]]\n",
      "-0.1648917074554358\n",
      "For fold :  5 / 8\n",
      "Training loss after  0  iterations is :  1.6459015544662219  | validation loss is :  1.5758408668167987\n",
      "Training loss after  500  iterations is :  1.538067353571791  | validation loss is :  1.474298451538263\n",
      "Training loss after  1000  iterations is :  1.5330615660000946  | validation loss is :  1.4702711319747008\n",
      "Training loss after  1500  iterations is :  1.5298001689644007  | validation loss is :  1.467796972223838\n",
      "Training loss after  2000  iterations is :  1.5276809362338755  | validation loss is :  1.4663103044017582\n",
      "Training loss after  2500  iterations is :  1.5263060682763294  | validation loss is :  1.4654432231466898\n",
      "Training loss after  3000  iterations is :  1.5254148346449936  | validation loss is :  1.464959462910816\n",
      "Training loss after  3500  iterations is :  1.5248372078303956  | validation loss is :  1.464708816147356\n",
      "Training loss after  4000  iterations is :  1.5244626737640756  | validation loss is :  1.4645967306396799\n",
      "Training loss after  4500  iterations is :  1.5242195538735503  | validation loss is :  1.4645643401917798\n",
      "[[0.91604271]\n",
      " [0.18069721]]\n",
      "-0.17814602273515487\n",
      "For fold :  6 / 8\n",
      "Training loss after  0  iterations is :  1.6189392695422706  | validation loss is :  1.7606024987057751\n",
      "Training loss after  500  iterations is :  1.5129778857142973  | validation loss is :  1.6474565705778845\n",
      "Training loss after  1000  iterations is :  1.5082327196710037  | validation loss is :  1.6422814894344695\n",
      "Training loss after  1500  iterations is :  1.505149741706452  | validation loss is :  1.6388162645473343\n",
      "Training loss after  2000  iterations is :  1.5031518307763305  | validation loss is :  1.6364870211091116\n",
      "Training loss after  2500  iterations is :  1.5018590570556491  | validation loss is :  1.634912553549937\n",
      "Training loss after  3000  iterations is :  1.5010231750660383  | validation loss is :  1.6338404989817368\n",
      "Training loss after  3500  iterations is :  1.50048277030957  | validation loss is :  1.6331041114794618\n",
      "Training loss after  4000  iterations is :  1.5001332185074547  | validation loss is :  1.632593176439606\n",
      "Training loss after  4500  iterations is :  1.499906843507863  | validation loss is :  1.6322346882529717\n",
      "[[0.89228823]\n",
      " [0.17272875]]\n",
      "-0.17062471368453688\n",
      "For fold :  7 / 8\n",
      "Training loss after  0  iterations is :  1.6713660954619765  | validation loss is :  1.3753995198332343\n",
      "Training loss after  500  iterations is :  1.5644789774662646  | validation loss is :  1.2657776829461689\n",
      "Training loss after  1000  iterations is :  1.5595522526549903  | validation loss is :  1.2613272567271574\n",
      "Training loss after  1500  iterations is :  1.5563110827403737  | validation loss is :  1.2586537344331354\n",
      "Training loss after  2000  iterations is :  1.5541842227136335  | validation loss is :  1.2571074446973154\n",
      "Training loss after  2500  iterations is :  1.5527907416051576  | validation loss is :  1.2562630799698444\n",
      "Training loss after  3000  iterations is :  1.5518785089837805  | validation loss is :  1.255846784221971\n",
      "Training loss after  3500  iterations is :  1.5512814631417464  | validation loss is :  1.2556844770500855\n",
      "Training loss after  4000  iterations is :  1.5508905822898842  | validation loss is :  1.2556669833659744\n",
      "Training loss after  4500  iterations is :  1.5506344431094754  | validation loss is :  1.2557269104502178\n",
      "[[0.91795185]\n",
      " [0.18076182]]\n",
      "-0.176831406879989\n",
      "For fold :  0 / 9\n",
      "Training loss after  0  iterations is :  1.6550120161401851  | validation loss is :  1.4880894025579865\n",
      "Training loss after  500  iterations is :  1.5497819545031317  | validation loss is :  1.366498976307026\n",
      "Training loss after  1000  iterations is :  1.5451240605093135  | validation loss is :  1.360583548954328\n",
      "Training loss after  1500  iterations is :  1.542072666191101  | validation loss is :  1.356606971345701\n",
      "Training loss after  2000  iterations is :  1.5400785651923932  | validation loss is :  1.3539273700883925\n",
      "Training loss after  2500  iterations is :  1.5387773312773163  | validation loss is :  1.3521140184734064\n",
      "Training loss after  3000  iterations is :  1.5379288712479977  | validation loss is :  1.3508795970425118\n",
      "Training loss after  3500  iterations is :  1.5373757444917013  | validation loss is :  1.350033026029008\n",
      "Training loss after  4000  iterations is :  1.5370150249591419  | validation loss is :  1.3494473216248755\n",
      "Training loss after  4500  iterations is :  1.5367795593374696  | validation loss is :  1.3490380084093267\n",
      "[[0.89314993]\n",
      " [0.17142342]]\n",
      "-0.166938170320105\n",
      "For fold :  1 / 9\n",
      "Training loss after  0  iterations is :  1.5306567281772112  | validation loss is :  2.320496031220495\n",
      "Training loss after  500  iterations is :  1.4165838302780955  | validation loss is :  2.239219867859332\n",
      "Training loss after  1000  iterations is :  1.411271656191846  | validation loss is :  2.235642580431789\n",
      "Training loss after  1500  iterations is :  1.4079122744472352  | validation loss is :  2.233393265294501\n",
      "Training loss after  2000  iterations is :  1.4057946209194034  | validation loss is :  2.231983957107282\n",
      "Training loss after  2500  iterations is :  1.4044621797742758  | validation loss is :  2.2311034038797963\n",
      "Training loss after  3000  iterations is :  1.4036245252126196  | validation loss is :  2.230554492594445\n",
      "Training loss after  3500  iterations is :  1.4030979584222485  | validation loss is :  2.230213023714362\n",
      "Training loss after  4000  iterations is :  1.4027667090933595  | validation loss is :  2.230001009287711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  4500  iterations is :  1.4025579834209332  | validation loss is :  2.2298696008437116\n",
      "[[0.91515197]\n",
      " [0.18310095]]\n",
      "-0.18175613814238512\n",
      "For fold :  2 / 9\n",
      "Training loss after  0  iterations is :  1.6156519726359981  | validation loss is :  1.8011538514390104\n",
      "Training loss after  500  iterations is :  1.5099415299435435  | validation loss is :  1.6855595019343195\n",
      "Training loss after  1000  iterations is :  1.5052241303643894  | validation loss is :  1.6802219855729297\n",
      "Training loss after  1500  iterations is :  1.5021680080831854  | validation loss is :  1.6765889485835397\n",
      "Training loss after  2000  iterations is :  1.5001932433012228  | validation loss is :  1.674099349692292\n",
      "Training loss after  2500  iterations is :  1.4989191890715783  | validation loss is :  1.6723788136391262\n",
      "Training loss after  3000  iterations is :  1.4980978606549655  | validation loss is :  1.671177827557057\n",
      "Training loss after  3500  iterations is :  1.4975684820637964  | validation loss is :  1.6703300978247122\n",
      "Training loss after  4000  iterations is :  1.4972271434313682  | validation loss is :  1.6697245137604064\n",
      "Training loss after  4500  iterations is :  1.497006823123639  | validation loss is :  1.6692864957604845\n",
      "[[0.88793585]\n",
      " [0.17287567]]\n",
      "-0.16724334078435973\n",
      "For fold :  3 / 9\n",
      "Training loss after  0  iterations is :  1.669772648334709  | validation loss is :  1.3495642807886605\n",
      "Training loss after  500  iterations is :  1.5630762788210109  | validation loss is :  1.2385804658579445\n",
      "Training loss after  1000  iterations is :  1.5582824196040783  | validation loss is :  1.2333300874358004\n",
      "Training loss after  1500  iterations is :  1.55513574787622  | validation loss is :  1.2299705184423513\n",
      "Training loss after  2000  iterations is :  1.5530753821822563  | validation loss is :  1.2278436947255482\n",
      "Training loss after  2500  iterations is :  1.551728307795236  | validation loss is :  1.2265128678212325\n",
      "Training loss after  3000  iterations is :  1.5508482515576558  | validation loss is :  1.225691957215415\n",
      "Training loss after  3500  iterations is :  1.5502733937292659  | validation loss is :  1.2251950375024918\n",
      "Training loss after  4000  iterations is :  1.5498977390116306  | validation loss is :  1.2249020658449374\n",
      "Training loss after  4500  iterations is :  1.5496520001324383  | validation loss is :  1.2247360119543225\n",
      "[[0.90668634]\n",
      " [0.1737885 ]]\n",
      "-0.17218548095135036\n",
      "For fold :  4 / 9\n",
      "Training loss after  0  iterations is :  1.6730302729055064  | validation loss is :  1.3170158497600235\n",
      "Training loss after  500  iterations is :  1.5679300679410344  | validation loss is :  1.189851825942558\n",
      "Training loss after  1000  iterations is :  1.5633252320439774  | validation loss is :  1.1832496397432812\n",
      "Training loss after  1500  iterations is :  1.5602962442480108  | validation loss is :  1.1788005824125676\n",
      "Training loss after  2000  iterations is :  1.5583085439639952  | validation loss is :  1.1757982932476938\n",
      "Training loss after  2500  iterations is :  1.5570060440454268  | validation loss is :  1.173765369103157\n",
      "Training loss after  3000  iterations is :  1.5561531895643037  | validation loss is :  1.1723818276407583\n",
      "Training loss after  3500  iterations is :  1.5555948707321103  | validation loss is :  1.1714340304791235\n",
      "Training loss after  4000  iterations is :  1.555229256219717  | validation loss is :  1.170779551115527\n",
      "Training loss after  4500  iterations is :  1.5549896240341943  | validation loss is :  1.1703234062444845\n",
      "[[0.8927692 ]\n",
      " [0.17083661]]\n",
      "-0.1648975562438999\n",
      "For fold :  5 / 9\n",
      "Training loss after  0  iterations is :  1.6792055592275283  | validation loss is :  1.2525437122957552\n",
      "Training loss after  500  iterations is :  1.5724424369618633  | validation loss is :  1.1389537837195653\n",
      "Training loss after  1000  iterations is :  1.5675477815170544  | validation loss is :  1.1342871076096714\n",
      "Training loss after  1500  iterations is :  1.5643299889169775  | validation loss is :  1.1314806715213583\n",
      "Training loss after  2000  iterations is :  1.5622199468981144  | validation loss is :  1.1298553806675995\n",
      "Training loss after  2500  iterations is :  1.5608384672364686  | validation loss is :  1.1289657693889044\n",
      "Training loss after  3000  iterations is :  1.5599347639692211  | validation loss is :  1.128524906463239\n",
      "Training loss after  3500  iterations is :  1.5593437769695617  | validation loss is :  1.1283503867890934\n",
      "Training loss after  4000  iterations is :  1.5589572137797527  | validation loss is :  1.128327820498233\n",
      "Training loss after  4500  iterations is :  1.5587041739143233  | validation loss is :  1.1283866014399961\n",
      "[[0.91540548]\n",
      " [0.1815521 ]]\n",
      "-0.17272469116517625\n",
      "For fold :  6 / 9\n",
      "Training loss after  0  iterations is :  1.6257409836891914  | validation loss is :  1.7271141586989103\n",
      "Training loss after  500  iterations is :  1.5189716369504138  | validation loss is :  1.6181621689723273\n",
      "Training loss after  1000  iterations is :  1.514068146502455  | validation loss is :  1.6136949755068726\n",
      "Training loss after  1500  iterations is :  1.51088739466972  | validation loss is :  1.6107996568869996\n",
      "Training loss after  2000  iterations is :  1.5088296219477364  | validation loss is :  1.6089277553300305\n",
      "Training loss after  2500  iterations is :  1.507500473105864  | validation loss is :  1.607719432049656\n",
      "Training loss after  3000  iterations is :  1.5066426484388853  | validation loss is :  1.6069401047428415\n",
      "Training loss after  3500  iterations is :  1.5060891110049435  | validation loss is :  1.6064375801531559\n",
      "Training loss after  4000  iterations is :  1.5057317729428636  | validation loss is :  1.6061134261322638\n",
      "Training loss after  4500  iterations is :  1.5055008380459693  | validation loss is :  1.6059041109904335\n",
      "[[0.90377199]\n",
      " [0.1778189 ]]\n",
      "-0.17396883777109834\n",
      "For fold :  7 / 9\n",
      "Training loss after  0  iterations is :  1.6180925180008847  | validation loss is :  1.7834824018341553\n",
      "Training loss after  500  iterations is :  1.5116670221258388  | validation loss is :  1.6726825282015259\n",
      "Training loss after  1000  iterations is :  1.5068781164744338  | validation loss is :  1.6676547754465245\n",
      "Training loss after  1500  iterations is :  1.5037699181942108  | validation loss is :  1.6643006452423208\n",
      "Training loss after  2000  iterations is :  1.5017577416861934  | validation loss is :  1.6620554183629255\n",
      "Training loss after  2500  iterations is :  1.5004570488810296  | validation loss is :  1.6605447609960602\n",
      "Training loss after  3000  iterations is :  1.4996168463136264  | validation loss is :  1.6595214509019274\n",
      "Training loss after  3500  iterations is :  1.499074111293158  | validation loss is :  1.6588225585587073\n",
      "Training loss after  4000  iterations is :  1.4987232956806351  | validation loss is :  1.6583406926371331\n",
      "Training loss after  4500  iterations is :  1.4984962033565283  | validation loss is :  1.6580049350151689\n",
      "[[0.89520955]\n",
      " [0.17153473]]\n",
      "-0.17362472404721502\n",
      "For fold :  8 / 9\n",
      "Training loss after  0  iterations is :  1.663021511913847  | validation loss is :  1.4148266551735422\n",
      "Training loss after  500  iterations is :  1.5563124918904547  | validation loss is :  1.3043058074608243\n",
      "Training loss after  1000  iterations is :  1.5514366461943638  | validation loss is :  1.29963305967199\n",
      "Training loss after  1500  iterations is :  1.5482328923371271  | validation loss is :  1.2967601464393559\n",
      "Training loss after  2000  iterations is :  1.5461331460262173  | validation loss is :  1.2950387896363091\n",
      "Training loss after  2500  iterations is :  1.5447590889132166  | validation loss is :  1.294043380379592\n",
      "Training loss after  3000  iterations is :  1.5438606454221713  | validation loss is :  1.293498476989284\n",
      "Training loss after  3500  iterations is :  1.5432733168962094  | validation loss is :  1.2932277691248744\n",
      "Training loss after  4000  iterations is :  1.5428892428029761  | validation loss is :  1.2931196220976249\n",
      "Training loss after  4500  iterations is :  1.5426378477921334  | validation loss is :  1.2931042044390888\n",
      "[[0.91273882]\n",
      " [0.17902537]]\n",
      "-0.17523319585965846\n",
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  1.6470572589143202  | validation loss is :  1.5467800379939498\n",
      "Training loss after  500  iterations is :  1.5422321479129268  | validation loss is :  1.4207865224874028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  1000  iterations is :  1.5376061197242206  | validation loss is :  1.4146202379630703\n",
      "Training loss after  1500  iterations is :  1.5345807771110997  | validation loss is :  1.4104131604960812\n",
      "Training loss after  2000  iterations is :  1.5326070796905327  | validation loss is :  1.4075280325735304\n",
      "Training loss after  2500  iterations is :  1.5313213566041133  | validation loss is :  1.4055356455132657\n",
      "Training loss after  3000  iterations is :  1.5304844352155775  | validation loss is :  1.4041479056020374\n",
      "Training loss after  3500  iterations is :  1.5299397535059034  | validation loss is :  1.403171708458772\n",
      "Training loss after  4000  iterations is :  1.5295851378864245  | validation loss is :  1.4024774741855226\n",
      "Training loss after  4500  iterations is :  1.5293540411737439  | validation loss is :  1.4019779783484578\n",
      "[[0.88877418]\n",
      " [0.17044556]]\n",
      "-0.16606862095091365\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  1.6695344554893319  | validation loss is :  1.3117960424440507\n",
      "Training loss after  500  iterations is :  1.5634823248569087  | validation loss is :  1.1920234513189862\n",
      "Training loss after  1000  iterations is :  1.5586578026656128  | validation loss is :  1.1868798734882844\n",
      "Training loss after  1500  iterations is :  1.5554755781311567  | validation loss is :  1.183738284657719\n",
      "Training loss after  2000  iterations is :  1.5533818536488173  | validation loss is :  1.181877940187008\n",
      "Training loss after  2500  iterations is :  1.5520064346883424  | validation loss is :  1.1808240221892727\n",
      "Training loss after  3000  iterations is :  1.551103660400483  | validation loss is :  1.1802685657058147\n",
      "Training loss after  3500  iterations is :  1.5505112941630177  | validation loss is :  1.1800142449808269\n",
      "Training loss after  4000  iterations is :  1.5501225325772476  | validation loss is :  1.179936161123079\n",
      "Training loss after  4500  iterations is :  1.5498672111835845  | validation loss is :  1.1799563426473156\n",
      "[[0.91137207]\n",
      " [0.18107658]]\n",
      "-0.17252907995504393\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  1.5024806297584101  | validation loss is :  2.548103917172718\n",
      "Training loss after  500  iterations is :  1.3892997214533993  | validation loss is :  2.458579724168878\n",
      "Training loss after  1000  iterations is :  1.3843091195149964  | validation loss is :  2.4537500607987224\n",
      "Training loss after  1500  iterations is :  1.381189125131822  | validation loss is :  2.4504185764095587\n",
      "Training loss after  2000  iterations is :  1.3792445130446522  | validation loss is :  2.4480934918894035\n",
      "Training loss after  2500  iterations is :  1.378034488595726  | validation loss is :  2.44645004612175\n",
      "Training loss after  3000  iterations is :  1.377282021403527  | validation loss is :  2.445273132330941\n",
      "Training loss after  3500  iterations is :  1.376813957791832  | validation loss is :  2.444419435255744\n",
      "Training loss after  4000  iterations is :  1.3765224404974523  | validation loss is :  2.4437926256032156\n",
      "Training loss after  4500  iterations is :  1.376340428145583  | validation loss is :  2.443327241357956\n",
      "[[0.88404069]\n",
      " [0.1665366 ]]\n",
      "-0.17208921383744508\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  1.6275176875700899  | validation loss is :  1.7229938420588136\n",
      "Training loss after  500  iterations is :  1.5197161077327106  | validation loss is :  1.6221494956176106\n",
      "Training loss after  1000  iterations is :  1.5147437056768467  | validation loss is :  1.6181023548246394\n",
      "Training loss after  1500  iterations is :  1.5115078762787968  | validation loss is :  1.6155934194067316\n",
      "Training loss after  2000  iterations is :  1.5094078219166704  | validation loss is :  1.6140651455810333\n",
      "Training loss after  2500  iterations is :  1.508047118230372  | validation loss is :  1.613155122726429\n",
      "Training loss after  3000  iterations is :  1.5071662311862168  | validation loss is :  1.6126302654573876\n",
      "Training loss after  3500  iterations is :  1.506596110010861  | validation loss is :  1.612342019262834\n",
      "Training loss after  4000  iterations is :  1.5062270044819188  | validation loss is :  1.612196523119272\n",
      "Training loss after  4500  iterations is :  1.5059878154208797  | validation loss is :  1.6121350252062843\n",
      "[[0.91164514]\n",
      " [0.18149336]]\n",
      "-0.17637411481959925\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  1.685869600600222  | validation loss is :  1.1080154082066997\n",
      "Training loss after  500  iterations is :  1.5793971192873215  | validation loss is :  0.9849076772007901\n",
      "Training loss after  1000  iterations is :  1.5745467730653737  | validation loss is :  0.9796685578299145\n",
      "Training loss after  1500  iterations is :  1.5713602536220648  | validation loss is :  0.976466709836634\n",
      "Training loss after  2000  iterations is :  1.5692720334123667  | validation loss is :  0.9745685201543887\n",
      "Training loss after  2500  iterations is :  1.5679056484395033  | validation loss is :  0.9734895968297513\n",
      "Training loss after  3000  iterations is :  1.567012312134711  | validation loss is :  0.972916371841659\n",
      "Training loss after  3500  iterations is :  1.5664283992980914  | validation loss is :  0.972648397354329\n",
      "Training loss after  4000  iterations is :  1.566046631952008  | validation loss is :  0.9725591108977667\n",
      "Training loss after  4500  iterations is :  1.5657968193914562  | validation loss is :  0.9725696914058513\n",
      "[[0.91202166]\n",
      " [0.17847741]]\n",
      "-0.1712710160329882\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  1.6470579630181719  | validation loss is :  1.5467632852774293\n",
      "Training loss after  500  iterations is :  1.542239583479968  | validation loss is :  1.4209346554846751\n",
      "Training loss after  1000  iterations is :  1.537639146105044  | validation loss is :  1.414606981419149\n",
      "Training loss after  1500  iterations is :  1.53462935936589  | validation loss is :  1.4102701627304222\n",
      "Training loss after  2000  iterations is :  1.5326650113571205  | validation loss is :  1.4072810830522948\n",
      "Training loss after  2500  iterations is :  1.5313848479839116  | validation loss is :  1.405205551981838\n",
      "Training loss after  3000  iterations is :  1.530551197689239  | validation loss is :  1.40375135690453\n",
      "Training loss after  3500  iterations is :  1.5300084192318781  | validation loss is :  1.4027220436643233\n",
      "Training loss after  4000  iterations is :  1.529654897996973  | validation loss is :  1.4019853360830832\n",
      "Training loss after  4500  iterations is :  1.5294244229355947  | validation loss is :  1.401451852896762\n",
      "[[0.88711716]\n",
      " [0.17012345]]\n",
      "-0.16468505355952356\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  1.687170952537904  | validation loss is :  1.0900258590644005\n",
      "Training loss after  500  iterations is :  1.5800311050951166  | validation loss is :  0.9750647884619617\n",
      "Training loss after  1000  iterations is :  1.575109603955113  | validation loss is :  0.970479929301263\n",
      "Training loss after  1500  iterations is :  1.5718698329185827  | validation loss is :  0.9678824314669159\n",
      "Training loss after  2000  iterations is :  1.569742492066682  | validation loss is :  0.9665234292580707\n",
      "Training loss after  2500  iterations is :  1.5683477585059977  | validation loss is :  0.965913979950048\n",
      "Training loss after  3000  iterations is :  1.5674340884351168  | validation loss is :  0.9657425659425314\n",
      "Training loss after  3500  iterations is :  1.5668357009502223  | validation loss is :  0.96581423870401\n",
      "Training loss after  4000  iterations is :  1.566443685684405  | validation loss is :  0.9660094250359357\n",
      "Training loss after  4500  iterations is :  1.5661866430715865  | validation loss is :  0.9662566286379078\n",
      "[[0.91896316]\n",
      " [0.18074773]]\n",
      "-0.17563471354327367\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  1.5879701485009319  | validation loss is :  2.0282265780885114\n",
      "Training loss after  500  iterations is :  1.4794828746230129  | validation loss is :  1.927726387368492\n",
      "Training loss after  1000  iterations is :  1.474465326796754  | validation loss is :  1.9236610905619584\n",
      "Training loss after  1500  iterations is :  1.471252844935456  | validation loss is :  1.9209856956648432\n",
      "Training loss after  2000  iterations is :  1.4692018975960732  | validation loss is :  1.9192180166340196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after  2500  iterations is :  1.4678946867382627  | validation loss is :  1.9180432669031444\n",
      "Training loss after  3000  iterations is :  1.4670621802640849  | validation loss is :  1.917256589924798\n",
      "Training loss after  3500  iterations is :  1.4665320470200505  | validation loss is :  1.9167248921431301\n",
      "Training loss after  4000  iterations is :  1.4661942674849255  | validation loss is :  1.9163616525816676\n",
      "Training loss after  4500  iterations is :  1.465978752680908  | validation loss is :  1.9161104919713534\n",
      "[[0.90236981]\n",
      " [0.17783958]]\n",
      "-0.17512763103977397\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  1.6533529895924597  | validation loss is :  1.484959705228396\n",
      "Training loss after  500  iterations is :  1.548105410191681  | validation loss is :  1.3612936146693388\n",
      "Training loss after  1000  iterations is :  1.5433671559435058  | validation loss is :  1.355754201034567\n",
      "Training loss after  1500  iterations is :  1.540253763259584  | validation loss is :  1.3521567160116912\n",
      "Training loss after  2000  iterations is :  1.5382130286995643  | validation loss is :  1.3498342345309877\n",
      "Training loss after  2500  iterations is :  1.5368773409804428  | validation loss is :  1.3483436423591475\n",
      "Training loss after  3000  iterations is :  1.5360037463958165  | validation loss is :  1.3473929801083637\n",
      "Training loss after  3500  iterations is :  1.5354324418241625  | validation loss is :  1.346791113695088\n",
      "Training loss after  4000  iterations is :  1.5350586447778614  | validation loss is :  1.346413521082451\n",
      "Training loss after  4500  iterations is :  1.534813790069102  | validation loss is :  1.346179400354365\n",
      "[[0.90080488]\n",
      " [0.17359514]]\n",
      "-0.1741121207216308\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  1.6562967830960804  | validation loss is :  1.4548657706237422\n",
      "Training loss after  500  iterations is :  1.5495334066862438  | validation loss is :  1.3457947170792948\n",
      "Training loss after  1000  iterations is :  1.5447052552564298  | validation loss is :  1.340870197462414\n",
      "Training loss after  1500  iterations is :  1.5415414742848805  | validation loss is :  1.3377450124483439\n",
      "Training loss after  2000  iterations is :  1.5394735597289593  | validation loss is :  1.3357860413923306\n",
      "Training loss after  2500  iterations is :  1.5381239961464044  | validation loss is :  1.3345755354606579\n",
      "Training loss after  3000  iterations is :  1.537243945365073  | validation loss is :  1.333841107876959\n",
      "Training loss after  3500  iterations is :  1.5366701811009267  | validation loss is :  1.3334065896448475\n",
      "Training loss after  4000  iterations is :  1.536295973727163  | validation loss is :  1.3331588472411608\n",
      "Training loss after  4500  iterations is :  1.5360516796921746  | validation loss is :  1.3330257321445587\n",
      "[[0.90715101]\n",
      " [0.17654128]]\n",
      "-0.17255402601825315\n",
      "{2: (1.5141941588477252, 1.5142285401432414), 3: (1.5118390563271598, 1.496362409969662), 4: (1.5156101249895972, 1.5076899028048625), 5: (1.5142353697811832, 1.4852774581327772), 6: (1.514411538615004, 1.4712364826041622), 7: (1.5154532501396092, 1.475578027842183), 8: (1.515841989250715, 1.4755463684221732), 9: (1.5161058802250535, 1.4808126886756798), 10: (1.5158274512681829, 1.457188220192617)}\n"
     ]
    }
   ],
   "source": [
    "k_models_video_game = {}\n",
    "for i in range(2,11):\n",
    "    models_rmse,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y, k = i, epochs = 5000, learning_rate = 0.01, loss = \"rmse\")\n",
    "#     print(\"The average loss with k = \", i, \" is \", \" train loss = \", avg_train_loss[-1], \"\")\n",
    "    k_models_video_game[i] = (avg_train_loss[-1], avg_val_loss[-1])\n",
    "print(k_models_video_game)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average loss with k =  2  is   train loss =  1.5141941588477252  validation loss =  1.5142285401432414\n",
      "The average loss with k =  3  is   train loss =  1.5118390563271598  validation loss =  1.496362409969662\n",
      "The average loss with k =  4  is   train loss =  1.5156101249895972  validation loss =  1.5076899028048625\n",
      "The average loss with k =  5  is   train loss =  1.5142353697811832  validation loss =  1.4852774581327772\n",
      "The average loss with k =  6  is   train loss =  1.514411538615004  validation loss =  1.4712364826041622\n",
      "The average loss with k =  7  is   train loss =  1.5154532501396092  validation loss =  1.475578027842183\n",
      "The average loss with k =  8  is   train loss =  1.515841989250715  validation loss =  1.4755463684221732\n",
      "The average loss with k =  9  is   train loss =  1.5161058802250535  validation loss =  1.4808126886756798\n",
      "The average loss with k =  10  is   train loss =  1.5158274512681829  validation loss =  1.457188220192617\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,11):\n",
    "    print(\"The average loss with k = \", i, \" is \", \" train loss = \", k_models_video_game[i][0], \" validation loss = \", k_models_video_game[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  1.6470572589143202  | validation loss is :  1.5467800379939498\n",
      "Training loss after  500  iterations is :  1.5453963161879807  | validation loss is :  1.4249934885263051\n",
      "Training loss after  1000  iterations is :  1.5422326508912987  | validation loss is :  1.4207872198309144\n",
      "Training loss after  1500  iterations is :  1.5396759935720998  | validation loss is :  1.4174088630178943\n",
      "Training loss after  2000  iterations is :  1.537606842020405  | validation loss is :  1.414621221604709\n",
      "Training loss after  2500  iterations is :  1.5359336347228842  | validation loss is :  1.412318506804313\n",
      "Training loss after  3000  iterations is :  1.534581507030307  | validation loss is :  1.41041420162932\n",
      "[[0.6073486 ]\n",
      " [0.03332883]]\n",
      "-0.016572483783465247\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  1.6695344554893319  | validation loss is :  1.3117960424440507\n",
      "Training loss after  500  iterations is :  1.566762664078828  | validation loss is :  1.1955265669026778\n",
      "Training loss after  1000  iterations is :  1.5634828350660588  | validation loss is :  1.1920239516208946\n",
      "Training loss after  1500  iterations is :  1.5608215877808276  | validation loss is :  1.1891450240645627\n",
      "Training loss after  2000  iterations is :  1.558658542984828  | validation loss is :  1.1868806332555186\n",
      "Training loss after  2500  iterations is :  1.556901922284111  | validation loss is :  1.1851114774413758\n",
      "Training loss after  3000  iterations is :  1.5554763331628214  | validation loss is :  1.1837389919975847\n",
      "[[0.6184941 ]\n",
      " [0.03682132]]\n",
      "-0.018220674778714228\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  1.5024806297584101  | validation loss is :  2.548103917172718\n",
      "Training loss after  500  iterations is :  1.3928181022561383  | validation loss is :  2.461891469217625\n",
      "Training loss after  1000  iterations is :  1.3893003240470783  | validation loss is :  2.4585803152085046\n",
      "Training loss after  1500  iterations is :  1.3865146888557782  | validation loss is :  2.4559389028547307\n",
      "Training loss after  2000  iterations is :  1.3843099543024568  | validation loss is :  2.4537509107953825\n",
      "Training loss after  2500  iterations is :  1.3825668722390922  | validation loss is :  2.451933445796586\n",
      "Training loss after  3000  iterations is :  1.3811899335043814  | validation loss is :  2.4504194922120233\n",
      "[[0.62040019]\n",
      " [0.04236795]]\n",
      "-0.027786635992909547\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  1.6275176875700899  | validation loss is :  1.7229938420588136\n",
      "Training loss after  500  iterations is :  1.5231271001575437  | validation loss is :  1.625012077453791\n",
      "Training loss after  1000  iterations is :  1.51971665242961  | validation loss is :  1.6221499501271268\n",
      "Training loss after  1500  iterations is :  1.5169656267046776  | validation loss is :  1.6198894029353614\n",
      "Training loss after  2000  iterations is :  1.5147444866954662  | validation loss is :  1.6181029753214586\n",
      "Training loss after  2500  iterations is :  1.5129528244591117  | validation loss is :  1.6166965976832466\n",
      "Training loss after  3000  iterations is :  1.511508662466983  | validation loss is :  1.6155940099928172\n",
      "[[0.62350665]\n",
      " [0.04069971]]\n",
      "-0.023297139700053822\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  1.685869600600222  | validation loss is :  1.1080154082066997\n",
      "Training loss after  500  iterations is :  1.58270544552678  | validation loss is :  0.9882180303660067\n",
      "Training loss after  1000  iterations is :  1.5793976395853029  | validation loss is :  0.9849080840465579\n",
      "Training loss after  1500  iterations is :  1.5767197550681915  | validation loss is :  0.9819767767216944\n",
      "Training loss after  2000  iterations is :  1.5745475238512623  | validation loss is :  0.9796693404654947\n",
      "Training loss after  2500  iterations is :  1.5727869593618167  | validation loss is :  0.9778664495301603\n",
      "Training loss after  3000  iterations is :  1.5713610159049687  | validation loss is :  0.9764674385541231\n",
      "[[0.62093113]\n",
      " [0.03577311]]\n",
      "-0.01743931971563486\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  1.6470579630181719  | validation loss is :  1.5467632852774293\n",
      "Training loss after  500  iterations is :  1.5453854272505483  | validation loss is :  1.4252347722422913\n",
      "Training loss after  1000  iterations is :  1.542240083063012  | validation loss is :  1.4209353673475524\n",
      "Training loss after  1500  iterations is :  1.5396977866209403  | validation loss is :  1.417471971609222\n",
      "Training loss after  2000  iterations is :  1.5376398638575342  | validation loss is :  1.4146079920037147\n",
      "Training loss after  2500  iterations is :  1.535975409418462  | validation loss is :  1.4122368402700607\n",
      "Training loss after  3000  iterations is :  1.534630084993377  | validation loss is :  1.4102712375083661\n",
      "[[0.60620551]\n",
      " [0.03300206]]\n",
      "-0.015709159975244455\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  1.687170952537904  | validation loss is :  1.0900258590644005\n",
      "Training loss after  500  iterations is :  1.5833828929920513  | validation loss is :  0.9779562863075039\n",
      "Training loss after  1000  iterations is :  1.5800316289654956  | validation loss is :  0.9750651025835567\n",
      "Training loss after  1500  iterations is :  1.5773156911932429  | validation loss is :  0.9724658023434017\n",
      "Training loss after  2000  iterations is :  1.5751103622160842  | validation loss is :  0.9704805868307848\n",
      "Training loss after  2500  iterations is :  1.573321176451289  | validation loss is :  0.9689873299865212\n",
      "Training loss after  3000  iterations is :  1.571870604587238  | validation loss is :  0.9678829870120371\n",
      "[[0.62457981]\n",
      " [0.03677095]]\n",
      "-0.019272541453954562\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  1.5879701485009319  | validation loss is :  2.0282265780885114\n",
      "Training loss after  500  iterations is :  1.4829633349721794  | validation loss is :  1.9306156708682698\n",
      "Training loss after  1000  iterations is :  1.4794834496091551  | validation loss is :  1.9277268866509278\n",
      "Training loss after  1500  iterations is :  1.4766974229183698  | validation loss is :  1.9254816848584746\n",
      "Training loss after  2000  iterations is :  1.4744661377830548  | validation loss is :  1.9236617566110306\n",
      "Training loss after  2500  iterations is :  1.472680897539142  | validation loss is :  1.9221852532623478\n",
      "Training loss after  3000  iterations is :  1.4712536479736227  | validation loss is :  1.920986376035867\n",
      "[[0.62333091]\n",
      " [0.04299904]]\n",
      "-0.02582443217809051\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  1.6533529895924597  | validation loss is :  1.484959705228396\n",
      "Training loss after  500  iterations is :  1.551335348600034  | validation loss is :  1.3651450177483082\n",
      "Training loss after  1000  iterations is :  1.5481059174843514  | validation loss is :  1.3612942306620919\n",
      "Training loss after  1500  iterations is :  1.5454900043736364  | validation loss is :  1.3582288762576642\n",
      "Training loss after  2000  iterations is :  1.5433678886518511  | validation loss is :  1.3557550533938991\n",
      "Training loss after  2500  iterations is :  1.541647799628068  | validation loss is :  1.353761559567059\n",
      "Training loss after  3000  iterations is :  1.5402545074315093  | validation loss is :  1.3521575704633804\n",
      "[[0.61301429]\n",
      " [0.03475998]]\n",
      "-0.01960925810215407\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  1.6562967830960804  | validation loss is :  1.4548657706237422\n",
      "Training loss after  500  iterations is :  1.5528310234479286  | validation loss is :  1.3491050565971143\n",
      "Training loss after  1000  iterations is :  1.5495339268534922  | validation loss is :  1.3457952084487417\n",
      "Training loss after  1500  iterations is :  1.5468667752783234  | validation loss is :  1.3430580192216357\n",
      "Training loss after  2000  iterations is :  1.5447060051334576  | validation loss is :  1.3408709504022462\n",
      "Training loss after  2500  iterations is :  1.542956986272597  | validation loss is :  1.3391290388015153\n",
      "Training loss after  3000  iterations is :  1.541542234024653  | validation loss is :  1.3377457475651058\n",
      "[[0.61843899]\n",
      " [0.03598989]]\n",
      "-0.01904924908356328\n"
     ]
    }
   ],
   "source": [
    "video_game_model_rmse,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y,k=10, epochs = 3500, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de3xU9Z3/8ddnJoHILVwSFUWNeAGVS8AgtaLiaq0X1ruLdBVprWxd223tQ37Ctj9x8ddfty2rrtVKvRVtrWhVlCoqghe8VDAgUBBQVLpGVCKU+8Uk89k/zkmYhJkkQGYm4byfj0cec873fM85nzkM8845Z/Idc3dERCS6YrkuQEREcktBICIScQoCEZGIUxCIiEScgkBEJOLycl3AnioqKvKSkpJclyEi0qYsWLDgS3cvTrWszQVBSUkJ5eXluS5DRKRNMbO/pVumS0MiIhGnIBARibiMBYGZPWhma81saZrl48xsUfiz1MxqzKx7puoREZHUMnmPYCpwF/BwqoXu/ivgVwBm9o/ADe6+PoP1iOwXqqqqqKioYMeOHbkuRVqhgoICevXqRX5+frPXyVgQuPtcMytpZvdRwKOZqkVkf1JRUUHnzp0pKSnBzHJdjrQi7s66deuoqKjgyCOPbPZ6Ob9HYGYdgHOAJxvpM9bMys2svLKyMnvFibRCO3bsoEePHgoB2Y2Z0aNHjz0+W8x5EAD/CLzZ2GUhd7/X3cvcvay4OOXHYEUiRSEg6ezNa6M1BMEVZOGy0MrPN/PzmcvZ9lV1pnclItKm5DQIzKwQOB14JtP7+mT9Nn479yOWrdmU6V2J7Pfi8TilpaUMHDiQwYMH89Zbb+3Vdu644w62bdu2W/vFF19MaWkpRx99NIWFhZSWllJaWrpH+7n77rt55JFHGu0zb948brjhhj2uO5Wf/vSn3HHHHS2yrWzL2M1iM3sUGA4UmVkFMBHIB3D3KWG3i4FZ7r41U3XUGnBYIQCLP9nAkBJ9SlVkXxxwwAEsWrQIgBdffJEJEybw2muv7fF27rjjDq688ko6dOhQr3369OkAvPrqq0yePJlnn3025frV1dXk5aV+G7v++uub3P/QoUMZOnToHla9/8nYGYG7j3L3nu6e7+693P0Bd5+SFAK4+1R3vyJTNSQ7sHMBPQsLWFKxMRu7E4mMTZs20a1bt7r5X/3qVwwZMoQBAwYwceJEALZu3cr555/PwIED6devH4899hh33nkna9as4YwzzuCMM85o9v569erFrbfeyimnnML06dOZMmUKQ4YMYeDAgVx++eVs374dqP8b+rBhwxg/fjwnnXQSffr0qTuzmD17NhdddFFd/2uuuYbTTz+d3r17c/fdd9ftc+LEifTt25dvfOMbjBw5co9+8//lL39Jv3796NevH7/+9a8B2Lx5M+eee27d8XjiiScAGDduHMcffzwDBgzgpptuavY+9lWbG2toXwzoVciSig25LkOkxfzHn5fxXgtf7jz+kC5M/McTGu2zfft2SktL2bFjB5999hkvv/wyALNmzeKDDz5g/vz5uDsXXHABc+fOpbKykkMOOYTnnnsOgI0bN1JYWMhtt93GK6+8QlFR0R7V2LFjR958800A1q1bx/e+9z0Axo8fz9SpU7nuuut2W8fdmT9/PjNmzGDSpEm88MILu/V5//33mTNnDhs2bOC4447je9/7Hu+88w7PPvssixcvZufOnZSWlnLyySc3q8758+fzyCOPMH/+fGpqajjppJM4/fTTWb58OSUlJTz//PN1x+OLL75g5syZLFu2DDNjw4bsvVe1hpvFWTOgV1dWr9vGxm1VuS5FpE2rvTS0YsUKXnjhBUaPHo27M2vWLGbNmsWgQYMYPHgwK1as4IMPPqB///7Mnj2bm266iddff53CwsJ92v/IkSPrppcsWcKpp55K//79mTZtGsuWLUu5ziWXXALAiSeeyOrVq1P2GTFiBO3atePAAw+ke/fuVFZW8sYbb3DRRRfRvn17unTpwogRI5pd5+uvv86ll15Khw4d6Ny5MxdddBFvvPEGAwYM4IUXXmD8+PG8+eabFBYW0r17d2KxGNdeey3Tp0+nY8eOzT8g+yhyZwQAiys2cNqx+hiqtH1N/eaeDSeffDJffvkllZWVuDsTJkzgX/7lX3brt2DBAmbOnMmECRM4++yzufnmm/d6n8lvkqNHj+b555+nX79+3H///bz99tsp12nfvj0Q3Oiurk796cHaPsn93H2v60y37nHHHUd5eTkzZ85k3LhxjBgxgn//93+nvLycl156iWnTpnHPPfcwa9asvd73nojUGcGgw7sRjxnzPl6X61JE9hsrVqygpqaGHj168M1vfpMHH3yQLVu2APDpp5+ydu1a1qxZQ4cOHbjyyiu58cYbWbhwIQCdO3dm8+bN+7T/rVu3cvDBB1NVVcUf//jHfX4+DQ0bNowZM2awc+dONm/ezMyZM5u97mmnncb06dPZvn07W7Zs4ZlnnuHUU0/l008/pVOnTlx11VX8+Mc/ZuHChWzevJlNmzYxYsQIbr/9dt59990Wfy7pROqMoFP7PPofWsjbH2lII5F9UXuPAILfeh966CHi8Thnn302y5cvr7uG3qlTJ/7whz+watUqxo0bRywWIz8/n3vuuQeAsWPHcu6559KzZ09eeeWVvapl0qRJnHTSSRx++OH069evxcdgOvnkkznnnHMYMGAAJSUlDBkyJO2lrVtuuYXJkycDkJeXx+rVqxk1ahRDhgwB4LrrrqN///7MnDmT8ePHE4vFaNeuHVOmTGHjxo1ccskl7Ny5k0QiwW233daiz6Mxti+nPblQVlbm+/LFNL94YQX3zf2IxRPPpmP7SOWg7CeWL1/Occcdl+syImXLli106tSJrVu3MmzYMB566CEGDBiQ67LSSvUaMbMF7l6Wqn+kLg0BnNy7B9UJp/xvf891KSLSRlxzzTWUlpZy4oknMmrUqFYdAnsjcr8Sl5V0o11ejNdWVnK6bhiLSDM89thjuS4hoyJ3RtChXR6nHl3Ei8s+36dPA4iI7C8iFwQAZ59wEJ9u2M57n2ncIRGRSAbBmccdhBm8sPTzXJciIpJzkQyCok7tGXZ0EU8t/JSahC4PiUi0RTIIAEYOOYxPN2znzVVf5roUkTYn08NQ33LLLUyYMKFe26JFi5r82Ozw4cOp/Xj5eeedl3K8nuTP+qfz9NNP895779XN33zzzcyePbvRdZrj1Vdf3aMhKrIlskHwjeMPoluHfP7w9t9yXYpIm1M71tDixYv5+c9/vtubdnOlC4JRo0bt9kmdadOm8a1vfavZ2545cyZdu3bdq7oaBsGkSZM466yz9mpbbUFkg6B9Xpwrv3YEs977gve/2Lc/cReJskwMQ92nTx+6du3KvHnz6toef/xxrrgiGLX+uuuuo6ysjBNOOKFuHw2VlJTw5ZfBGf/PfvYz+vTpw1lnncXKlSvr+tx33311Q1hfeumlbNu2jbfeeosZM2Ywbtw4SktL+fDDDxkzZkzdUNFz5sxh0KBB9O/fn+985zvs3Lmzbn8TJ05k8ODB9O/fnxUrVjT7GKbb5vjx4+uGpb7xxhsB+NOf/kS/fv0YOHAgp512WrP30ZjI/R1Bsm+fciQPvPExd728ijtHDcp1OSJ77vnx8PlfW3abB/eHc/+z0S7ZGIZ61KhRTJs2jaFDh/L222/To0cPjjnmGCB4Y+/evTs1NTWceeaZLFmyJO0feS1YsIBp06bx7rvvUl1dzeDBgznxxBOBYETSa6+9Fgi+j+CBBx7gBz/4ARdccAEjRozgsssuq7etHTt2MGbMGObMmcOxxx7L6NGjueeee/jRj34EQFFREQsXLuQ3v/kNkydP5v7772/ycKfb5ujRo5k+fTorVqyoNyz1pEmTePHFFzn00ENbbKjqyJ4RAHTv2I5vn1LCjMVreGe1xh8Saa5sDEN9xRVX8MQTT5BIJJg2bRqjRo2qW/b4448zePBgBg0axLJly+pdxmno9ddf5+KLL6ZDhw506dKFCy64oG7Z0qVL64awfuSRR9IOYV1r5cqVHHnkkRx77LEAXH311cydO7dueXOGum7uNrt06UJBQQHf/e53eeqpp+q+xe2UU05hzJgx3HfffdTU1DRrH02J9BkBwPVnHM3T767hJ9P/yozvD6MgP57rkkSar4nf3LMhU8NQH3bYYZSUlPDaa6/x5JNP8pe//AWAjz/+mMmTJ/POO+/QrVs3xowZ0+RAc2aWsn3MmDE8/fTTDBw4kKlTp/Lqq682up2m/gi1OUNdN3ebeXl5zJ8/nzlz5jBt2jTuuusuXn75ZaZMmcK8efN47rnnKC0tZdGiRfTo0aNZ+0onY2cEZvagma01s6WN9BluZovMbJmZ7fkXnraADu3y+NnF/Xj/iy1MfKbx3wZEZHeZHIZ61KhR3HDDDRx11FH06tULCO5JdOzYkcLCQr744ou6b/lKJ3ko6M2bN/PnP/+5btnmzZvp2bMnVVVV9b7oPl1dffv2ZfXq1axatQqA3//+95x++unNPFKppdvmli1b2LhxI+eddx533HFH3XdEf/jhhwwdOpRJkyZRVFTEJ598sk/7h8yeEUwF7gIeTrXQzLoCvwHOcff/MbMDM1hLo4b3OZDvn3E0d72yikO7HcC/nXlMrkoRaROyNQz15Zdfzg9/+MO67/oFGDhwIIMGDeKEE06gd+/enHLKKY3WOnjwYEaOHElpaSlHHHEEp556at2yW2+9laFDh3LEEUfQv3//ujf/K664gmuvvZY777yz7iYxQEFBAb/73e+4/PLLqa6uZsiQIXVfk9lcc+bMqQs1CG7+ptrm+vXrufDCC9mxYwfuzu233w4E32v8wQcf4O6ceeaZDBw4cI/2n0pGh6E2sxLgWXfvl2LZvwKHuPtP92Sb+zoMdTqJhDPuiSU8ubCC7w47kpvO7Ut+PNK3UKSV0jDU0pS2NAz1sUA3M3vVzBaY2eh0Hc1srJmVm1l5ZWVlRoqJxYxfXjaAMV8v4f43PuaffvsX/lqxMSP7EhFpTXIZBHnAicD5wDeB/2tmx6bq6O73unuZu5cVF2du6Oh4zLjlghO4c9QgPlm/jQvufoNrpr7Dyyu+YGd1y9ydFxFpbXL5qaEK4Et33wpsNbO5wEDg/RzWBMAFAw9heJ9i7n/9Y/4472/MmbqWDu3inNy7BwN6daXfoV0oKerIIYUHcEA7fcpIss/d034SRqJtby735zIIngHuMrM8oB0wFLg9h/XU06Ugnx9/41iuP+Mo3lz1JS+vWMtbH67j5ZVrST7OPTq2o6hTewoPyKfLAfl07ZBP4QH5HJAfpyA/RkF+nPb58V3zefGwLUZ+PEZ+3GgXD6bb5YWP8Rj5eUZ+PEZezPQfXuopKChg3bp19OjRQ68NqcfdWbduHQUFBXu0XsaCwMweBYYDRWZWAUwE8gHcfYq7LzezF4AlQAK4393TftQ0V9rnxfmHvgfxD30PAmDrzmpWfL6JT9Zv59MN26n4+3bWb93Jxu1VVPx9G++tqWLj9iq2V9XQUgObtgsDIz85KOIWBkkQIMnhUdsnL27kxYIwyYvvCpa8cP28WCxsD6bz48GyvFjYt0F7frhuXtzIj9Uu332dumXhdCymN6uW1KtXLyoqKsjU/TJp2woKCup9Kqk5Ivfl9dni7lTVODuqa9hRVcPOqgTbq4LpHVUJdlbXUFWT4Ktqp6omUffzVY1TVd1gviZR1/ZVjfNV0vJ063xVXUN1wqkO169OBI81tW2JBNn6p48Z9YKkfgg1DKdd0/EwxOIxq3uM15tvsDy+qz1uQRAlr7vbOnEjZlZvPtU+82IxYjHIi8Xqt8d3LW9Yo87kpLVp7FNDkf/L4kwxM9rlGe3yYnQpyM91OSnVJHaFRHVNgqoapzqR2C08qsP2qppdIVJdE64Trhv0Se5Xf/1061Sl2feOqgQ1iZoguBJOTSIRPu76qZ2vrg24pPbWIGYNwiNuxM2IxYLHeMyIxdi9rW6ZETfqtcXDgGnYvvv6DZYnrV+7PGbp2+u3JdWdvNzCWlK0x1Ksb0k1xyz4PxKzXXXEkmpqarnVtYePOuvcJwqCCAv+A++fN7sT9YJh96BINAiY6pr64VLTyLo1YWAlPDmMfLfQSuy2Xu32gvpq3OseaxLB9oL9Uje9q602IJ0aD9dPXu6OO3V9U62fqF2etN82dkGgUcmhkSooaqfrBwzhsuTl9bfT3PBquE9rELjNqanhvo1dNZrB13r34Iw+Lf+3twoC2S/FYka7ut8S98+wawmeFCSJBLtCKVVQ1QZUivaGYbZ7wAXrugeBlAgfg/kmlieC8KvtW7vcPSlQm1juddtM3i7husnLk+oIl6evuTZcE2mfU8qawnWTa6pJACSvv6t/8nx+LKYgEJGWZeG9FL0RRJvGUBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoCEZGIUxCIiERcxoLAzB40s7VmlvJ7iM1suJltNLNF4c/NmapFRETSy+Qw5FOBu4CHG+nzuruPyGANIiLShIydEbj7XGB9prYvIiItI9f3CE42s8Vm9ryZnZDjWkREIimX31C3EDjC3beY2XnA08AxqTqa2VhgLMDhhx+evQpFRCIgZ2cE7r7J3beE0zOBfDMrStP3Xncvc/ey4uLirNYpIrK/y1kQmNnBZmbh9ElhLetyVY+ISFRl7NKQmT0KDAeKzKwCmAjkA7j7FOAy4Dozqwa2A1e4u2eqHhERSS1jQeDuo5pYfhfBx0tFRCSHcv2pIRERyTEFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoCEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCIuY0FgZg+a2VozW9pEvyFmVmNml2WqFhERSS+TZwRTgXMa62BmceAXwIsZrENERBqRsSBw97nA+ia6/QB4ElibqTpERKRxObtHYGaHAhcDU5rRd6yZlZtZeWVlZeaLExGJkFzeLL4DuMnda5rq6O73unuZu5cVFxdnoTQRkejIy+G+y4BpZgZQBJxnZtXu/nQOaxIRiZycBYG7H1k7bWZTgWcVAiIi2ZexIDCzR4HhQJGZVQATgXwAd2/yvoCIiGRHxoLA3UftQd8xmapDREQap78sFhGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiIhHXrCAwsx+aWRcLPGBmC83s7EwXJyIimdfcM4LvuPsm4GygGPg28J8Zq0pERLKmuUFg4eN5wO/cfXFSm4iItGHNDYIFZjaLIAheNLPOQCJzZYmISLY0d9C5a4BS4CN332Zm3QkuD4mISBvX3DOCk4GV7r7BzK4EfgpszFxZIiKSLc0NgnuAbWY2EPg/wN+AhzNWlYiIZE1zg6Da3R24EPhvd/9voHPmyhIRkWxp7j2CzWY2AbgKONXM4oTfNiYiIm1bc88IRgI7Cf6e4HPgUOBXGatKRESypllBEL75PwIUmtkIYIe7N3qPwMweNLO1ZrY0zfILzWyJmS0ys3IzG7bH1YuIyD5r7hAT/wTMBy4H/gmYZ2aXNbHaVOCcRpbPAQa6eynwHeD+5tQiIiItq7n3CH4CDHH3tQBmVgzMBp5It4K7zzWzkkaWb0ma7Qh4M2sREZEW1Nx7BLHaEAit24N10zKzi81sBfAcwVmBiIhkWXPfzF8wsxfNbIyZjSF44565rzt39+nu3he4CLg1XT8zGxveRyivrKzc192KiEiS5t4sHgfcCwwABgL3uvtNLVWEu88FjjKzojTL73X3MncvKy4ubqndiogIzb9HgLs/CTzZUjs2s6OBD93dzWww0I7gkpOIiGRRo0FgZptJfRPXAHf3Lo2s+ygwHCgyswpgIuEfobn7FOBSYLSZVQHbgZHhXy+LiEgWNRoE7r7Xw0i4+6gmlv8C+MXebl9ERFqGvrNYRCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoCEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJuIwFgZk9aGZrzWxpmuX/bGZLwp+3zGxgpmoREZH0MnlGMBU4p5HlHwOnu/sA4Fbg3gzWIiIiaeRlasPuPtfMShpZ/lbS7NtAr0zVIiIi6bWWewTXAM+nW2hmY82s3MzKKysrs1iWiMj+L+dBYGZnEATBTen6uPu97l7m7mXFxcXZK05EJAIydmmoOcxsAHA/cK67r8tlLSIiUZWzMwIzOxx4CrjK3d/PVR0iIlGXsTMCM3sUGA4UmVkFMBHIB3D3KcDNQA/gN2YGUO3uZZmqR0REUsvkp4ZGNbH8u8B3M7V/ERFpnpzfLBYRkdxSEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoCEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEXMaCwMweNLO1ZrY0zfK+ZvYXM9tpZjdmqg4REWlcJs8IpgLnNLJ8PfBvwOQM1iAiIk3IWBC4+1yCN/t0y9e6+ztAVaZqEBGRprWJewRmNtbMys2svLKyMtfliIjsV9pEELj7ve5e5u5lxcXFuS5HRGS/0iaCoEV8uhCe+T5s/3uuKxERaVWiEwRbK+Hd30PlylxXIiLSquRlasNm9igwHCgyswpgIpAP4O5TzOxgoBzoAiTM7EfA8e6+KSMFFR0bPFauhMO/lpFdiIi0RRkLAncf1cTyz4Femdr/broeDnkF8OX7WduliEhbEJ1LQ7E49DhGl4ZERBqIThAAFB8LXyoIRESSRSwI+sKGT+CrbbmuRESk1YhWEBQdCzis+yDXlYiItBrRCoIDjwsev1iW2zpERFqRaAVBj6MhvyN8tjjXlYiItBrRCoJYHA7uD2sW5boSEZFWI1pBAHBIKXy+BBI1ua5ERKRViF4Q9CyFqm3wpW4Yi4hAFIPgkEHB45p3c1uHiEgrEb0gKDoG2hfC//wl15WIiLQK0QuCWByO+DqsfiPXlYiItArRCwKAkmGw/kPYtCbXlYiI5Fx0gwB0ViAiQlSD4OD+cEB3+OClXFciIpJz0QyCWBz6nAvvvwg1VbmuRkQkp6IZBAB9z4edG3V5SEQiL7pB0PsMyO8AS5/MdSUiIjkV3SBo1wFOuASWPgU7N+e6GhGRnMlYEJjZg2a21syWplluZnanma0ysyVmNjhTtaR14tVQtRX++kTWdy0i0lpk8oxgKnBOI8vPBY4Jf8YC92SwltR6DYGDB8Bbd0JNddZ3LyLSGmQsCNx9LrC+kS4XAg974G2gq5n1zFQ9KZnB6TfB+o9gyWNZ3bWISGuRy3sEhwKfJM1XhG27MbOxZlZuZuWVlZUtW0Xf8+GQwTB7ImxrLLdERPZPuQwCS9HmqTq6+73uXubuZcXFxS1chcEFd8L2v8OffwiJRMtuX0SklctlEFQAhyXN9wJyM/jPwf3hzImwfAbMvhk8ZR6JiOyXchkEM4DR4aeHvgZsdPfPclbN138AZdfAW7+Gp8YGZwgiIhGQl6kNm9mjwHCgyMwqgIlAPoC7TwFmAucBq4BtwLczVUuzmMH5/wWde8Kr/x8+egVOGgv9LoXuvYPlIiL7IfM2dhmkrKzMy8vLM7uTNYtgziT4cE4w3/mQ4AttCg+D9p2gXcegPVEDXhM81lRBoip4rKmCmq8gUR081mtL6uM1gIHFgqCxWPCDhfO2az4Wh1g+xPMglhdO54fTeSmm84N1aqd3Wy+eNJ3cN68Z8+n2mdRXwSnSqpjZAncvS7UsY2cEbdohpXDVU7D+Y1g1Gz6ZH3x/wYcvw1db4astQb9YPHjzs3jwRhtvt+vNNZ4fzoft8fzgr5lj+bvmLQZ4cE/CE/Wn67UlgrBJVAcB8tW2MFCqg7bdpqvC6eqkwMmyWF4LhMpezsfywn+bePBvU/cYtlssabq2PdagT7zBNhpbN1V7fFfAi7RyCoLGdD8STro2+GnLEokgDOrOWtKERm1wJGqCZQ3nU/Zt5rpp5xusW70jCNrd+ibVnGq+taoLlPAXhlgsabq2PdagT1IA1S6rPVtM/qnXHt91Brlbe23/WJr2pHV3a481sawFa2p4dlxv3ppY3mAemt/XYnvefz8LeAVBFMRiQCz4zXl/5L7rjKk2GGrDL1Fd/xJe3XRyeyKYr+tTXf8sbLd1m9PeYJtp959oUEuD2j2xq5ba5+nhWV7DZZ5I0V57dpmqvcFPvfYcnEW2OZYmhFKFSJq+TQUeVv9x8NXw9e+3+DNREEjbZxZemssDCnJdzf7DPU1wJAVMw/DYl3Da7TJpIvjLonrzDZc3vKyaYnmj6za1fB9raZFt+67HTgdl5J9aQSAiqZntulQl+7XoDkMtIiKAgkBEJPIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiGtzo4+aWSXwt71cvQj4sgXLybS2VG9bqhXaVr1tqVZoW/W2pVph3+o9wt1TfsVjmwuCfWFm5emGYW2N2lK9balWaFv1tqVaoW3V25ZqhczVq0tDIiIRpyAQEYm4qAXBvbkuYA+1pXrbUq3QtuptS7VC26q3LdUKGao3UvcIRERkd1E7IxARkQYUBCIiEReZIDCzc8xspZmtMrPxua4HwMxWm9lfzWyRmZWHbd3N7CUz+yB87Ba2m5ndGda/xMwGZ6G+B81srZktTWrb4/rM7Oqw/wdmdnUWa73FzD4Nj+8iMzsvadmEsNaVZvbNpPasvE7M7DAze8XMlpvZMjP7Ydje6o5vI7W2yuNrZgVmNt/MFof1/kfYfqSZzQuP02Nm1i5sbx/OrwqXlzT1PLJQ61Qz+zjp2JaG7Zl5Hbj7fv8DxIEPgd5AO2AxcHwrqGs1UNSg7ZfA+HB6PPCLcPo84HnAgK8B87JQ32nAYGDp3n9jKe8AAAWfSURBVNYHdAc+Ch+7hdPdslTrLcCNKfoeH74G2gNHhq+NeDZfJ0BPYHA43Rl4P6yr1R3fRmptlcc3PEadwul8YF54zB4HrgjbpwDXhdP/CkwJp68AHmvseWSp1qnAZSn6Z+R1EJUzgpOAVe7+kbt/BUwDLsxxTelcCDwUTj8EXJTU/rAH3ga6mlnPTBbi7nOB9ftY3zeBl9x9vbv/HXgJOCdLtaZzITDN3Xe6+8fAKoLXSNZeJ+7+mbsvDKc3A8uBQ2mFx7eRWtPJ6fENj9GWcDY//HHgH4AnwvaGx7b2mD8BnGlm1sjzyEat6WTkdRCVIDgU+CRpvoLGX8jZ4sAsM1tgZmPDtoPc/TMI/gMCB4btreU57Gl9ua77++Ep9IO1l1kaqSkntYaXIgYR/DbYqo9vg1qhlR5fM4ub2SJgLcGb4ofABnevTrHvurrC5RuBHtmqt2Gt7l57bH8WHtvbzax9w1ob1LRPtUYlCCxFW2v43Owp7j4YOBe43sxOa6Rva30OtdLVl8u67wGOAkqBz4D/CttbTa1m1gl4EviRu29qrGuKtqzWnKLWVnt83b3G3UuBXgS/xR/XyL5zWm/DWs2sHzAB6AsMIbjcc1Mma41KEFQAhyXN9wLW5KiWOu6+JnxcC0wneMF+UXvJJ3xcG3ZvLc9hT+vLWd3u/kX4nywB3Meu0/pWUauZ5RO8sT7i7k+Fza3y+KaqtbUf37DGDcCrBNfTu5pZXop919UVLi8kuMyY1XqTaj0nvBzn7r4T+B0ZPrZRCYJ3gGPCTw20I7ghNCOXBZlZRzPrXDsNnA0sDeuqveN/NfBMOD0DGB1+auBrwMbaSwhZtqf1vQicbWbdwksHZ4dtGdfgHsrFBMe3ttYrwk+LHAkcA8wni6+T8Br0A8Byd78taVGrO77pam2tx9fMis2sazh9AHAWwX2NV4DLwm4Nj23tMb8MeNmDO7Dpnkema12R9MuAEdzLSD62Lf862Nu73W3th+Bu+/sE1wp/0grq6U3wiYTFwLLamgiuTc4BPggfu/uuTxfcHdb/V6AsCzU+SnDKX0XwG8c1e1Mf8B2CG22rgG9nsdbfh7UsCf8D9Uzq/5Ow1pXAudl+nQDDCE7dlwCLwp/zWuPxbaTWVnl8gQHAu2FdS4Gbk/7PzQ+P05+A9mF7QTi/Klzeu6nnkYVaXw6P7VLgD+z6ZFFGXgcaYkJEJOKicmlIRETSUBCIiEScgkBEJOIUBCIiEacgEBGJOAWBSIaZ2XAzezbXdYikoyAQEYk4BYFIyMyuDMeGX2Rmvw0HA9tiZv9lZgvNbI6ZFYd9S83s7XBQsOm263sDjjaz2RaML7/QzI4KN9/JzJ4wsxVm9kj4F6OY2X+a2Xvhdibn6KlLxCkIRAAzOw4YSTAQYClQA/wz0BFY6MHggK8BE8NVHgZucvcBBH/hWdv+CHC3uw8Evk7w184QjNj5I4Ix7nsDp5hZd4KhGU4It/P/MvssRVJTEIgEzgROBN4JhwQ+k+ANOwE8Fvb5AzDMzAqBru7+Wtj+EHBaOHbUoe4+HcDdd7j7trDPfHev8GCAtkVACbAJ2AHcb2aXALV9RbJKQSASMOAhdy8Nf/q4+y0p+jU2JkuqoYBr7UyargHyPBj7/iSCUT0vAl7Yw5pFWoSCQCQwB7jMzA6Euu8OPoLg/0jtiJXfAt5w943A383s1LD9KuA1D8borzCzi8JttDezDul2GI7vX+juMwkuG5Vm4omJNCWv6S4i+z93f8/MfkrwjXExglFMrwe2AieY2QKCb64aGa5yNTAlfKP/CPh22H4V8FszmxRu4/JGdtsZeMbMCgjOJm5o4acl0iwafVSkEWa2xd075boOkUzSpSERkYjTGYGISMTpjEBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCLufwH/t24zgnUO4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot([x for x in range(len(video_game_model_rmse[6][2]))],video_game_model_rmse[6][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(video_game_model_rmse[6][3]))], video_game_model_rmse[6][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold :  0 / 10\n",
      "Training loss after  0  iterations is :  0.5308240845351233  | validation loss is :  0.5582345900658288\n",
      "Training loss after  500  iterations is :  0.4574597569877251  | validation loss is :  0.4806604071028415\n",
      "Training loss after  1000  iterations is :  0.45714732913823797  | validation loss is :  0.48014913670514964\n",
      "Training loss after  1500  iterations is :  0.4570892760509346  | validation loss is :  0.4799591350424304\n",
      "Training loss after  2000  iterations is :  0.4570766222535795  | validation loss is :  0.4798785921776796\n",
      "Training loss after  2500  iterations is :  0.45707295484603794  | validation loss is :  0.4798495186357821\n",
      "Training loss after  3000  iterations is :  0.4570714734626665  | validation loss is :  0.4798333686841573\n",
      "[[ 0.15309965]\n",
      " [-0.02090973]]\n",
      "0.01571276666445228\n",
      "For fold :  1 / 10\n",
      "Training loss after  0  iterations is :  0.5368040140891872  | validation loss is :  0.5039736684619988\n",
      "Training loss after  500  iterations is :  0.46246583626508275  | validation loss is :  0.43485981752168873\n",
      "Training loss after  1000  iterations is :  0.4621203213909539  | validation loss is :  0.43471654659179965\n",
      "Training loss after  1500  iterations is :  0.4620456551804074  | validation loss is :  0.43473598844015354\n",
      "Training loss after  2000  iterations is :  0.46202632813339517  | validation loss is :  0.4347869600751374\n",
      "Training loss after  2500  iterations is :  0.46201967178940945  | validation loss is :  0.43482699361798455\n",
      "Training loss after  3000  iterations is :  0.4620157315462388  | validation loss is :  0.4348569242025349\n",
      "[[ 0.16253171]\n",
      " [-0.01839489]]\n",
      "0.012193792782614293\n",
      "For fold :  2 / 10\n",
      "Training loss after  0  iterations is :  0.5250249219113444  | validation loss is :  0.6104069419509276\n",
      "Training loss after  500  iterations is :  0.45122856601811595  | validation loss is :  0.5369752777061315\n",
      "Training loss after  1000  iterations is :  0.4509250690215356  | validation loss is :  0.5362577782326947\n",
      "Training loss after  1500  iterations is :  0.4508679097356612  | validation loss is :  0.5360075271312346\n",
      "Training loss after  2000  iterations is :  0.4508494107542168  | validation loss is :  0.5359047792845446\n",
      "Training loss after  2500  iterations is :  0.4508452887309597  | validation loss is :  0.535869069893265\n",
      "Training loss after  3000  iterations is :  0.4508436122459377  | validation loss is :  0.5358520135081278\n",
      "[[ 0.15271688]\n",
      " [-0.02227744]]\n",
      "0.015238253472453516\n",
      "For fold :  3 / 10\n",
      "Training loss after  0  iterations is :  0.5333169402538712  | validation loss is :  0.5358767205266307\n",
      "Training loss after  500  iterations is :  0.45999575337677456  | validation loss is :  0.45734292085964573\n",
      "Training loss after  1000  iterations is :  0.4596580672556648  | validation loss is :  0.45728884398953296\n",
      "Training loss after  1500  iterations is :  0.45959184650608664  | validation loss is :  0.45723993211171704\n",
      "Training loss after  2000  iterations is :  0.45957502730240457  | validation loss is :  0.457268157323798\n",
      "Training loss after  2500  iterations is :  0.45956987528814247  | validation loss is :  0.4572758938457923\n",
      "Training loss after  3000  iterations is :  0.45956649219510515  | validation loss is :  0.45726555627182214\n",
      "[[ 0.15845339]\n",
      " [-0.01785729]]\n",
      "0.012773974878713335\n",
      "For fold :  4 / 10\n",
      "Training loss after  0  iterations is :  0.5401555127267894  | validation loss is :  0.4742788749251945\n",
      "Training loss after  500  iterations is :  0.4664678330156551  | validation loss is :  0.39963438677492874\n",
      "Training loss after  1000  iterations is :  0.466136353178242  | validation loss is :  0.3991260640391694\n",
      "Training loss after  1500  iterations is :  0.4660710701928811  | validation loss is :  0.39896894973035085\n",
      "Training loss after  2000  iterations is :  0.4660573690025226  | validation loss is :  0.3989164666800517\n",
      "Training loss after  2500  iterations is :  0.46605398857458125  | validation loss is :  0.39888817585487296\n",
      "Training loss after  3000  iterations is :  0.4660524667672269  | validation loss is :  0.398876707351482\n",
      "[[ 0.1545707 ]\n",
      " [-0.02205558]]\n",
      "0.015524689306839076\n",
      "For fold :  5 / 10\n",
      "Training loss after  0  iterations is :  0.5310440619392569  | validation loss is :  0.5561879114302812\n",
      "Training loss after  500  iterations is :  0.4572271217402465  | validation loss is :  0.4823673780695265\n",
      "Training loss after  1000  iterations is :  0.45689222496817417  | validation loss is :  0.48218138280254874\n",
      "Training loss after  1500  iterations is :  0.45681946163170856  | validation loss is :  0.4821660640633634\n",
      "Training loss after  2000  iterations is :  0.45680320524029283  | validation loss is :  0.4821818065604143\n",
      "Training loss after  2500  iterations is :  0.4567988079319663  | validation loss is :  0.4821954319634366\n",
      "Training loss after  3000  iterations is :  0.456796228998611  | validation loss is :  0.4822087095516968\n",
      "[[ 0.15851928]\n",
      " [-0.01922797]]\n",
      "0.011952548680799521\n",
      "For fold :  6 / 10\n",
      "Training loss after  0  iterations is :  0.5417252608493387  | validation loss is :  0.4600777977259126\n",
      "Training loss after  500  iterations is :  0.4677388064305266  | validation loss is :  0.38760437822299504\n",
      "Training loss after  1000  iterations is :  0.4673916550216097  | validation loss is :  0.3875792359328853\n",
      "Training loss after  1500  iterations is :  0.4673205260902654  | validation loss is :  0.38762061107103224\n",
      "Training loss after  2000  iterations is :  0.46730423759433903  | validation loss is :  0.3876667556711327\n",
      "Training loss after  2500  iterations is :  0.46729747644270164  | validation loss is :  0.3877054040772345\n",
      "Training loss after  3000  iterations is :  0.46729449836623305  | validation loss is :  0.387722791521476\n",
      "[[ 0.16169699]\n",
      " [-0.01965603]]\n",
      "0.012593872532730714\n",
      "For fold :  7 / 10\n",
      "Training loss after  0  iterations is :  0.5278806406592675  | validation loss is :  0.5846080191502094\n",
      "Training loss after  500  iterations is :  0.45433618829728223  | validation loss is :  0.5089260525501939\n",
      "Training loss after  1000  iterations is :  0.45403314815250867  | validation loss is :  0.5082086864553774\n",
      "Training loss after  1500  iterations is :  0.45397568500936164  | validation loss is :  0.5079584911131118\n",
      "Training loss after  2000  iterations is :  0.45396074690526234  | validation loss is :  0.5078489197110013\n",
      "Training loss after  2500  iterations is :  0.4539573364475712  | validation loss is :  0.5078047752802264\n",
      "Training loss after  3000  iterations is :  0.4539561074470399  | validation loss is :  0.507787693097963\n",
      "[[ 0.15112037]\n",
      " [-0.02286299]]\n",
      "0.016113510998871344\n",
      "For fold :  8 / 10\n",
      "Training loss after  0  iterations is :  0.5323114242041602  | validation loss is :  0.5445002992220227\n",
      "Training loss after  500  iterations is :  0.45828606941763567  | validation loss is :  0.4726527969137821\n",
      "Training loss after  1000  iterations is :  0.4579193005873251  | validation loss is :  0.4726076787614507\n",
      "Training loss after  1500  iterations is :  0.4578410120816015  | validation loss is :  0.472673906490848\n",
      "Training loss after  2000  iterations is :  0.4578201056407254  | validation loss is :  0.47275760314364823\n",
      "Training loss after  2500  iterations is :  0.457812522025024  | validation loss is :  0.47280783063474213\n",
      "Training loss after  3000  iterations is :  0.45781032987580056  | validation loss is :  0.47282363602343663\n",
      "[[ 0.16262375]\n",
      " [-0.01834589]]\n",
      "0.013223898451518404\n",
      "For fold :  9 / 10\n",
      "Training loss after  0  iterations is :  0.5363255133913737  | validation loss is :  0.508001196888091\n",
      "Training loss after  500  iterations is :  0.4621441792567936  | validation loss is :  0.4373175793795152\n",
      "Training loss after  1000  iterations is :  0.46180291228431797  | validation loss is :  0.43728670720521934\n",
      "Training loss after  1500  iterations is :  0.4617297644137147  | validation loss is :  0.43734425750269784\n",
      "Training loss after  2000  iterations is :  0.46171100901200823  | validation loss is :  0.43741041863532965\n",
      "Training loss after  2500  iterations is :  0.4617029905580032  | validation loss is :  0.43745862323928236\n",
      "Training loss after  3000  iterations is :  0.46170087787508707  | validation loss is :  0.4374688348548858\n",
      "[[ 0.16200152]\n",
      " [-0.01969939]]\n",
      "0.012619126736226255\n"
     ]
    }
   ],
   "source": [
    "video_game_model_mae,avg_train_loss, avg_val_loss = k_fold_cross_validation(X,Y,k=10, epochs = 3500,learning_rate = 0.005,loss = \"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x for x in range(len(avg_val_loss))],avg_val_loss, label = \"Average Validation Loss\")\n",
    "plt.plot([x for x in range(len(avg_train_loss))], avg_train_loss, label = \"Average Training Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot([x for x in range(len(video_game_model_mae[6][2]))],video_game_model_mae[6][2], label = \"Best Training Loss\")\n",
    "plt.plot([x for x in range(len(video_game_model_mae[6][3]))], video_game_model_mae[6][3], label = \"Best Validation Loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for LR MAE Loss\n",
      "For CV number  0 the train loss =  0.4570705294598559  and the val loss =  0.47981963845536096\n",
      "For CV number  1 the train loss =  0.46201269656039756  and the val loss =  0.43487858426953513\n",
      "For CV number  2 the train loss =  0.45084234718446087  and the val loss =  0.5358408110548142\n",
      "For CV number  3 the train loss =  0.459565379625231  and the val loss =  0.457260603111768\n",
      "For CV number  4 the train loss =  0.4660512328311721  and the val loss =  0.3988677620028403\n",
      "For CV number  5 the train loss =  0.4567942765219069  and the val loss =  0.48222056223054266\n",
      "For CV number  6 the train loss =  0.4672929233511366  and the val loss =  0.3877298917992819\n",
      "For CV number  7 the train loss =  0.45395515922545576  and the val loss =  0.50777767972726\n",
      "For CV number  8 the train loss =  0.4578089983274051  and the val loss =  0.47283351046913547\n",
      "For CV number  9 the train loss =  0.4616995771966845  and the val loss =  0.4374756103735225\n",
      "Stats for LR RMSE Loss\n",
      "For CV number  0 the train loss =  1.5334913798246819  and the val loss =  1.4088400332255244\n",
      "For CV number  1 the train loss =  1.5543221012307915  and the val loss =  1.182684831327262\n",
      "For CV number  2 the train loss =  1.3801048496440143  and the val loss =  2.4491568742256598\n",
      "For CV number  3 the train loss =  1.5103473647453438  and the val loss =  1.6147350888898901\n",
      "For CV number  4 the train loss =  1.5702087908648306  and the val loss =  0.9753923015042985\n",
      "For CV number  5 the train loss =  1.5335452219715833  and the val loss =  1.4086423075494674\n",
      "For CV number  6 the train loss =  1.5706973153144799  and the val loss =  0.9670855180351106\n",
      "For CV number  7 the train loss =  1.470115329543413  and the val loss =  1.9200136081203985\n",
      "For CV number  8 the train loss =  1.5391285455320443  and the val loss =  1.3508711746561313\n",
      "For CV number  9 the train loss =  1.5404005426481786  and the val loss =  1.336652603031023\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats for LR MAE Loss\")\n",
    "for i in range(len(video_game_model_mae)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", video_game_model_mae[i][0], \" and the val loss = \", video_game_model_mae[i][1] )\n",
    "print(\"Stats for LR RMSE Loss\")\n",
    "for i in range(len(video_game_model_rmse)):\n",
    "    print(\"For CV number \",i, \"the train loss = \", video_game_model_rmse[i][0], \" and the val loss = \", video_game_model_rmse[i][1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>User_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>40.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Global_Sales  Critic_Score User_Score\n",
       "0         82.53          76.0          8\n",
       "1         40.24           NaN        NaN\n",
       "2         35.52          82.0        8.3\n",
       "3         32.77          80.0          8\n",
       "4         31.37           NaN        NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"LR_dataset/VideoGameDataset - Video_Games_Sales_as_at_22_Dec_2016.csv\",  usecols=['Critic_Score','User_Score','Global_Sales'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Global_Sales       0\n",
       "Critic_Score    8582\n",
       "User_Score      6704\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0    256\n",
       "71.0    254\n",
       "75.0    245\n",
       "78.0    240\n",
       "73.0    238\n",
       "       ... \n",
       "20.0      3\n",
       "17.0      1\n",
       "22.0      1\n",
       "13.0      1\n",
       "21.0      1\n",
       "Name: Critic_Score, Length: 82, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Critic_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tbd    2425\n",
       "7.8     324\n",
       "8       290\n",
       "8.2     282\n",
       "8.3     254\n",
       "       ... \n",
       "0.2       2\n",
       "9.6       2\n",
       "0.5       2\n",
       "0         1\n",
       "9.7       1\n",
       "Name: User_Score, Length: 96, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"User_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical_sol(X,y):\n",
    "#     bias = np.zeros((X.shape[0],1))\n",
    "#     bias.fill(1)\n",
    "#     X = np.append(X,bias, axis = 1)\n",
    "#     print(X)\n",
    "    W = np.dot(np.linalg.inv(np.dot(X.T,X)), np.dot(X.T,y))\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = MyPreProcessor()\n",
    "X,y = preproc.pre_process(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]  #number of examples\n",
    "fold_size = int(m/10)\n",
    "start = fold_size\n",
    "end = 2*fold_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_i = np.concatenate((X[0:start], X[end+1:]))\n",
    "ytrain_i = np.concatenate((y[0:start],y[end+1:]))\n",
    "X_test =  X[start:end]\n",
    "y_test = y[start:end]\n",
    "\n",
    "bias = np.zeros((Xtrain_i.shape[0],1))\n",
    "bias.fill(1)\n",
    "Xtrain_i = np.append(Xtrain_i,bias, axis = 1)\n",
    "\n",
    "bias = np.zeros((X_test.shape[0],1))\n",
    "bias.fill(1)\n",
    "X_test = np.append(X_test,bias, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63253946 -0.28473155 -0.42006662 ... -0.77419337 -0.74336705\n",
      "   1.        ]\n",
      " [ 0.63253946  0.10623642 -0.10428479 ... -0.46442644 -0.19450817\n",
      "   1.        ]\n",
      " [ 0.63253946 -0.19450817 -0.35239908 ... -0.70201467 -0.66818091\n",
      "   1.        ]\n",
      " ...\n",
      " [ 3.63998538 -0.32984324 -0.45765969 ... -0.808779   -0.7659229\n",
      "   1.        ]\n",
      " [ 0.63253946 -0.24713848 -0.42006662 ... -0.76817848 -0.72832982\n",
      "   1.        ]\n",
      " [ 0.63253946  0.15886672 -0.10428479 ... -0.40427753 -0.3110467\n",
      "   1.        ]]\n",
      "[[ 2.13626242  0.00849443 -0.19450817 ... -0.55916099 -0.53284584\n",
      "   1.        ]\n",
      " [ 3.63998538 -0.09676618 -0.28473155 ... -0.70051095 -0.53510142\n",
      "   1.        ]\n",
      " [ 3.63998538 -0.11180341 -0.28473155 ... -0.65765484 -0.61254316\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.63253946  0.0460875  -0.1569151  ... -0.52833467 -0.4711932\n",
      "   1.        ]\n",
      " [ 2.13626242 -0.14939648 -0.31480601 ... -0.6448732  -0.63133969\n",
      "   1.        ]\n",
      " [ 0.63253946  0.06864335 -0.1569151  ... -0.51705675 -0.40427753\n",
      "   1.        ]]\n",
      "(3758, 9)\n",
      "(417, 9)\n",
      "(3758, 1)\n",
      "(417, 1)\n",
      "[[ 6.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " ...\n",
      " [10.]\n",
      " [15.]\n",
      " [10.]]\n",
      "[[ 9.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [12.]\n",
      " [12.]\n",
      " [ 4.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 6.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [11.]\n",
      " [11.]\n",
      " [20.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [18.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [14.]\n",
      " [13.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [18.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]\n",
      " [11.]\n",
      " [10.]\n",
      " [10.]\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [14.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [16.]\n",
      " [11.]\n",
      " [ 6.]\n",
      " [13.]\n",
      " [13.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [ 3.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [13.]\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [16.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [26.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [14.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [11.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [17.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [19.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [17.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [14.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [18.]\n",
      " [10.]\n",
      " [17.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [ 6.]\n",
      " [14.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [12.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [15.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [10.]\n",
      " [10.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [12.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [18.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 6.]\n",
      " [11.]\n",
      " [11.]\n",
      " [13.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [13.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [ 5.]\n",
      " [16.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 5.]\n",
      " [14.]\n",
      " [10.]\n",
      " [10.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 6.]\n",
      " [15.]\n",
      " [15.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [15.]\n",
      " [12.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [18.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [13.]\n",
      " [12.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [15.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 4.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [ 5.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [ 3.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [11.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 6.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [12.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [12.]\n",
      " [12.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [18.]\n",
      " [ 4.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [10.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [11.]\n",
      " [12.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [13.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 3.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [13.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [15.]\n",
      " [16.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [13.]\n",
      " [14.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [10.]\n",
      " [10.]\n",
      " [11.]\n",
      " [10.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [11.]\n",
      " [11.]\n",
      " [13.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 9.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(Xtrain_i)\n",
    "print(Xtrain_i.shape)\n",
    "print(X_test.shape)\n",
    "print(ytrain_i.shape)\n",
    "print(y_test.shape)\n",
    "print(ytrain_i)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.25435685]\n",
      " [ -0.05179893]\n",
      " [  7.38089846]\n",
      " [  6.93155226]\n",
      " [  6.32780996]\n",
      " [-13.90095662]\n",
      " [ -6.61419301]\n",
      " [  5.71294489]\n",
      " [  8.94980457]]\n"
     ]
    }
   ],
   "source": [
    "W = get_analytical_sol(Xtrain_i,ytrain_i)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5972868051383804\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = np.dot(Xtrain_i,W)\n",
    "error = (np.sum(abs(ytrain_i-y_pred_train))/y_pred_train.shape[0])\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4951636472575514\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.dot(X_test,W)\n",
    "error = (np.sum(abs(y_test-y_pred_test))/y_pred_test.shape[0])\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_i = Xtrain_i[:,:-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
