{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Q1 import *\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/mnist/mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0          5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4          9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "59995      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59997      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59998      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59999      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0          0      0      0      0      0      0      0      0  \n",
       "1          0      0      0      0      0      0      0      0  \n",
       "2          0      0      0      0      0      0      0      0  \n",
       "3          0      0      0      0      0      0      0      0  \n",
       "4          0      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "59995      0      0      0      0      0      0      0      0  \n",
       "59996      0      0      0      0      0      0      0      0  \n",
       "59997      0      0      0      0      0      0      0      0  \n",
       "59998      0      0      0      0      0      0      0      0  \n",
       "59999      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[60000 rows x 785 columns]>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[:, 0]\n",
    "X = dataset[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyNeuralNetwork(5,[8,16,8,4], \"relu\", 0.01, \"normal\", 0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.expand_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(y)\n",
    "c = np.max(y)\n",
    "new_y = np.zeros((m,c+1))\n",
    "for i in range(m):\n",
    "    l = y[i]\n",
    "    new_y[i,l] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=int64)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "class MyNeuralNetwork():\n",
    "    \"\"\"\n",
    "    My implementation of a Neural Network Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh']\n",
    "    weight_inits = ['zero', 'random', 'normal']\n",
    "\n",
    "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
    "        \"\"\"\n",
    "        Initializing a new MyNeuralNetwork object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers : int value specifying the number of layers\n",
    "\n",
    "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
    "\n",
    "        activation : string specifying the activation function to be used\n",
    "                     possible inputs: relu, sigmoid, linear, tanh\n",
    "\n",
    "        learning_rate : float value specifying the learning rate to be used\n",
    "\n",
    "        weight_init : string specifying the weight initialization function to be used\n",
    "                      possible inputs: zero, random, normal\n",
    "\n",
    "        batch_size : int value specifying the batch size to be used\n",
    "\n",
    "        num_epochs : int value specifying the number of epochs to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if activation not in self.acti_fns:\n",
    "            raise Exception('Incorrect Activation Function')\n",
    "\n",
    "        if weight_init not in self.weight_inits:\n",
    "            raise Exception('Incorrect Weight Initialization Function')\n",
    "\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_init = weight_init\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "    def relu(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        x_calc = np.maximum(0,X)\n",
    "        cache = {\"Z\" : X}\n",
    "\n",
    "        return np.array(x_calc) , cache\n",
    "\n",
    "    def relu_grad(self, da, activation_cache):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        z = activation_cache[\"Z\"]\n",
    "        x_calc = np.array(da)\n",
    "        x_calc[z <= 0] = 0\n",
    "        return x_calc\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "\n",
    "        x_calc = 1/(1+np.exp(-1*X))\n",
    "        cache = {\"Z\" : X}\n",
    "        return x_calc,cache\n",
    "\n",
    "    def sigmoid_grad(self, da, activation_cache):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        z = activation_cache[\"Z\"]\n",
    "        a = 1/(1+np.exp(-1*z + 10**-7))\n",
    "\n",
    "        dz = np.multiply(da,a*(1-a))\n",
    "#         dz = np.add(np.log(da), np.log(a*(1-a)))\n",
    "        return dz\n",
    "\n",
    "    def linear(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        cache = {\"Z\" : X}\n",
    "        return X, cache\n",
    "\n",
    "    def linear_grad(self, da, activation_cache):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        z = activation_cache[\"Z\"]\n",
    "        dz = np.array(da)\n",
    "        return np.array(dz)\n",
    "\n",
    "    def tanh(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "\n",
    "        x_calc = (np.exp(X) - np.exp(-X))/(np.exp(X) + np.exp(-X))\n",
    "\n",
    "        cache = {\"Z\" : X}\n",
    "\n",
    "        return x_calc, cache\n",
    "\n",
    "    def tanh_grad(self, da, activation_cache):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        z = activation_cache[\"Z\"]\n",
    "        dz = np.multiply(da,1 - np.power(self.tanh(z)[0],2))\n",
    "\n",
    "        return dz\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "#         print(\"Softmax : \", X.shape)\n",
    "        x_calc = np.exp(X)/(np.sum(np.exp(X), axis = 0) + 10**-6)\n",
    "#         print(\"X : \", X)\n",
    "#         print(\"x_calc : \", x_calc)\n",
    "#         print(\"np.exp(X): \", np.exp(X))\n",
    "#         print(\"np.sum(np.exp(X), axis = 0) : \", np.sum(np.exp(X), axis = 0))\n",
    "\n",
    "            \n",
    "        cache = {\"Z\" : X}\n",
    "        return x_calc, cache\n",
    "\n",
    "    def softmax_grad(self, da, activation_cache,y):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Softmax activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        z = activation_cache[\"Z\"]\n",
    "#         print(\"da :\", da.shape)\n",
    "#         print(\"softmax : \", self.softmax(z)[0].shape)\n",
    "#         print(\"y : \", y.shape)\n",
    "#         print()\n",
    "        dz = np.multiply(da,(self.softmax(z)[0] - y))\n",
    "#         print(\"dz : \", dz.shape)\n",
    "#         dz = np.add(np.log(da), np.log(self.softmax(z)[0] - y))\n",
    "        return dz\n",
    "\n",
    "    def zero_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Zero Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 1-dimensional numpy array which contains the initial weights for the requested layer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        weight = np.zeros(shape)\n",
    "        return weight\n",
    "\n",
    "    def random_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Random Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 1-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        weight = np.random.rand(shape[0], shape[1])*0.01\n",
    "        return weight\n",
    "\n",
    "    def normal_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 1-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        weight = np.random.normal(scale = 0.01,size = shape)\n",
    "        return weight\n",
    "\n",
    "    def fit(self, X, y, X_test = None, y_test = None):\n",
    "        \"\"\"\n",
    "        Fitting (training) the linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : an instance of self\n",
    "        \"\"\"\n",
    "\n",
    "        # fit function has to return an instance of itself or else it won't work with test.py\n",
    "        X = X.T\n",
    "        y = self.expand_labels(y)\n",
    "        \n",
    "        n_0 , m = X.shape\n",
    "        n_l = y.shape[0]\n",
    "\n",
    "        parameters = self.initialization(n_0,n_l)\n",
    "        self.parameters = parameters\n",
    "\n",
    "        train_loss_history = []\n",
    "        train_accuracy_history = []\n",
    "        test_loss_history = []\n",
    "        test_accuracy_history = []\n",
    "\n",
    "        \n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs), desc = \"Progress Total : \", position = 0, leave = True):\n",
    "\n",
    "\n",
    "            n_batches = m//self.batch_size\n",
    "            X_batches = [X[:,self.batch_size*i:self.batch_size*(i+1)] for i in range(0,n_batches)]\n",
    "            y_batches = [y[:,self.batch_size*i:self.batch_size*(i+1)] for i in range(0,n_batches)]\n",
    "\n",
    "            train_batch_loss = []\n",
    "            test_batch_loss = []\n",
    "            train_batch_accuracy = []\n",
    "            test_batch_accuracy = []\n",
    "\n",
    "            for curr_x, curr_y in tqdm(zip(X_batches,y_batches), desc = \"Progress Epoch: \" + str(epoch+1) + \"/\" + str(self.num_epochs), position = 0, leave = True, total = len(X_batches)):\n",
    "                A, cache = self.forward_prop(curr_x,parameters)\n",
    "#                 print(\"A_l : \" , A)\n",
    "\n",
    "                train_cost = self.compute_cost(A,curr_y)\n",
    "                train_batch_loss.append(train_cost)\n",
    "\n",
    "                gradients = self.backward_prop(curr_y,A,cache)\n",
    "                \n",
    "                \n",
    "#                 for key in list(gradients.keys()):\n",
    "#                     gradients[key] = np.maximum(np.minimum(gradients[key],10), -10)\n",
    "\n",
    "                parameters = self.update_parameters(parameters,gradients, self.learning_rate)\n",
    "\n",
    "\n",
    "                self.parameters = parameters\n",
    "#                 self.backwardPhase(curr_x.T, curr_y.T, A.T, cache)\n",
    "                \n",
    "#                 print(gradients)\n",
    "                \n",
    "#             print(parameters)\n",
    "            print(\"Training Loss : \", np.array(train_batch_loss).mean())\n",
    "\n",
    "\n",
    "            # if(X_test is not None):\n",
    "\n",
    "        self.parameters = parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicting probabilities using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which contains the prediction probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        proba,_ = self.forward_prop(X,self.parameters)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
    "        \"\"\"\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "\n",
    "\n",
    "        A_l, cache = self.forward_prop(X.T,self.parameters)\n",
    "        predict = np.argmax(A_l, axis = 0)\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        acc : float value specifying the accuracy of the model on the provided testing set\n",
    "        \"\"\"\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        \n",
    "        cur = 0\n",
    "        y_pred = self.predict(X.T)\n",
    "        for i in range(len(y_pred)):\n",
    "            if(y[i] == y_pred[i]):\n",
    "                cur+=1\n",
    "        return cur/len(y_pred)\n",
    "\n",
    "    def initialization(self, n_0, n_l):\n",
    "        \"\"\"\n",
    "\n",
    "        Initializes the model parameters of the deep neural network\n",
    "\n",
    "        Parameters \n",
    "        -----------\n",
    "        n_0 : int signifying the number of nodes in the input layer (n_features)\n",
    "        n_l : int signifying the number of output nodes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Paramsters : Model parameters \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        parameters = {}\n",
    "        layers = self.layer_sizes\n",
    "        layers.insert(0,n_0)\n",
    "        layers.append(n_l)\n",
    "\n",
    "\n",
    "        for i in range(1,len(layers)):\n",
    "            if(self.weight_init == 'zero'):\n",
    "                curr_layer = self.zero_init((layers[i],layers[i-1]))\n",
    "\n",
    "            elif(self.weight_init == 'random'):\n",
    "                curr_layer = self.random_init((layers[i],layers[i-1]))\n",
    "\n",
    "            else:\n",
    "                curr_layer = self.normal_init((layers[i],layers[i-1]))\n",
    "\n",
    "            parameters[\"W\" + str(i)] = curr_layer\n",
    "            parameters[\"b\" + str(i)] = np.zeros((layers[i],1))\n",
    "\n",
    "        self.parameters = parameters\n",
    "\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def forward_prop(self,X,parameters):\n",
    "\n",
    "        \"\"\"\n",
    "        Implements one forward propagation of the deep neural network.\n",
    "\n",
    "        Parameters \n",
    "        ----------\n",
    "        X : Training set to be forward propagated\n",
    "        parameters : model parameters \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A_l : Activations of the final layer\n",
    "        forward_cache : list contraining all the linear_cache and activation_cache of all the layers\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A = X\n",
    "        L = len(parameters)//2\n",
    "\n",
    "        forward_cache  = []\n",
    "#         activations = []\n",
    "\n",
    "        for i in range(1,L):\n",
    "            A_prev = A\n",
    "            Z = np.dot(parameters[\"W\" + str(i)],A_prev) + parameters[\"b\" + str(i)]\n",
    "            linear_cache = (A_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)])\n",
    "\n",
    "            if(self.activation == \"relu\"):\n",
    "                A, activation_cache = self.relu(Z)\n",
    "\n",
    "            elif (self.activation == \"tanh\"):\n",
    "                A, activation_cache = self.tanh(Z)\n",
    "\n",
    "            elif (self.activation == \"linear\"):\n",
    "                A, activation_cache = self.linear(Z)\n",
    "\n",
    "            elif (self.activation == \"sigmoid\"):\n",
    "                A, activation_cache = self.sigmoid(Z)\n",
    "\n",
    "            layer_cache = (linear_cache, activation_cache)\n",
    "\n",
    "            forward_cache.append(layer_cache)\n",
    "#             activations.append(A)           \n",
    "            A_prev = A\n",
    "        \n",
    "        \n",
    "        Z_l = np.dot(parameters[\"W\" + str(L)],A_prev) + parameters[\"b\" + str(L)]\n",
    "        linear_cache = (A_prev, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)])\n",
    "        \n",
    "        A_l, activation_cache = self.softmax(Z_l)\n",
    "        layer_cache = (linear_cache, activation_cache)\n",
    "        forward_cache.append(layer_cache)\n",
    "#         activation.append(A_l)\n",
    "\n",
    "        return A_l, forward_cache\n",
    "\n",
    "\n",
    "\n",
    "    def compute_cost(self,A,y):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : Activations of the final layer of the model\n",
    "        y : ground truth labels of the training batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cost : calculated cost\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        m = y.shape[1]\n",
    "\n",
    "        cost = (-1 / m) * np.sum(np.multiply(y, np.log(A + 10**-7)) + np.multiply(1 - y, np.log(1 - A - 10**-7)))\n",
    "#         print(np.multiply(y, np.log(A)) + np.multiply(1 - y, np.log(1 - A)))\n",
    "\n",
    "        cost = np.squeeze(cost)\n",
    "\n",
    "        return cost\n",
    "\n",
    "\n",
    "\n",
    "    def backward_prop(self, y, A_l,cache):\n",
    "        \"\"\"\n",
    "        Implements backward propagation of the complete model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : The ground truth labels of the curr training set\n",
    "        A_l : activations of the final layer of the model\n",
    "        cache : tuple containing the linear_cache and activation_cache\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradients : Dictionary containing the gradient vectors for each layer of the model\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        dL / dw = dL/d(a_l) * d(a_l)/d(z_l)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        gradients  = {}\n",
    "        L = len(cache)\n",
    "\n",
    "#         da_l = -1*np.divide(y,A_l) + np.divide(1-y, 1 - A_l)\n",
    "#         print(da_l.shape)\n",
    "        \n",
    "        da_l = (A_l - y)/y.shape[1]\n",
    "        linear_cache, activation_cache = cache[L-1]\n",
    "\n",
    "#         dz_l = self.sigmoid_grad(da_l, activation_cache)\n",
    "        gradients[\"dW\" + str(L)], gradients[\"db\" + str(L)], gradients[\"dA\" + str(L)] = self.one_step_backward(da_l,linear_cache)\n",
    "\n",
    "\n",
    "        for i in range(L-1,0,-1):\n",
    "            linear_cache, activation_cache = cache[i-1]\n",
    "\n",
    "            if(self.activation == \"relu\"):\n",
    "                dz = self.relu_grad(gradients[\"dA\" + str(i+1)], activation_cache)\n",
    "                gradients[\"dW\" + str(i)], gradients[\"db\" + str(i)], gradients[\"dA\" + str(i)] = self.one_step_backward(dz,linear_cache)\n",
    "\n",
    "            elif (self.activation == \"tanh\"):\n",
    "                dz = self.tanh_grad(gradients[\"dA\" + str(i+1)], activation_cache)\n",
    "                gradients[\"dW\" + str(i)], gradients[\"db\" + str(i)], gradients[\"dA\" + str(i)] = self.one_step_backward(dz,linear_cache)\n",
    "\n",
    "            elif (self.activation == \"linear\"):\n",
    "                dz = self.linear_grad(gradients[\"dA\" + str(i+1)], activation_cache)\n",
    "                gradients[\"dW\" + str(i)], gradients[\"db\" + str(i)], gradients[\"dA\" + str(i)] = self.one_step_backward(dz,linear_cache)\n",
    "\n",
    "            elif (self.activation == \"sigmoid\"):\n",
    "                dz = self.sigmoid_grad(gradients[\"dA\" + str(i+1)], activation_cache)\n",
    "                gradients[\"dW\" + str(i)], gradients[\"db\" + str(i)], gradients[\"dA\" + str(i)] = self.one_step_backward(dz,linear_cache)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def one_step_backward(self,dz, linear_cache):\n",
    "\n",
    "        \"\"\"\n",
    "        Performs one step backward propagationm\n",
    "\n",
    "        Parameters \n",
    "        ----------\n",
    "        dz : The gradient of the loss with respoct to z \n",
    "        linear_cache : tuple containing the preactivated a,w,b for the layer\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        dw : The gradient of the loss with respoct to w\n",
    "        db : The gradient of the loss with respoct to b \n",
    "        da_prev : The gradient of the loss with respoct to a_prev\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        A_prev, W, b = linear_cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dw = (1/m)*(np.dot(dz, A_prev.T))\n",
    "        db = (1/m)*(np.sum(dz, axis=1, keepdims = True))\n",
    "        da_prev = np.dot(W.T,dz)\n",
    "        \n",
    "\n",
    "        return dw,db,da_prev\n",
    "    \n",
    "\n",
    "\n",
    "    def update_parameters(self, parameters, gradients, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the model parameters. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        parameters : dictionary containing the model parameters.\n",
    "        gradients : dictionary contrainig the gradients for each layer.\n",
    "        learning_rate : learning rate of the model.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        parameters : updated paramteters \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        L = len(parameters) //2\n",
    "\n",
    "        for i in range(0, L):\n",
    "#             print(gradients[\"dW\" + str(i+1)])\n",
    "#                 print(\"dgdgdgdgd\")\n",
    "            parameters[\"W\" + str(i+1)] = parameters[\"W\" + str(i+1)] -learning_rate*gradients[\"dW\" + str(i+1)]\n",
    "            parameters[\"b\" + str(i+1)] = parameters[\"b\" + str(i+1)] -learning_rate*gradients[\"db\" + str(i+1)]\n",
    "\n",
    "        self.parameters = copy.deepcopy(parameters)\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "\n",
    "    def expand_labels(self, y):\n",
    "\n",
    "        m = len(y)\n",
    "        c = np.max(y)\n",
    "        new_y = np.zeros((m,c+1))\n",
    "        for i in range(m):\n",
    "            l = y[i]\n",
    "            new_y[i,l] = 1\n",
    "\n",
    "        return new_y.T\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynn = MyNeuralNetwork(3,[256,128,64], \"sigmoid\", 0.01, \"normal\", 5000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 1/20: 100%|████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss :  3.251008260391982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 2/20: 100%|████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss :  3.251008115942813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 3/20: 100%|████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss :  3.2510079715054623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 4/20: 100%|████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss :  3.2510078270799325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 5/20: 100%|████████████████████████████████████████████████████████████| 12/12 [00:03<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss :  3.25100768266622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 6/20:  17%|██████████▏                                                  | 2/12 [00:00<00:04,  2.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-677-ff1ac45f50e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmynn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-673-2386567fc009>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_test, y_test)\u001b[0m\n\u001b[0;32m    331\u001b[0m                 \u001b[0mtrain_batch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                 \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-673-2386567fc009>\u001b[0m in \u001b[0;36mbackward_prop\u001b[1;34m(self, y, A_l, cache)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                 \u001b[0mdz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dA\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m                 \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dW\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dA\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_step_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinear_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-673-2386567fc009>\u001b[0m in \u001b[0;36msigmoid_grad\u001b[1;34m(self, da, activation_cache)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mdz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;31m#         dz = np.add(np.log(da), np.log(a*(1-a)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mynn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mynn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [0],\n",
       "       [4],\n",
       "       ...,\n",
       "       [5],\n",
       "       [6],\n",
       "       [8]], dtype=int64)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11236666666666667\n"
     ]
    }
   ],
   "source": [
    "cur = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y[i] == y_pred[i]):\n",
    "        cur+=1\n",
    "print(cur/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mynn.predict_proba(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - ETA: 1s - loss: 2.2946 - accuracy: 0.21 - ETA: 4s - loss: 1.6196 - accuracy: 0.44 - ETA: 4s - loss: 1.2519 - accuracy: 0.56 - ETA: 4s - loss: 1.0754 - accuracy: 0.63 - ETA: 4s - loss: 0.9786 - accuracy: 0.67 - ETA: 4s - loss: 0.8961 - accuracy: 0.70 - ETA: 4s - loss: 0.8331 - accuracy: 0.73 - ETA: 4s - loss: 0.7813 - accuracy: 0.75 - ETA: 4s - loss: 0.7546 - accuracy: 0.76 - ETA: 4s - loss: 0.7203 - accuracy: 0.77 - ETA: 4s - loss: 0.7068 - accuracy: 0.78 - ETA: 4s - loss: 0.6955 - accuracy: 0.78 - ETA: 4s - loss: 0.6778 - accuracy: 0.79 - ETA: 4s - loss: 0.6611 - accuracy: 0.79 - ETA: 4s - loss: 0.6478 - accuracy: 0.80 - ETA: 4s - loss: 0.6316 - accuracy: 0.81 - ETA: 4s - loss: 0.6137 - accuracy: 0.81 - ETA: 4s - loss: 0.5966 - accuracy: 0.82 - ETA: 4s - loss: 0.5873 - accuracy: 0.82 - ETA: 4s - loss: 0.5749 - accuracy: 0.82 - ETA: 4s - loss: 0.5611 - accuracy: 0.83 - ETA: 4s - loss: 0.5516 - accuracy: 0.83 - ETA: 4s - loss: 0.5447 - accuracy: 0.84 - ETA: 4s - loss: 0.5396 - accuracy: 0.84 - ETA: 3s - loss: 0.5327 - accuracy: 0.84 - ETA: 3s - loss: 0.5237 - accuracy: 0.84 - ETA: 3s - loss: 0.5220 - accuracy: 0.84 - ETA: 3s - loss: 0.5144 - accuracy: 0.85 - ETA: 3s - loss: 0.5091 - accuracy: 0.85 - ETA: 3s - loss: 0.5027 - accuracy: 0.85 - ETA: 3s - loss: 0.4946 - accuracy: 0.85 - ETA: 3s - loss: 0.4871 - accuracy: 0.85 - ETA: 3s - loss: 0.4842 - accuracy: 0.86 - ETA: 3s - loss: 0.4782 - accuracy: 0.86 - ETA: 3s - loss: 0.4735 - accuracy: 0.86 - ETA: 3s - loss: 0.4680 - accuracy: 0.86 - ETA: 3s - loss: 0.4628 - accuracy: 0.86 - ETA: 3s - loss: 0.4619 - accuracy: 0.86 - ETA: 3s - loss: 0.4592 - accuracy: 0.87 - ETA: 3s - loss: 0.4554 - accuracy: 0.87 - ETA: 3s - loss: 0.4523 - accuracy: 0.87 - ETA: 3s - loss: 0.4490 - accuracy: 0.87 - ETA: 3s - loss: 0.4478 - accuracy: 0.87 - ETA: 3s - loss: 0.4463 - accuracy: 0.87 - ETA: 3s - loss: 0.4443 - accuracy: 0.87 - ETA: 3s - loss: 0.4431 - accuracy: 0.87 - ETA: 2s - loss: 0.4393 - accuracy: 0.87 - ETA: 2s - loss: 0.4366 - accuracy: 0.87 - ETA: 2s - loss: 0.4352 - accuracy: 0.88 - ETA: 2s - loss: 0.4338 - accuracy: 0.88 - ETA: 2s - loss: 0.4307 - accuracy: 0.88 - ETA: 2s - loss: 0.4265 - accuracy: 0.88 - ETA: 2s - loss: 0.4223 - accuracy: 0.88 - ETA: 2s - loss: 0.4190 - accuracy: 0.88 - ETA: 2s - loss: 0.4185 - accuracy: 0.88 - ETA: 2s - loss: 0.4164 - accuracy: 0.88 - ETA: 2s - loss: 0.4135 - accuracy: 0.88 - ETA: 2s - loss: 0.4108 - accuracy: 0.88 - ETA: 2s - loss: 0.4087 - accuracy: 0.88 - ETA: 2s - loss: 0.4077 - accuracy: 0.88 - ETA: 2s - loss: 0.4049 - accuracy: 0.88 - ETA: 2s - loss: 0.4019 - accuracy: 0.89 - ETA: 2s - loss: 0.3996 - accuracy: 0.89 - ETA: 2s - loss: 0.3978 - accuracy: 0.89 - ETA: 2s - loss: 0.3958 - accuracy: 0.89 - ETA: 2s - loss: 0.3931 - accuracy: 0.89 - ETA: 1s - loss: 0.3919 - accuracy: 0.89 - ETA: 1s - loss: 0.3901 - accuracy: 0.89 - ETA: 1s - loss: 0.3888 - accuracy: 0.89 - ETA: 1s - loss: 0.3860 - accuracy: 0.89 - ETA: 1s - loss: 0.3839 - accuracy: 0.89 - ETA: 1s - loss: 0.3828 - accuracy: 0.89 - ETA: 1s - loss: 0.3813 - accuracy: 0.89 - ETA: 1s - loss: 0.3791 - accuracy: 0.89 - ETA: 1s - loss: 0.3786 - accuracy: 0.89 - ETA: 1s - loss: 0.3765 - accuracy: 0.89 - ETA: 1s - loss: 0.3766 - accuracy: 0.89 - ETA: 1s - loss: 0.3762 - accuracy: 0.89 - ETA: 1s - loss: 0.3753 - accuracy: 0.89 - ETA: 1s - loss: 0.3744 - accuracy: 0.89 - ETA: 1s - loss: 0.3728 - accuracy: 0.90 - ETA: 1s - loss: 0.3710 - accuracy: 0.90 - ETA: 1s - loss: 0.3691 - accuracy: 0.90 - ETA: 1s - loss: 0.3670 - accuracy: 0.90 - ETA: 1s - loss: 0.3658 - accuracy: 0.90 - ETA: 1s - loss: 0.3643 - accuracy: 0.90 - ETA: 1s - loss: 0.3629 - accuracy: 0.90 - ETA: 1s - loss: 0.3617 - accuracy: 0.90 - ETA: 0s - loss: 0.3599 - accuracy: 0.90 - ETA: 0s - loss: 0.3590 - accuracy: 0.90 - ETA: 0s - loss: 0.3589 - accuracy: 0.90 - ETA: 0s - loss: 0.3584 - accuracy: 0.90 - ETA: 0s - loss: 0.3568 - accuracy: 0.90 - ETA: 0s - loss: 0.3554 - accuracy: 0.90 - ETA: 0s - loss: 0.3539 - accuracy: 0.90 - ETA: 0s - loss: 0.3530 - accuracy: 0.90 - ETA: 0s - loss: 0.3518 - accuracy: 0.90 - ETA: 0s - loss: 0.3506 - accuracy: 0.90 - ETA: 0s - loss: 0.3503 - accuracy: 0.90 - ETA: 0s - loss: 0.3500 - accuracy: 0.90 - ETA: 0s - loss: 0.3485 - accuracy: 0.90 - ETA: 0s - loss: 0.3475 - accuracy: 0.90 - ETA: 0s - loss: 0.3462 - accuracy: 0.90 - ETA: 0s - loss: 0.3456 - accuracy: 0.90 - ETA: 0s - loss: 0.3448 - accuracy: 0.90 - ETA: 0s - loss: 0.3441 - accuracy: 0.90 - ETA: 0s - loss: 0.3438 - accuracy: 0.90 - ETA: 0s - loss: 0.3432 - accuracy: 0.90 - ETA: 0s - loss: 0.3426 - accuracy: 0.90 - ETA: 0s - loss: 0.3423 - accuracy: 0.90 - 6s 3ms/step - loss: 0.3421 - accuracy: 0.9095\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.93 - ETA: 6s - loss: 0.1708 - accuracy: 0.94 - ETA: 6s - loss: 0.1531 - accuracy: 0.95 - ETA: 6s - loss: 0.1670 - accuracy: 0.95 - ETA: 6s - loss: 0.2011 - accuracy: 0.94 - ETA: 6s - loss: 0.2194 - accuracy: 0.94 - ETA: 6s - loss: 0.2198 - accuracy: 0.94 - ETA: 6s - loss: 0.2309 - accuracy: 0.94 - ETA: 5s - loss: 0.2457 - accuracy: 0.94 - ETA: 5s - loss: 0.2656 - accuracy: 0.93 - ETA: 5s - loss: 0.2695 - accuracy: 0.93 - ETA: 5s - loss: 0.2554 - accuracy: 0.93 - ETA: 5s - loss: 0.2536 - accuracy: 0.93 - ETA: 5s - loss: 0.2561 - accuracy: 0.93 - ETA: 5s - loss: 0.2485 - accuracy: 0.93 - ETA: 5s - loss: 0.2569 - accuracy: 0.93 - ETA: 5s - loss: 0.2594 - accuracy: 0.93 - ETA: 4s - loss: 0.2625 - accuracy: 0.93 - ETA: 4s - loss: 0.2568 - accuracy: 0.93 - ETA: 4s - loss: 0.2632 - accuracy: 0.93 - ETA: 4s - loss: 0.2699 - accuracy: 0.93 - ETA: 4s - loss: 0.2695 - accuracy: 0.93 - ETA: 4s - loss: 0.2670 - accuracy: 0.93 - ETA: 4s - loss: 0.2632 - accuracy: 0.93 - ETA: 4s - loss: 0.2648 - accuracy: 0.93 - ETA: 4s - loss: 0.2632 - accuracy: 0.93 - ETA: 4s - loss: 0.2620 - accuracy: 0.93 - ETA: 4s - loss: 0.2591 - accuracy: 0.93 - ETA: 4s - loss: 0.2602 - accuracy: 0.93 - ETA: 4s - loss: 0.2581 - accuracy: 0.93 - ETA: 4s - loss: 0.2551 - accuracy: 0.93 - ETA: 4s - loss: 0.2545 - accuracy: 0.93 - ETA: 4s - loss: 0.2532 - accuracy: 0.93 - ETA: 4s - loss: 0.2517 - accuracy: 0.94 - ETA: 4s - loss: 0.2488 - accuracy: 0.94 - ETA: 4s - loss: 0.2498 - accuracy: 0.94 - ETA: 3s - loss: 0.2485 - accuracy: 0.94 - ETA: 3s - loss: 0.2463 - accuracy: 0.94 - ETA: 3s - loss: 0.2453 - accuracy: 0.94 - ETA: 3s - loss: 0.2443 - accuracy: 0.94 - ETA: 3s - loss: 0.2443 - accuracy: 0.94 - ETA: 3s - loss: 0.2465 - accuracy: 0.94 - ETA: 3s - loss: 0.2469 - accuracy: 0.94 - ETA: 3s - loss: 0.2466 - accuracy: 0.94 - ETA: 3s - loss: 0.2468 - accuracy: 0.94 - ETA: 3s - loss: 0.2473 - accuracy: 0.94 - ETA: 3s - loss: 0.2463 - accuracy: 0.94 - ETA: 3s - loss: 0.2439 - accuracy: 0.94 - ETA: 3s - loss: 0.2455 - accuracy: 0.94 - ETA: 3s - loss: 0.2451 - accuracy: 0.94 - ETA: 3s - loss: 0.2431 - accuracy: 0.94 - ETA: 3s - loss: 0.2410 - accuracy: 0.94 - ETA: 3s - loss: 0.2408 - accuracy: 0.94 - ETA: 3s - loss: 0.2424 - accuracy: 0.94 - ETA: 3s - loss: 0.2423 - accuracy: 0.94 - ETA: 3s - loss: 0.2395 - accuracy: 0.94 - ETA: 3s - loss: 0.2412 - accuracy: 0.94 - ETA: 3s - loss: 0.2427 - accuracy: 0.94 - ETA: 2s - loss: 0.2440 - accuracy: 0.94 - ETA: 2s - loss: 0.2439 - accuracy: 0.94 - ETA: 2s - loss: 0.2430 - accuracy: 0.94 - ETA: 2s - loss: 0.2417 - accuracy: 0.94 - ETA: 2s - loss: 0.2411 - accuracy: 0.94 - ETA: 2s - loss: 0.2402 - accuracy: 0.94 - ETA: 2s - loss: 0.2408 - accuracy: 0.94 - ETA: 2s - loss: 0.2414 - accuracy: 0.94 - ETA: 2s - loss: 0.2408 - accuracy: 0.94 - ETA: 2s - loss: 0.2394 - accuracy: 0.94 - ETA: 2s - loss: 0.2384 - accuracy: 0.94 - ETA: 2s - loss: 0.2389 - accuracy: 0.94 - ETA: 2s - loss: 0.2389 - accuracy: 0.94 - ETA: 2s - loss: 0.2397 - accuracy: 0.94 - ETA: 2s - loss: 0.2389 - accuracy: 0.94 - ETA: 2s - loss: 0.2387 - accuracy: 0.94 - ETA: 2s - loss: 0.2378 - accuracy: 0.94 - ETA: 2s - loss: 0.2391 - accuracy: 0.94 - ETA: 2s - loss: 0.2399 - accuracy: 0.94 - ETA: 2s - loss: 0.2397 - accuracy: 0.94 - ETA: 1s - loss: 0.2399 - accuracy: 0.94 - ETA: 1s - loss: 0.2394 - accuracy: 0.94 - ETA: 1s - loss: 0.2401 - accuracy: 0.94 - ETA: 1s - loss: 0.2411 - accuracy: 0.94 - ETA: 1s - loss: 0.2422 - accuracy: 0.94 - ETA: 1s - loss: 0.2435 - accuracy: 0.94 - ETA: 1s - loss: 0.2449 - accuracy: 0.94 - ETA: 1s - loss: 0.2462 - accuracy: 0.94 - ETA: 1s - loss: 0.2471 - accuracy: 0.94 - ETA: 1s - loss: 0.2472 - accuracy: 0.94 - ETA: 1s - loss: 0.2472 - accuracy: 0.94 - ETA: 1s - loss: 0.2466 - accuracy: 0.94 - ETA: 1s - loss: 0.2459 - accuracy: 0.94 - ETA: 1s - loss: 0.2486 - accuracy: 0.94 - ETA: 1s - loss: 0.2495 - accuracy: 0.94 - ETA: 1s - loss: 0.2489 - accuracy: 0.94 - ETA: 1s - loss: 0.2492 - accuracy: 0.94 - ETA: 1s - loss: 0.2490 - accuracy: 0.94 - ETA: 1s - loss: 0.2487 - accuracy: 0.94 - ETA: 0s - loss: 0.2487 - accuracy: 0.94 - ETA: 0s - loss: 0.2478 - accuracy: 0.94 - ETA: 0s - loss: 0.2476 - accuracy: 0.94 - ETA: 0s - loss: 0.2468 - accuracy: 0.94 - ETA: 0s - loss: 0.2469 - accuracy: 0.94 - ETA: 0s - loss: 0.2463 - accuracy: 0.94 - ETA: 0s - loss: 0.2459 - accuracy: 0.94 - ETA: 0s - loss: 0.2450 - accuracy: 0.94 - ETA: 0s - loss: 0.2448 - accuracy: 0.94 - ETA: 0s - loss: 0.2441 - accuracy: 0.94 - ETA: 0s - loss: 0.2440 - accuracy: 0.94 - ETA: 0s - loss: 0.2439 - accuracy: 0.94 - ETA: 0s - loss: 0.2435 - accuracy: 0.94 - ETA: 0s - loss: 0.2430 - accuracy: 0.94 - ETA: 0s - loss: 0.2432 - accuracy: 0.94 - ETA: 0s - loss: 0.2427 - accuracy: 0.94 - ETA: 0s - loss: 0.2436 - accuracy: 0.94 - ETA: 0s - loss: 0.2446 - accuracy: 0.94 - ETA: 0s - loss: 0.2449 - accuracy: 0.94 - ETA: 0s - loss: 0.2456 - accuracy: 0.94 - 6s 3ms/step - loss: 0.2455 - accuracy: 0.9424\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.96 - ETA: 6s - loss: 0.1587 - accuracy: 0.95 - ETA: 6s - loss: 0.1905 - accuracy: 0.94 - ETA: 6s - loss: 0.1991 - accuracy: 0.94 - ETA: 6s - loss: 0.1932 - accuracy: 0.95 - ETA: 6s - loss: 0.2103 - accuracy: 0.95 - ETA: 6s - loss: 0.2334 - accuracy: 0.94 - ETA: 5s - loss: 0.2215 - accuracy: 0.94 - ETA: 5s - loss: 0.2147 - accuracy: 0.94 - ETA: 5s - loss: 0.2138 - accuracy: 0.94 - ETA: 5s - loss: 0.2088 - accuracy: 0.94 - ETA: 5s - loss: 0.2041 - accuracy: 0.95 - ETA: 5s - loss: 0.2045 - accuracy: 0.94 - ETA: 5s - loss: 0.2034 - accuracy: 0.95 - ETA: 5s - loss: 0.2033 - accuracy: 0.95 - ETA: 5s - loss: 0.2044 - accuracy: 0.94 - ETA: 5s - loss: 0.2039 - accuracy: 0.94 - ETA: 5s - loss: 0.2056 - accuracy: 0.94 - ETA: 5s - loss: 0.2082 - accuracy: 0.94 - ETA: 5s - loss: 0.2129 - accuracy: 0.94 - ETA: 4s - loss: 0.2127 - accuracy: 0.94 - ETA: 4s - loss: 0.2146 - accuracy: 0.94 - ETA: 4s - loss: 0.2131 - accuracy: 0.94 - ETA: 4s - loss: 0.2126 - accuracy: 0.94 - ETA: 4s - loss: 0.2096 - accuracy: 0.95 - ETA: 4s - loss: 0.2079 - accuracy: 0.95 - ETA: 4s - loss: 0.2072 - accuracy: 0.95 - ETA: 4s - loss: 0.2068 - accuracy: 0.95 - ETA: 4s - loss: 0.2065 - accuracy: 0.95 - ETA: 4s - loss: 0.2050 - accuracy: 0.95 - ETA: 4s - loss: 0.2038 - accuracy: 0.95 - ETA: 4s - loss: 0.2013 - accuracy: 0.95 - ETA: 4s - loss: 0.2005 - accuracy: 0.95 - ETA: 4s - loss: 0.1999 - accuracy: 0.95 - ETA: 4s - loss: 0.1991 - accuracy: 0.95 - ETA: 4s - loss: 0.1993 - accuracy: 0.95 - ETA: 4s - loss: 0.1991 - accuracy: 0.95 - ETA: 4s - loss: 0.1971 - accuracy: 0.95 - ETA: 4s - loss: 0.1952 - accuracy: 0.95 - ETA: 4s - loss: 0.1975 - accuracy: 0.95 - ETA: 4s - loss: 0.1970 - accuracy: 0.95 - ETA: 3s - loss: 0.1979 - accuracy: 0.95 - ETA: 3s - loss: 0.1981 - accuracy: 0.95 - ETA: 3s - loss: 0.1969 - accuracy: 0.95 - ETA: 3s - loss: 0.1958 - accuracy: 0.95 - ETA: 3s - loss: 0.1956 - accuracy: 0.95 - ETA: 3s - loss: 0.1946 - accuracy: 0.95 - ETA: 3s - loss: 0.1940 - accuracy: 0.95 - ETA: 3s - loss: 0.1946 - accuracy: 0.95 - ETA: 3s - loss: 0.1987 - accuracy: 0.95 - ETA: 3s - loss: 0.1991 - accuracy: 0.95 - ETA: 3s - loss: 0.1993 - accuracy: 0.95 - ETA: 3s - loss: 0.1994 - accuracy: 0.95 - ETA: 3s - loss: 0.1994 - accuracy: 0.95 - ETA: 3s - loss: 0.1983 - accuracy: 0.95 - ETA: 3s - loss: 0.1990 - accuracy: 0.95 - ETA: 3s - loss: 0.2002 - accuracy: 0.95 - ETA: 3s - loss: 0.2000 - accuracy: 0.95 - ETA: 3s - loss: 0.1995 - accuracy: 0.95 - ETA: 3s - loss: 0.1988 - accuracy: 0.95 - ETA: 3s - loss: 0.1981 - accuracy: 0.95 - ETA: 3s - loss: 0.1974 - accuracy: 0.95 - ETA: 3s - loss: 0.1970 - accuracy: 0.95 - ETA: 2s - loss: 0.1967 - accuracy: 0.95 - ETA: 2s - loss: 0.1965 - accuracy: 0.95 - ETA: 2s - loss: 0.1973 - accuracy: 0.95 - ETA: 2s - loss: 0.1974 - accuracy: 0.95 - ETA: 2s - loss: 0.1979 - accuracy: 0.95 - ETA: 2s - loss: 0.1982 - accuracy: 0.95 - ETA: 2s - loss: 0.1981 - accuracy: 0.95 - ETA: 2s - loss: 0.1990 - accuracy: 0.95 - ETA: 2s - loss: 0.1984 - accuracy: 0.95 - ETA: 2s - loss: 0.1972 - accuracy: 0.95 - ETA: 2s - loss: 0.1986 - accuracy: 0.95 - ETA: 2s - loss: 0.1995 - accuracy: 0.95 - ETA: 2s - loss: 0.1989 - accuracy: 0.95 - ETA: 2s - loss: 0.1982 - accuracy: 0.95 - ETA: 2s - loss: 0.1971 - accuracy: 0.95 - ETA: 2s - loss: 0.1983 - accuracy: 0.95 - ETA: 2s - loss: 0.1988 - accuracy: 0.95 - ETA: 1s - loss: 0.1992 - accuracy: 0.95 - ETA: 1s - loss: 0.1987 - accuracy: 0.95 - ETA: 1s - loss: 0.1987 - accuracy: 0.95 - ETA: 1s - loss: 0.1987 - accuracy: 0.95 - ETA: 1s - loss: 0.1987 - accuracy: 0.95 - ETA: 1s - loss: 0.1988 - accuracy: 0.95 - ETA: 1s - loss: 0.1984 - accuracy: 0.95 - ETA: 1s - loss: 0.1987 - accuracy: 0.95 - ETA: 1s - loss: 0.1984 - accuracy: 0.95 - ETA: 1s - loss: 0.1983 - accuracy: 0.95 - ETA: 1s - loss: 0.1982 - accuracy: 0.95 - ETA: 1s - loss: 0.1979 - accuracy: 0.95 - ETA: 1s - loss: 0.1976 - accuracy: 0.95 - ETA: 1s - loss: 0.1974 - accuracy: 0.95 - ETA: 1s - loss: 0.1979 - accuracy: 0.95 - ETA: 1s - loss: 0.1988 - accuracy: 0.95 - ETA: 1s - loss: 0.1986 - accuracy: 0.95 - ETA: 1s - loss: 0.1985 - accuracy: 0.95 - ETA: 1s - loss: 0.1983 - accuracy: 0.95 - ETA: 1s - loss: 0.1989 - accuracy: 0.95 - ETA: 1s - loss: 0.1992 - accuracy: 0.95 - ETA: 0s - loss: 0.2001 - accuracy: 0.95 - ETA: 0s - loss: 0.2007 - accuracy: 0.95 - ETA: 0s - loss: 0.2011 - accuracy: 0.95 - ETA: 0s - loss: 0.2009 - accuracy: 0.95 - ETA: 0s - loss: 0.2002 - accuracy: 0.95 - ETA: 0s - loss: 0.2008 - accuracy: 0.95 - ETA: 0s - loss: 0.2008 - accuracy: 0.95 - ETA: 0s - loss: 0.2012 - accuracy: 0.95 - ETA: 0s - loss: 0.2017 - accuracy: 0.95 - ETA: 0s - loss: 0.2018 - accuracy: 0.95 - ETA: 0s - loss: 0.2017 - accuracy: 0.95 - ETA: 0s - loss: 0.2017 - accuracy: 0.95 - ETA: 0s - loss: 0.2014 - accuracy: 0.95 - ETA: 0s - loss: 0.2012 - accuracy: 0.95 - ETA: 0s - loss: 0.2015 - accuracy: 0.95 - ETA: 0s - loss: 0.2015 - accuracy: 0.95 - ETA: 0s - loss: 0.2017 - accuracy: 0.95 - ETA: 0s - loss: 0.2015 - accuracy: 0.95 - ETA: 0s - loss: 0.2012 - accuracy: 0.95 - 6s 3ms/step - loss: 0.2008 - accuracy: 0.9522\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.93 - ETA: 5s - loss: 0.1701 - accuracy: 0.96 - ETA: 5s - loss: 0.1695 - accuracy: 0.96 - ETA: 5s - loss: 0.1524 - accuracy: 0.96 - ETA: 5s - loss: 0.1684 - accuracy: 0.96 - ETA: 5s - loss: 0.1650 - accuracy: 0.96 - ETA: 5s - loss: 0.1687 - accuracy: 0.96 - ETA: 5s - loss: 0.1661 - accuracy: 0.96 - ETA: 5s - loss: 0.1651 - accuracy: 0.96 - ETA: 5s - loss: 0.1617 - accuracy: 0.96 - ETA: 5s - loss: 0.1575 - accuracy: 0.96 - ETA: 5s - loss: 0.1571 - accuracy: 0.96 - ETA: 5s - loss: 0.1549 - accuracy: 0.96 - ETA: 5s - loss: 0.1574 - accuracy: 0.96 - ETA: 5s - loss: 0.1563 - accuracy: 0.96 - ETA: 5s - loss: 0.1601 - accuracy: 0.96 - ETA: 5s - loss: 0.1590 - accuracy: 0.96 - ETA: 4s - loss: 0.1620 - accuracy: 0.96 - ETA: 4s - loss: 0.1597 - accuracy: 0.96 - ETA: 4s - loss: 0.1630 - accuracy: 0.95 - ETA: 4s - loss: 0.1605 - accuracy: 0.96 - ETA: 4s - loss: 0.1681 - accuracy: 0.96 - ETA: 4s - loss: 0.1688 - accuracy: 0.95 - ETA: 4s - loss: 0.1687 - accuracy: 0.95 - ETA: 4s - loss: 0.1701 - accuracy: 0.95 - ETA: 4s - loss: 0.1703 - accuracy: 0.95 - ETA: 4s - loss: 0.1698 - accuracy: 0.95 - ETA: 4s - loss: 0.1707 - accuracy: 0.95 - ETA: 4s - loss: 0.1717 - accuracy: 0.95 - ETA: 4s - loss: 0.1709 - accuracy: 0.95 - ETA: 4s - loss: 0.1706 - accuracy: 0.95 - ETA: 4s - loss: 0.1696 - accuracy: 0.95 - ETA: 4s - loss: 0.1692 - accuracy: 0.95 - ETA: 4s - loss: 0.1693 - accuracy: 0.95 - ETA: 4s - loss: 0.1684 - accuracy: 0.95 - ETA: 4s - loss: 0.1696 - accuracy: 0.95 - ETA: 4s - loss: 0.1694 - accuracy: 0.95 - ETA: 4s - loss: 0.1708 - accuracy: 0.95 - ETA: 4s - loss: 0.1704 - accuracy: 0.95 - ETA: 4s - loss: 0.1717 - accuracy: 0.95 - ETA: 4s - loss: 0.1717 - accuracy: 0.95 - ETA: 3s - loss: 0.1707 - accuracy: 0.95 - ETA: 3s - loss: 0.1698 - accuracy: 0.95 - ETA: 3s - loss: 0.1697 - accuracy: 0.95 - ETA: 3s - loss: 0.1688 - accuracy: 0.95 - ETA: 3s - loss: 0.1718 - accuracy: 0.95 - ETA: 3s - loss: 0.1733 - accuracy: 0.95 - ETA: 3s - loss: 0.1728 - accuracy: 0.95 - ETA: 3s - loss: 0.1756 - accuracy: 0.95 - ETA: 3s - loss: 0.1768 - accuracy: 0.95 - ETA: 3s - loss: 0.1763 - accuracy: 0.95 - ETA: 3s - loss: 0.1761 - accuracy: 0.95 - ETA: 3s - loss: 0.1779 - accuracy: 0.95 - ETA: 3s - loss: 0.1784 - accuracy: 0.95 - ETA: 3s - loss: 0.1794 - accuracy: 0.95 - ETA: 3s - loss: 0.1795 - accuracy: 0.95 - ETA: 3s - loss: 0.1802 - accuracy: 0.95 - ETA: 3s - loss: 0.1801 - accuracy: 0.95 - ETA: 3s - loss: 0.1805 - accuracy: 0.95 - ETA: 3s - loss: 0.1812 - accuracy: 0.95 - ETA: 2s - loss: 0.1801 - accuracy: 0.95 - ETA: 2s - loss: 0.1797 - accuracy: 0.95 - ETA: 2s - loss: 0.1797 - accuracy: 0.95 - ETA: 2s - loss: 0.1805 - accuracy: 0.95 - ETA: 2s - loss: 0.1810 - accuracy: 0.95 - ETA: 2s - loss: 0.1824 - accuracy: 0.95 - ETA: 2s - loss: 0.1838 - accuracy: 0.95 - ETA: 2s - loss: 0.1841 - accuracy: 0.95 - ETA: 2s - loss: 0.1842 - accuracy: 0.95 - ETA: 2s - loss: 0.1840 - accuracy: 0.95 - ETA: 2s - loss: 0.1846 - accuracy: 0.95 - ETA: 2s - loss: 0.1845 - accuracy: 0.95 - ETA: 2s - loss: 0.1846 - accuracy: 0.95 - ETA: 2s - loss: 0.1848 - accuracy: 0.95 - ETA: 2s - loss: 0.1860 - accuracy: 0.95 - ETA: 2s - loss: 0.1860 - accuracy: 0.95 - ETA: 2s - loss: 0.1852 - accuracy: 0.95 - ETA: 2s - loss: 0.1841 - accuracy: 0.95 - ETA: 1s - loss: 0.1839 - accuracy: 0.95 - ETA: 1s - loss: 0.1841 - accuracy: 0.95 - ETA: 1s - loss: 0.1845 - accuracy: 0.95 - ETA: 1s - loss: 0.1843 - accuracy: 0.95 - ETA: 1s - loss: 0.1842 - accuracy: 0.95 - ETA: 1s - loss: 0.1839 - accuracy: 0.95 - ETA: 1s - loss: 0.1838 - accuracy: 0.95 - ETA: 1s - loss: 0.1841 - accuracy: 0.95 - ETA: 1s - loss: 0.1834 - accuracy: 0.95 - ETA: 1s - loss: 0.1831 - accuracy: 0.95 - ETA: 1s - loss: 0.1850 - accuracy: 0.95 - ETA: 1s - loss: 0.1850 - accuracy: 0.95 - ETA: 1s - loss: 0.1873 - accuracy: 0.95 - ETA: 1s - loss: 0.1864 - accuracy: 0.95 - ETA: 1s - loss: 0.1864 - accuracy: 0.95 - ETA: 1s - loss: 0.1862 - accuracy: 0.95 - ETA: 1s - loss: 0.1867 - accuracy: 0.95 - ETA: 1s - loss: 0.1871 - accuracy: 0.95 - ETA: 1s - loss: 0.1871 - accuracy: 0.95 - ETA: 0s - loss: 0.1882 - accuracy: 0.95 - ETA: 0s - loss: 0.1895 - accuracy: 0.95 - ETA: 0s - loss: 0.1914 - accuracy: 0.95 - ETA: 0s - loss: 0.1912 - accuracy: 0.95 - ETA: 0s - loss: 0.1931 - accuracy: 0.95 - ETA: 0s - loss: 0.1936 - accuracy: 0.95 - ETA: 0s - loss: 0.1944 - accuracy: 0.95 - ETA: 0s - loss: 0.1956 - accuracy: 0.95 - ETA: 0s - loss: 0.1955 - accuracy: 0.95 - ETA: 0s - loss: 0.1961 - accuracy: 0.95 - ETA: 0s - loss: 0.1963 - accuracy: 0.95 - ETA: 0s - loss: 0.1958 - accuracy: 0.95 - ETA: 0s - loss: 0.1957 - accuracy: 0.95 - ETA: 0s - loss: 0.1950 - accuracy: 0.95 - ETA: 0s - loss: 0.1948 - accuracy: 0.95 - ETA: 0s - loss: 0.1943 - accuracy: 0.95 - ETA: 0s - loss: 0.1945 - accuracy: 0.95 - ETA: 0s - loss: 0.1944 - accuracy: 0.95 - ETA: 0s - loss: 0.1946 - accuracy: 0.95 - 6s 3ms/step - loss: 0.1949 - accuracy: 0.9564\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.93 - ETA: 5s - loss: 0.1218 - accuracy: 0.96 - ETA: 5s - loss: 0.1168 - accuracy: 0.96 - ETA: 5s - loss: 0.1370 - accuracy: 0.96 - ETA: 5s - loss: 0.1301 - accuracy: 0.96 - ETA: 5s - loss: 0.1425 - accuracy: 0.96 - ETA: 5s - loss: 0.1473 - accuracy: 0.96 - ETA: 5s - loss: 0.1570 - accuracy: 0.96 - ETA: 5s - loss: 0.1539 - accuracy: 0.96 - ETA: 5s - loss: 0.1529 - accuracy: 0.96 - ETA: 5s - loss: 0.1486 - accuracy: 0.96 - ETA: 5s - loss: 0.1478 - accuracy: 0.96 - ETA: 5s - loss: 0.1471 - accuracy: 0.96 - ETA: 5s - loss: 0.1502 - accuracy: 0.96 - ETA: 5s - loss: 0.1511 - accuracy: 0.96 - ETA: 5s - loss: 0.1545 - accuracy: 0.96 - ETA: 5s - loss: 0.1548 - accuracy: 0.96 - ETA: 5s - loss: 0.1563 - accuracy: 0.96 - ETA: 5s - loss: 0.1566 - accuracy: 0.96 - ETA: 5s - loss: 0.1565 - accuracy: 0.96 - ETA: 5s - loss: 0.1563 - accuracy: 0.96 - ETA: 5s - loss: 0.1565 - accuracy: 0.96 - ETA: 4s - loss: 0.1570 - accuracy: 0.96 - ETA: 4s - loss: 0.1691 - accuracy: 0.96 - ETA: 4s - loss: 0.1705 - accuracy: 0.96 - ETA: 4s - loss: 0.1686 - accuracy: 0.96 - ETA: 4s - loss: 0.1698 - accuracy: 0.96 - ETA: 4s - loss: 0.1680 - accuracy: 0.96 - ETA: 4s - loss: 0.1664 - accuracy: 0.96 - ETA: 4s - loss: 0.1657 - accuracy: 0.96 - ETA: 4s - loss: 0.1652 - accuracy: 0.96 - ETA: 4s - loss: 0.1663 - accuracy: 0.96 - ETA: 4s - loss: 0.1689 - accuracy: 0.96 - ETA: 4s - loss: 0.1704 - accuracy: 0.96 - ETA: 4s - loss: 0.1710 - accuracy: 0.96 - ETA: 4s - loss: 0.1729 - accuracy: 0.95 - ETA: 4s - loss: 0.1729 - accuracy: 0.96 - ETA: 4s - loss: 0.1726 - accuracy: 0.96 - ETA: 4s - loss: 0.1729 - accuracy: 0.95 - ETA: 4s - loss: 0.1715 - accuracy: 0.96 - ETA: 4s - loss: 0.1702 - accuracy: 0.96 - ETA: 4s - loss: 0.1735 - accuracy: 0.96 - ETA: 4s - loss: 0.1743 - accuracy: 0.95 - ETA: 3s - loss: 0.1720 - accuracy: 0.96 - ETA: 3s - loss: 0.1719 - accuracy: 0.96 - ETA: 3s - loss: 0.1712 - accuracy: 0.96 - ETA: 3s - loss: 0.1690 - accuracy: 0.96 - ETA: 3s - loss: 0.1678 - accuracy: 0.96 - ETA: 3s - loss: 0.1692 - accuracy: 0.96 - ETA: 3s - loss: 0.1784 - accuracy: 0.96 - ETA: 3s - loss: 0.1799 - accuracy: 0.96 - ETA: 3s - loss: 0.1826 - accuracy: 0.95 - ETA: 3s - loss: 0.1832 - accuracy: 0.95 - ETA: 3s - loss: 0.1869 - accuracy: 0.95 - ETA: 3s - loss: 0.1983 - accuracy: 0.95 - ETA: 3s - loss: 0.2018 - accuracy: 0.95 - ETA: 3s - loss: 0.2049 - accuracy: 0.95 - ETA: 3s - loss: 0.2061 - accuracy: 0.95 - ETA: 3s - loss: 0.2055 - accuracy: 0.95 - ETA: 3s - loss: 0.2046 - accuracy: 0.95 - ETA: 3s - loss: 0.2060 - accuracy: 0.95 - ETA: 3s - loss: 0.2064 - accuracy: 0.95 - ETA: 2s - loss: 0.2074 - accuracy: 0.95 - ETA: 2s - loss: 0.2084 - accuracy: 0.95 - ETA: 2s - loss: 0.2076 - accuracy: 0.95 - ETA: 2s - loss: 0.2074 - accuracy: 0.95 - ETA: 2s - loss: 0.2068 - accuracy: 0.95 - ETA: 2s - loss: 0.2065 - accuracy: 0.95 - ETA: 2s - loss: 0.2057 - accuracy: 0.95 - ETA: 2s - loss: 0.2058 - accuracy: 0.95 - ETA: 2s - loss: 0.2059 - accuracy: 0.95 - ETA: 2s - loss: 0.2053 - accuracy: 0.95 - ETA: 2s - loss: 0.2062 - accuracy: 0.95 - ETA: 2s - loss: 0.2055 - accuracy: 0.95 - ETA: 2s - loss: 0.2060 - accuracy: 0.95 - ETA: 2s - loss: 0.2054 - accuracy: 0.95 - ETA: 2s - loss: 0.2048 - accuracy: 0.95 - ETA: 2s - loss: 0.2042 - accuracy: 0.95 - ETA: 2s - loss: 0.2033 - accuracy: 0.95 - ETA: 2s - loss: 0.2033 - accuracy: 0.95 - ETA: 2s - loss: 0.2030 - accuracy: 0.95 - ETA: 2s - loss: 0.2027 - accuracy: 0.95 - ETA: 2s - loss: 0.2028 - accuracy: 0.95 - ETA: 2s - loss: 0.2031 - accuracy: 0.95 - ETA: 1s - loss: 0.2039 - accuracy: 0.95 - ETA: 1s - loss: 0.2045 - accuracy: 0.95 - ETA: 1s - loss: 0.2040 - accuracy: 0.95 - ETA: 1s - loss: 0.2041 - accuracy: 0.95 - ETA: 1s - loss: 0.2040 - accuracy: 0.95 - ETA: 1s - loss: 0.2052 - accuracy: 0.95 - ETA: 1s - loss: 0.2052 - accuracy: 0.95 - ETA: 1s - loss: 0.2052 - accuracy: 0.95 - ETA: 1s - loss: 0.2049 - accuracy: 0.95 - ETA: 1s - loss: 0.2053 - accuracy: 0.95 - ETA: 1s - loss: 0.2054 - accuracy: 0.95 - ETA: 1s - loss: 0.2049 - accuracy: 0.95 - ETA: 1s - loss: 0.2049 - accuracy: 0.95 - ETA: 1s - loss: 0.2039 - accuracy: 0.95 - ETA: 1s - loss: 0.2039 - accuracy: 0.95 - ETA: 1s - loss: 0.2045 - accuracy: 0.95 - ETA: 1s - loss: 0.2046 - accuracy: 0.95 - ETA: 1s - loss: 0.2037 - accuracy: 0.95 - ETA: 1s - loss: 0.2034 - accuracy: 0.95 - ETA: 1s - loss: 0.2029 - accuracy: 0.95 - ETA: 1s - loss: 0.2026 - accuracy: 0.95 - ETA: 0s - loss: 0.2022 - accuracy: 0.95 - ETA: 0s - loss: 0.2020 - accuracy: 0.95 - ETA: 0s - loss: 0.2017 - accuracy: 0.95 - ETA: 0s - loss: 0.2015 - accuracy: 0.95 - ETA: 0s - loss: 0.2012 - accuracy: 0.95 - ETA: 0s - loss: 0.2015 - accuracy: 0.95 - ETA: 0s - loss: 0.2009 - accuracy: 0.95 - ETA: 0s - loss: 0.2005 - accuracy: 0.95 - ETA: 0s - loss: 0.2004 - accuracy: 0.95 - ETA: 0s - loss: 0.1999 - accuracy: 0.95 - ETA: 0s - loss: 0.1992 - accuracy: 0.95 - ETA: 0s - loss: 0.1997 - accuracy: 0.95 - ETA: 0s - loss: 0.2003 - accuracy: 0.95 - ETA: 0s - loss: 0.2003 - accuracy: 0.95 - ETA: 0s - loss: 0.2003 - accuracy: 0.95 - ETA: 0s - loss: 0.2011 - accuracy: 0.95 - ETA: 0s - loss: 0.2008 - accuracy: 0.95 - ETA: 0s - loss: 0.2000 - accuracy: 0.95 - ETA: 0s - loss: 0.1993 - accuracy: 0.95 - ETA: 0s - loss: 0.1987 - accuracy: 0.95 - 6s 3ms/step - loss: 0.1986 - accuracy: 0.9559\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.96 - ETA: 6s - loss: 0.2011 - accuracy: 0.93 - ETA: 6s - loss: 0.1597 - accuracy: 0.95 - ETA: 6s - loss: 0.1988 - accuracy: 0.95 - ETA: 6s - loss: 0.1930 - accuracy: 0.95 - ETA: 6s - loss: 0.1828 - accuracy: 0.95 - ETA: 6s - loss: 0.1718 - accuracy: 0.95 - ETA: 6s - loss: 0.1715 - accuracy: 0.96 - ETA: 6s - loss: 0.1702 - accuracy: 0.96 - ETA: 6s - loss: 0.1651 - accuracy: 0.96 - ETA: 6s - loss: 0.1605 - accuracy: 0.96 - ETA: 6s - loss: 0.1540 - accuracy: 0.96 - ETA: 6s - loss: 0.1558 - accuracy: 0.96 - ETA: 6s - loss: 0.1603 - accuracy: 0.96 - ETA: 6s - loss: 0.1656 - accuracy: 0.96 - ETA: 6s - loss: 0.1649 - accuracy: 0.96 - ETA: 6s - loss: 0.1730 - accuracy: 0.96 - ETA: 6s - loss: 0.1796 - accuracy: 0.96 - ETA: 6s - loss: 0.1792 - accuracy: 0.96 - ETA: 6s - loss: 0.1753 - accuracy: 0.96 - ETA: 6s - loss: 0.1731 - accuracy: 0.96 - ETA: 6s - loss: 0.1695 - accuracy: 0.96 - ETA: 6s - loss: 0.1722 - accuracy: 0.96 - ETA: 6s - loss: 0.1689 - accuracy: 0.96 - ETA: 6s - loss: 0.1685 - accuracy: 0.96 - ETA: 6s - loss: 0.1671 - accuracy: 0.96 - ETA: 6s - loss: 0.1664 - accuracy: 0.96 - ETA: 6s - loss: 0.1663 - accuracy: 0.96 - ETA: 5s - loss: 0.1674 - accuracy: 0.96 - ETA: 5s - loss: 0.1676 - accuracy: 0.96 - ETA: 5s - loss: 0.1675 - accuracy: 0.95 - ETA: 5s - loss: 0.1664 - accuracy: 0.96 - ETA: 5s - loss: 0.1663 - accuracy: 0.96 - ETA: 5s - loss: 0.1646 - accuracy: 0.96 - ETA: 5s - loss: 0.1637 - accuracy: 0.96 - ETA: 5s - loss: 0.1625 - accuracy: 0.96 - ETA: 5s - loss: 0.1646 - accuracy: 0.96 - ETA: 5s - loss: 0.1641 - accuracy: 0.96 - ETA: 5s - loss: 0.1637 - accuracy: 0.96 - ETA: 5s - loss: 0.1635 - accuracy: 0.96 - ETA: 4s - loss: 0.1639 - accuracy: 0.96 - ETA: 4s - loss: 0.1647 - accuracy: 0.96 - ETA: 4s - loss: 0.1687 - accuracy: 0.95 - ETA: 4s - loss: 0.1695 - accuracy: 0.95 - ETA: 4s - loss: 0.1687 - accuracy: 0.95 - ETA: 4s - loss: 0.1754 - accuracy: 0.95 - ETA: 4s - loss: 0.1824 - accuracy: 0.96 - ETA: 4s - loss: 0.1830 - accuracy: 0.95 - ETA: 4s - loss: 0.1859 - accuracy: 0.95 - ETA: 4s - loss: 0.1859 - accuracy: 0.95 - ETA: 4s - loss: 0.1861 - accuracy: 0.95 - ETA: 4s - loss: 0.1863 - accuracy: 0.95 - ETA: 4s - loss: 0.1860 - accuracy: 0.95 - ETA: 4s - loss: 0.1851 - accuracy: 0.95 - ETA: 4s - loss: 0.1871 - accuracy: 0.95 - ETA: 4s - loss: 0.1911 - accuracy: 0.95 - ETA: 3s - loss: 0.1930 - accuracy: 0.95 - ETA: 3s - loss: 0.1945 - accuracy: 0.95 - ETA: 3s - loss: 0.1943 - accuracy: 0.95 - ETA: 3s - loss: 0.1943 - accuracy: 0.95 - ETA: 3s - loss: 0.1937 - accuracy: 0.95 - ETA: 3s - loss: 0.1945 - accuracy: 0.95 - ETA: 3s - loss: 0.1940 - accuracy: 0.95 - ETA: 3s - loss: 0.1935 - accuracy: 0.95 - ETA: 3s - loss: 0.1930 - accuracy: 0.95 - ETA: 3s - loss: 0.1922 - accuracy: 0.95 - ETA: 3s - loss: 0.1909 - accuracy: 0.95 - ETA: 3s - loss: 0.1913 - accuracy: 0.95 - ETA: 3s - loss: 0.1928 - accuracy: 0.95 - ETA: 3s - loss: 0.1927 - accuracy: 0.95 - ETA: 3s - loss: 0.1928 - accuracy: 0.95 - ETA: 3s - loss: 0.1963 - accuracy: 0.95 - ETA: 2s - loss: 0.1955 - accuracy: 0.95 - ETA: 2s - loss: 0.1958 - accuracy: 0.95 - ETA: 2s - loss: 0.1954 - accuracy: 0.95 - ETA: 2s - loss: 0.1960 - accuracy: 0.95 - ETA: 2s - loss: 0.1950 - accuracy: 0.95 - ETA: 2s - loss: 0.1945 - accuracy: 0.95 - ETA: 2s - loss: 0.1943 - accuracy: 0.95 - ETA: 2s - loss: 0.1944 - accuracy: 0.95 - ETA: 2s - loss: 0.1941 - accuracy: 0.95 - ETA: 2s - loss: 0.1939 - accuracy: 0.95 - ETA: 2s - loss: 0.1933 - accuracy: 0.95 - ETA: 2s - loss: 0.1919 - accuracy: 0.95 - ETA: 2s - loss: 0.1910 - accuracy: 0.95 - ETA: 2s - loss: 0.1913 - accuracy: 0.95 - ETA: 2s - loss: 0.1920 - accuracy: 0.95 - ETA: 2s - loss: 0.1916 - accuracy: 0.95 - ETA: 2s - loss: 0.1909 - accuracy: 0.95 - ETA: 2s - loss: 0.1908 - accuracy: 0.95 - ETA: 1s - loss: 0.1899 - accuracy: 0.95 - ETA: 1s - loss: 0.1889 - accuracy: 0.95 - ETA: 1s - loss: 0.1887 - accuracy: 0.95 - ETA: 1s - loss: 0.1887 - accuracy: 0.95 - ETA: 1s - loss: 0.1883 - accuracy: 0.95 - ETA: 1s - loss: 0.1876 - accuracy: 0.95 - ETA: 1s - loss: 0.1871 - accuracy: 0.95 - ETA: 1s - loss: 0.1861 - accuracy: 0.95 - ETA: 1s - loss: 0.1866 - accuracy: 0.95 - ETA: 1s - loss: 0.1873 - accuracy: 0.95 - ETA: 1s - loss: 0.1878 - accuracy: 0.95 - ETA: 1s - loss: 0.1879 - accuracy: 0.95 - ETA: 1s - loss: 0.1876 - accuracy: 0.95 - ETA: 1s - loss: 0.1871 - accuracy: 0.95 - ETA: 1s - loss: 0.1886 - accuracy: 0.95 - ETA: 1s - loss: 0.1883 - accuracy: 0.95 - ETA: 1s - loss: 0.1879 - accuracy: 0.95 - ETA: 0s - loss: 0.1878 - accuracy: 0.95 - ETA: 0s - loss: 0.1883 - accuracy: 0.95 - ETA: 0s - loss: 0.1881 - accuracy: 0.95 - ETA: 0s - loss: 0.1877 - accuracy: 0.95 - ETA: 0s - loss: 0.1873 - accuracy: 0.95 - ETA: 0s - loss: 0.1883 - accuracy: 0.95 - ETA: 0s - loss: 0.1883 - accuracy: 0.95 - ETA: 0s - loss: 0.1882 - accuracy: 0.95 - ETA: 0s - loss: 0.1877 - accuracy: 0.95 - ETA: 0s - loss: 0.1872 - accuracy: 0.95 - ETA: 0s - loss: 0.1874 - accuracy: 0.95 - ETA: 0s - loss: 0.1873 - accuracy: 0.95 - ETA: 0s - loss: 0.1870 - accuracy: 0.95 - ETA: 0s - loss: 0.1867 - accuracy: 0.95 - ETA: 0s - loss: 0.1855 - accuracy: 0.95 - ETA: 0s - loss: 0.1846 - accuracy: 0.95 - ETA: 0s - loss: 0.1846 - accuracy: 0.95 - 6s 3ms/step - loss: 0.1843 - accuracy: 0.9590\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.96 - ETA: 5s - loss: 0.1468 - accuracy: 0.96 - ETA: 5s - loss: 0.1319 - accuracy: 0.97 - ETA: 5s - loss: 0.1357 - accuracy: 0.97 - ETA: 5s - loss: 0.1197 - accuracy: 0.97 - ETA: 5s - loss: 0.1380 - accuracy: 0.96 - ETA: 5s - loss: 0.1400 - accuracy: 0.96 - ETA: 5s - loss: 0.1376 - accuracy: 0.96 - ETA: 5s - loss: 0.1374 - accuracy: 0.96 - ETA: 5s - loss: 0.1408 - accuracy: 0.96 - ETA: 5s - loss: 0.1397 - accuracy: 0.96 - ETA: 5s - loss: 0.1409 - accuracy: 0.96 - ETA: 5s - loss: 0.1459 - accuracy: 0.96 - ETA: 5s - loss: 0.1454 - accuracy: 0.96 - ETA: 5s - loss: 0.1499 - accuracy: 0.96 - ETA: 5s - loss: 0.1579 - accuracy: 0.96 - ETA: 5s - loss: 0.1639 - accuracy: 0.96 - ETA: 5s - loss: 0.1694 - accuracy: 0.96 - ETA: 5s - loss: 0.1667 - accuracy: 0.96 - ETA: 4s - loss: 0.1654 - accuracy: 0.96 - ETA: 4s - loss: 0.1660 - accuracy: 0.96 - ETA: 4s - loss: 0.1622 - accuracy: 0.96 - ETA: 4s - loss: 0.1593 - accuracy: 0.96 - ETA: 4s - loss: 0.1589 - accuracy: 0.96 - ETA: 4s - loss: 0.1566 - accuracy: 0.96 - ETA: 4s - loss: 0.1576 - accuracy: 0.96 - ETA: 4s - loss: 0.1580 - accuracy: 0.96 - ETA: 4s - loss: 0.1583 - accuracy: 0.96 - ETA: 4s - loss: 0.1566 - accuracy: 0.96 - ETA: 4s - loss: 0.1551 - accuracy: 0.96 - ETA: 4s - loss: 0.1536 - accuracy: 0.96 - ETA: 4s - loss: 0.1550 - accuracy: 0.96 - ETA: 4s - loss: 0.1552 - accuracy: 0.96 - ETA: 4s - loss: 0.1554 - accuracy: 0.96 - ETA: 4s - loss: 0.1545 - accuracy: 0.96 - ETA: 4s - loss: 0.1522 - accuracy: 0.96 - ETA: 4s - loss: 0.1535 - accuracy: 0.96 - ETA: 4s - loss: 0.1518 - accuracy: 0.96 - ETA: 3s - loss: 0.1527 - accuracy: 0.96 - ETA: 3s - loss: 0.1534 - accuracy: 0.96 - ETA: 3s - loss: 0.1524 - accuracy: 0.96 - ETA: 3s - loss: 0.1534 - accuracy: 0.96 - ETA: 3s - loss: 0.1539 - accuracy: 0.96 - ETA: 3s - loss: 0.1539 - accuracy: 0.96 - ETA: 3s - loss: 0.1543 - accuracy: 0.96 - ETA: 3s - loss: 0.1544 - accuracy: 0.96 - ETA: 3s - loss: 0.1539 - accuracy: 0.96 - ETA: 3s - loss: 0.1543 - accuracy: 0.96 - ETA: 3s - loss: 0.1556 - accuracy: 0.96 - ETA: 3s - loss: 0.1570 - accuracy: 0.96 - ETA: 3s - loss: 0.1570 - accuracy: 0.96 - ETA: 3s - loss: 0.1564 - accuracy: 0.96 - ETA: 3s - loss: 0.1564 - accuracy: 0.96 - ETA: 3s - loss: 0.1562 - accuracy: 0.96 - ETA: 3s - loss: 0.1598 - accuracy: 0.96 - ETA: 3s - loss: 0.1605 - accuracy: 0.96 - ETA: 3s - loss: 0.1609 - accuracy: 0.96 - ETA: 2s - loss: 0.1600 - accuracy: 0.96 - ETA: 2s - loss: 0.1600 - accuracy: 0.96 - ETA: 2s - loss: 0.1602 - accuracy: 0.96 - ETA: 2s - loss: 0.1597 - accuracy: 0.96 - ETA: 2s - loss: 0.1593 - accuracy: 0.96 - ETA: 2s - loss: 0.1589 - accuracy: 0.96 - ETA: 2s - loss: 0.1582 - accuracy: 0.96 - ETA: 2s - loss: 0.1575 - accuracy: 0.96 - ETA: 2s - loss: 0.1579 - accuracy: 0.96 - ETA: 2s - loss: 0.1570 - accuracy: 0.96 - ETA: 2s - loss: 0.1566 - accuracy: 0.96 - ETA: 2s - loss: 0.1573 - accuracy: 0.96 - ETA: 2s - loss: 0.1573 - accuracy: 0.96 - ETA: 2s - loss: 0.1569 - accuracy: 0.96 - ETA: 2s - loss: 0.1574 - accuracy: 0.96 - ETA: 2s - loss: 0.1564 - accuracy: 0.96 - ETA: 2s - loss: 0.1555 - accuracy: 0.96 - ETA: 2s - loss: 0.1558 - accuracy: 0.96 - ETA: 2s - loss: 0.1557 - accuracy: 0.96 - ETA: 1s - loss: 0.1554 - accuracy: 0.96 - ETA: 1s - loss: 0.1568 - accuracy: 0.96 - ETA: 1s - loss: 0.1576 - accuracy: 0.96 - ETA: 1s - loss: 0.1582 - accuracy: 0.96 - ETA: 1s - loss: 0.1587 - accuracy: 0.96 - ETA: 1s - loss: 0.1589 - accuracy: 0.96 - ETA: 1s - loss: 0.1592 - accuracy: 0.96 - ETA: 1s - loss: 0.1591 - accuracy: 0.96 - ETA: 1s - loss: 0.1609 - accuracy: 0.96 - ETA: 1s - loss: 0.1624 - accuracy: 0.96 - ETA: 1s - loss: 0.1644 - accuracy: 0.96 - ETA: 1s - loss: 0.1656 - accuracy: 0.96 - ETA: 1s - loss: 0.1676 - accuracy: 0.96 - ETA: 1s - loss: 0.1708 - accuracy: 0.96 - ETA: 1s - loss: 0.1727 - accuracy: 0.95 - ETA: 1s - loss: 0.1741 - accuracy: 0.95 - ETA: 1s - loss: 0.1776 - accuracy: 0.95 - ETA: 1s - loss: 0.1781 - accuracy: 0.95 - ETA: 0s - loss: 0.1818 - accuracy: 0.95 - ETA: 0s - loss: 0.1834 - accuracy: 0.95 - ETA: 0s - loss: 0.1839 - accuracy: 0.95 - ETA: 0s - loss: 0.1847 - accuracy: 0.95 - ETA: 0s - loss: 0.1848 - accuracy: 0.95 - ETA: 0s - loss: 0.1849 - accuracy: 0.95 - ETA: 0s - loss: 0.1847 - accuracy: 0.95 - ETA: 0s - loss: 0.1857 - accuracy: 0.95 - ETA: 0s - loss: 0.1859 - accuracy: 0.95 - ETA: 0s - loss: 0.1865 - accuracy: 0.95 - ETA: 0s - loss: 0.1866 - accuracy: 0.95 - ETA: 0s - loss: 0.1863 - accuracy: 0.95 - ETA: 0s - loss: 0.1867 - accuracy: 0.95 - ETA: 0s - loss: 0.1864 - accuracy: 0.95 - ETA: 0s - loss: 0.1861 - accuracy: 0.95 - ETA: 0s - loss: 0.1857 - accuracy: 0.95 - ETA: 0s - loss: 0.1862 - accuracy: 0.95 - ETA: 0s - loss: 0.1861 - accuracy: 0.95 - ETA: 0s - loss: 0.1860 - accuracy: 0.95 - ETA: 0s - loss: 0.1866 - accuracy: 0.95 - ETA: 0s - loss: 0.1871 - accuracy: 0.95 - 6s 3ms/step - loss: 0.1870 - accuracy: 0.9575\n",
      "Epoch 8/100\n",
      " 499/1875 [======>.......................] - ETA: 0s - loss: 0.1201 - accuracy: 0.96 - ETA: 5s - loss: 0.2045 - accuracy: 0.95 - ETA: 5s - loss: 0.1742 - accuracy: 0.96 - ETA: 5s - loss: 0.1761 - accuracy: 0.95 - ETA: 5s - loss: 0.1657 - accuracy: 0.96 - ETA: 5s - loss: 0.1633 - accuracy: 0.96 - ETA: 5s - loss: 0.1575 - accuracy: 0.96 - ETA: 5s - loss: 0.1578 - accuracy: 0.96 - ETA: 5s - loss: 0.1576 - accuracy: 0.96 - ETA: 5s - loss: 0.1537 - accuracy: 0.96 - ETA: 5s - loss: 0.1565 - accuracy: 0.96 - ETA: 5s - loss: 0.1585 - accuracy: 0.96 - ETA: 5s - loss: 0.1660 - accuracy: 0.96 - ETA: 5s - loss: 0.1628 - accuracy: 0.96 - ETA: 5s - loss: 0.1599 - accuracy: 0.96 - ETA: 5s - loss: 0.1636 - accuracy: 0.96 - ETA: 5s - loss: 0.1637 - accuracy: 0.96 - ETA: 5s - loss: 0.1650 - accuracy: 0.96 - ETA: 4s - loss: 0.1674 - accuracy: 0.96 - ETA: 4s - loss: 0.1744 - accuracy: 0.96 - ETA: 4s - loss: 0.1732 - accuracy: 0.96 - ETA: 4s - loss: 0.1820 - accuracy: 0.96 - ETA: 4s - loss: 0.1897 - accuracy: 0.95 - ETA: 4s - loss: 0.1950 - accuracy: 0.95 - ETA: 4s - loss: 0.2001 - accuracy: 0.95 - ETA: 4s - loss: 0.2005 - accuracy: 0.95 - ETA: 4s - loss: 0.1994 - accuracy: 0.95 - ETA: 4s - loss: 0.2016 - accuracy: 0.95 - ETA: 4s - loss: 0.2011 - accuracy: 0.95 - ETA: 4s - loss: 0.2048 - accuracy: 0.95 - ETA: 4s - loss: 0.2051 - accuracy: 0.9565"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-65941f53ed37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m model.fit(\n\u001b[0;32m     15\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_n\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256,input_shape = (784,),activation='relu'),\n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X,y_n,\n",
    "    epochs=100\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = mynn.expand_labels(y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
