{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork():\n",
    "    \"\"\"\n",
    "    My implementation of a Neural Network Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
    "    weight_inits = ['zero', 'random', 'normal']\n",
    "\n",
    "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
    "        \"\"\"\n",
    "        Initializing a new MyNeuralNetwork object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers : int value specifying the number of layers\n",
    "\n",
    "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
    "\n",
    "        activation : string specifying the activation function to be used\n",
    "                     possible inputs: relu, sigmoid, linear, tanh\n",
    "\n",
    "        learning_rate : float value specifying the learning rate to be used\n",
    "\n",
    "        weight_init : string specifying the weight initialization function to be used\n",
    "                      possible inputs: zero, random, normal\n",
    "\n",
    "        batch_size : int value specifying the batch size to be used\n",
    "\n",
    "        num_epochs : int value specifying the number of epochs to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if activation not in self.acti_fns:\n",
    "            raise Exception('Incorrect Activation Function')\n",
    "\n",
    "        if weight_init not in self.weight_inits:\n",
    "            raise Exception('Incorrect Weight Initialization Function')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_init = weight_init\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def relu(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return X * (X>=0)\n",
    "\n",
    "    def relu_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return 1*(X>=0)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-X))\n",
    "\n",
    "    def sigmoid_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return self.sigmoid(X) *(1-self.sigmoid (X))\n",
    "\n",
    "    def linear(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def linear_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return np.ones(X.shape)\n",
    "\n",
    "    def tanh(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def tanh_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return 1-self.tanh(X)*self.tanh(X)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        exp = np.exp(X)\n",
    "        return exp/(np.sum(exp,axis = 1, keepdims = True))\n",
    "\n",
    "    def softmax_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Softmax activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def zero_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Zero Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 1-dimensional numpy array which contains the initial weights for the requested layer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        weight = np.zeros(shape)\n",
    "        return weight\n",
    "\n",
    "    def random_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Random Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 1-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        weight = np.random.rand(shape[0], shape[1])*0.01\n",
    "        return weight\n",
    "\n",
    "    def normal_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 1-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        weight = np.random.normal(size = shape)\n",
    "        return weight\n",
    "\n",
    "    def fit(self, X, y, x_test = None, y_test = None):\n",
    "        \"\"\"\n",
    "        Fitting (training) the linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : an instance of self\n",
    "        \"\"\"\n",
    "\n",
    "        # fit function has to return an instance of itself or else it won't work with test.py\n",
    "        \n",
    "        \n",
    "        y = self.expand_labels(y)\n",
    "        \n",
    "        m , n_0 = X.shape\n",
    "        n_l = y.shape[1]\n",
    "\n",
    "        parameters = self.initialization()\n",
    "        self.parameters = parameters\n",
    "\n",
    "        train_loss_history = []\n",
    "        train_accuracy_history = []\n",
    "        test_loss_history = []\n",
    "        test_accuracy_history = []\n",
    "\n",
    "        \n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs), desc = \"Progress Total : \", position = 0, leave = True):\n",
    "\n",
    "\n",
    "            n_batches = m//self.batch_size\n",
    "            X_batches = [X[self.batch_size*i:self.batch_size*(i+1),:] for i in range(0,n_batches)]\n",
    "            y_batches = [y[self.batch_size*i:self.batch_size*(i+1),:] for i in range(0,n_batches)]\n",
    "\n",
    "            train_batch_loss = []\n",
    "            test_batch_loss = []\n",
    "            train_batch_accuracy = []\n",
    "            test_batch_accuracy = []\n",
    "\n",
    "            for curr_x, curr_y in tqdm(zip(X_batches,y_batches), desc = \"Progress Epoch: \" + str(epoch+1) + \"/\" + str(self.num_epochs), position = 0, leave = True, total = len(X_batches)):\n",
    "                A, activations = self.forward_prop(curr_x,parameters)\n",
    "\n",
    "                train_cost = self.cross_entropy_loss(A,curr_y)\n",
    "                train_batch_loss.append(train_cost)\n",
    "#                 print(A)\n",
    "                self.backward_prop(curr_x,curr_y,A,activations)\n",
    "                train_batch_accuracy.append(self.score(curr_x,np.argmax(curr_y,axis = 1)))\n",
    "                if(x_test is not None):\n",
    "                    proba = self.predict_proba(x_test)\n",
    "#                     print(proba.shape)\n",
    "                    test_loss = self.cross_entropy_loss(proba, self.expand_labels(y_test))\n",
    "                    test_batch_loss.append(test_loss)\n",
    "                    test_batch_accuracy.append(self.score(x_test, y_test))\n",
    "                    \n",
    "            print(\"Training Accuracy : \", np.array(train_batch_accuracy).mean())\n",
    "            print(\"Validation Accuracy : \", np.array(test_batch_accuracy).mean())\n",
    "            print(\"Validation loss : \" ,np.array(test_batch_loss).mean())\n",
    "            print(\"Training Loss : \", np.array(train_batch_loss).mean())\n",
    "            \n",
    "\n",
    "\n",
    "            train_loss_history.append( np.array(train_batch_loss).mean())\n",
    "            train_accuracy_history.append( np.array(train_batch_accuracy).mean())\n",
    "            test_loss_history.append( np.array(test_batch_loss).mean())\n",
    "            test_accuracy_history.append(  np.array(test_batch_accuracy).mean())\n",
    "                \n",
    "                \n",
    "        \n",
    "        self.train_loss_history = train_loss_history\n",
    "        self.train_accuracy_history = train_accuracy_history\n",
    "        self.test_loss_history = test_loss_history\n",
    "        self.test_accuracy_history = test_accuracy_history\n",
    "        \n",
    "        \n",
    "        self.parameters = parameters\n",
    "\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def initialization(self):\n",
    "        parameters = {}\n",
    "        layers = self.layer_sizes\n",
    "        \n",
    "        for i in range(0,len(layers)-1):\n",
    "            if(self.weight_init == 'zero'):\n",
    "                curr_layer = self.zero_init((layers[i],layers[i+1]))\n",
    "\n",
    "            elif(self.weight_init == 'random'):\n",
    "                curr_layer = self.random_init((layers[i],layers[i+1]))\n",
    "\n",
    "            else:\n",
    "                curr_layer = self.normal_init((layers[i],layers[i+1]))\n",
    "\n",
    "            parameters[\"W\" + str(i+1)] = curr_layer\n",
    "            parameters[\"b\" + str(i+1)] = np.zeros((1,layers[i+1]))\n",
    "\n",
    "        self.parameters = parameters\n",
    "\n",
    "        return parameters\n",
    "        \n",
    "    \n",
    "    def forward_prop(self,X,parameters):\n",
    "\n",
    "        \"\"\"\n",
    "        Implements one forward propagation of the deep neural network.\n",
    "\n",
    "        Parameters \n",
    "        ----------\n",
    "        X : Training set to be forward propagated\n",
    "        parameters : model parameters \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A_l : Activations of the final layer\n",
    "        forward_cache : list contraining all the linear_cache and activation_cache of all the layers\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        A = X\n",
    "        L = len(parameters)//2\n",
    "\n",
    "        activations = {}\n",
    "\n",
    "        for i in range(0,L-1):\n",
    "            A_prev = A\n",
    "#             print(A_prev.shape)\n",
    "            \n",
    "            Z = np.dot(A_prev, parameters[\"W\" + str(i+1)]) + parameters[\"b\" + str(i+1)]\n",
    "\n",
    "            if(self.activation == \"relu\"):\n",
    "                A = self.relu(Z)\n",
    "\n",
    "            elif (self.activation == \"tanh\"):\n",
    "                A = self.tanh(Z)\n",
    "\n",
    "            elif (self.activation == \"linear\"):\n",
    "                A = self.linear(Z)\n",
    "\n",
    "            elif (self.activation == \"sigmoid\"):\n",
    "                A = self.sigmoid(Z)\n",
    "\n",
    "\n",
    "            activations[\"A\" + str(i+1)] = A           \n",
    "            A_prev = A\n",
    "        \n",
    "        \n",
    "        Z_l = np.dot(A_prev, parameters[\"W\" + str(L)]) + parameters[\"b\" + str(L)]        \n",
    "        A_l = self.softmax(Z_l)\n",
    "        activations[\"A\" + str(L)] = A_l\n",
    "\n",
    "        return A_l, activations\n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward_prop(self, X, y, A_l,activations):\n",
    "        \"\"\"\n",
    "        Implements backward propagation of the complete model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : The ground truth labels of the curr training set\n",
    "        A_l : activations of the final layer of the model\n",
    "        cache : tuple containing the linear_cache and activation_cache\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradients : Dictionary containing the gradient vectors for each layer of the model\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        L = len(activations)\n",
    "        da = (activations[\"A\" + str(L)] - y)/len(y)\n",
    "#         print(\"da :\", da.shape)\n",
    "#         print(activations[\"A\" + str(L-1)].shape)\n",
    "        prev_weights = copy.deepcopy(self.parameters[\"W\" + str(L)])\n",
    "        self.parameters[\"W\" + str(L)] -= self.learning_rate *np.maximum(np.minimum(np.dot(activations[\"A\" + str(L-1)].T, da),50),-50)\n",
    "        self.parameters[\"b\" + str(L)]-= self.learning_rate * np.maximum(np.minimum(np.sum(da, axis = 0, keepdims = True),50),-50)\n",
    "        activations[\"A0\"] = X\n",
    "        \n",
    "        for i in range(L-1, 0, -1):\n",
    "            dz = np.dot(da,prev_weights.T)\n",
    "            if(self.activation == \"relu\"):\n",
    "                da = dz*self.relu_grad(activations[\"A\" + str(i)])\n",
    "\n",
    "            elif (self.activation == \"tanh\"):\n",
    "                da = dz*self.tanh_grad(activations[\"A\" + str(i)])\n",
    "\n",
    "            elif (self.activation == \"linear\"):\n",
    "                da = dz*self.linear_grad(activations[\"A\" + str(i)])\n",
    "\n",
    "            elif (self.activation == \"sigmoid\"):\n",
    "                da = dz*self.sigmoid_grad(activations[\"A\" + str(i)])\n",
    "        \n",
    "        \n",
    "            prev_weights = copy.deepcopy(self.parameters[\"W\" + str(i)])\n",
    "#             print(\"W:\",self.parameters[\"W\" + str(i)].shape)\n",
    "#             print(\"A.da\",np.dot(activations[\"A\" + str(i-1)].T, da).shape)\n",
    "#             print(\"A\",activations[\"A\" + str(i-1)].shape)\n",
    "#             print(\"da\",da.shape)\n",
    "            self.parameters[\"W\" + str(i)]-= self.learning_rate * np.maximum(np.minimum(np.dot(activations[\"A\" + str(i-1)].T, da),50),-50)\n",
    "            self.parameters[\"b\" + str(i)] -= self.learning_rate * np.maximum(np.minimum(np.sum(da, axis = 0),50),-50)\n",
    "           \n",
    "\n",
    "  \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicting probabilities using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the \n",
    "            class wise prediction probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        proba,_ = self.forward_prop(X,self.parameters)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
    "        \"\"\"\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        proba = self.predict_proba(X)\n",
    "#         print(proba.shape)\n",
    "        y_pred = np.argmax(proba, axis = 1)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        acc : float value specifying the accuracy of the model on the provided testing set\n",
    "        \"\"\"\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        y_pred = self.predict(X)\n",
    "        acc = (y_pred == y)\n",
    "        return acc.sum()/len(y)\n",
    "    \n",
    "    \n",
    "    def cross_entropy_loss(self, A, y):\n",
    "        n = len(y)\n",
    "        logp = - np.log(A[np.arange(n), y.argmax(axis=1)])\n",
    "        loss = np.sum(logp)/n\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def expand_labels(self, y):\n",
    "        m = len(y)\n",
    "        c = np.max(y)\n",
    "        new_y = np.zeros((m,c+1))\n",
    "        for i in range(m):\n",
    "            l = y[i]\n",
    "            new_y[i,l] = 1\n",
    "\n",
    "        return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/mnist/mnist_train.csv')\n",
    "test_df = pd.read_csv('dataset/mnist/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_df.to_numpy()\n",
    "testset = test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = dataset[:, 1:]\n",
    "X_test = testset[:, 1:]\n",
    "standardscalar = StandardScaler()\n",
    "X_train = standardscalar.fit_transform(X_train)\n",
    "X_test = standardscalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dataset[:, 0]\n",
    "y_test = testset[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyNeuralNetwork(5, [784, 256, 128, 64, 10], 'linear', 0.5, 'random', len(X_train)//20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 1/20: 100%|████████████████████████████████████████████████████████████| 20/20 [00:12<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy :  0.2129166666666667\n",
      "Validation Accuracy :  0.21245499999999998\n",
      "Validation loss :  2.0103377695581024\n",
      "Training Loss :  2.0361859074095525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 2/20: 100%|████████████████████████████████████████████████████████████| 20/20 [00:12<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy :  0.22965\n",
      "Validation Accuracy :  0.22812999999999994\n",
      "Validation loss :  1.8258399286379414\n",
      "Training Loss :  1.8391145894946068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 3/20: 100%|████████████████████████████████████████████████████████████| 20/20 [00:12<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy :  0.26139999999999997\n",
      "Validation Accuracy :  0.26042499999999996\n",
      "Validation loss :  1.7425493712202265\n",
      "Training Loss :  1.7459553821999214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 4/20: 100%|████████████████████████████████████████████████████████████| 20/20 [00:12<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy :  0.2830666666666667\n",
      "Validation Accuracy :  0.28036\n",
      "Validation loss :  1.7099253661700426\n",
      "Training Loss :  1.7087594085952265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Epoch: 5/20:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:11<00:00,  1.50it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-60b2b561b41f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-175-e2c1a9ea8976>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, x_test, y_test)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[0mtrain_batch_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                     \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;31m#                     print(proba.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                     \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-175-e2c1a9ea8976>\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# return the numpy array y which contains the predicted values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m         \u001b[0mproba\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-175-e2c1a9ea8976>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(self, X, parameters)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;31m#             print(A_prev.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2611"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
